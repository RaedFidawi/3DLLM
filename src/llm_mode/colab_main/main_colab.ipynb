{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "pgD2eHnsz3gO",
      "metadata": {
        "id": "pgD2eHnsz3gO"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.amp import autocast, GradScaler\n",
        "import os\n",
        "import itertools\n",
        "import math\n",
        "from pathlib import Path\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "339b0d3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "339b0d3c",
        "outputId": "218251c3-70e4-41d5-f5b2-9f3cf552c2c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "191c6626",
      "metadata": {
        "id": "191c6626"
      },
      "outputs": [],
      "source": [
        "def unzip_to_dir(zip_path: str, out_dir: str, overwrite: bool = False, only_ext=(\".npz\",)):\n",
        "    \"\"\"\n",
        "    Safely extract files from a zip into out_dir.\n",
        "    - Protects against Zip Slip (path traversal).\n",
        "    - Skips existing files unless overwrite=True.\n",
        "    - Optionally restricts to certain extensions (default: .npz).\n",
        "\n",
        "    Returns:\n",
        "        extracted_count (int)\n",
        "    \"\"\"\n",
        "    zip_path = Path(zip_path)\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    extracted = 0\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        for member in zf.infolist():\n",
        "            # skip directories and non-matching extensions\n",
        "            if member.is_dir():\n",
        "                continue\n",
        "            if only_ext and not member.filename.lower().endswith(only_ext):\n",
        "                continue\n",
        "\n",
        "            # resolve safe, normalized extraction path\n",
        "            dest = out_dir / member.filename\n",
        "            dest_parent = dest.parent.resolve()\n",
        "            out_root = out_dir.resolve()\n",
        "            if not str(dest_parent).startswith(str(out_root)):\n",
        "                # Zip Slip detected, skip\n",
        "                continue\n",
        "\n",
        "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "            if dest.exists() and not overwrite:\n",
        "                continue\n",
        "\n",
        "            with zf.open(member) as src, open(dest, \"wb\") as dst:\n",
        "                shutil.copyfileobj(src, dst)\n",
        "            extracted += 1\n",
        "\n",
        "    return extracted\n",
        "\n",
        "class VoxelFileLister:\n",
        "    \"\"\"\n",
        "    Lists .npz files under a directory (recursively) and loads 'complete' arrays.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir: str, recursive: bool = True):\n",
        "        self.root_dir = os.path.abspath(root_dir)\n",
        "        self.recursive = recursive\n",
        "        self.npz_files = self._scan_npz(self.root_dir, recursive)\n",
        "        if len(self.npz_files) == 0:\n",
        "            raise ValueError(f\"No .npz files found under {self.root_dir}\")\n",
        "        self.npz_files.sort()  # stable order\n",
        "\n",
        "    @staticmethod\n",
        "    def _scan_npz(root: str, recursive: bool):\n",
        "        paths = []\n",
        "        if recursive:\n",
        "            for dirpath, _, filenames in os.walk(root):\n",
        "                for fn in filenames:\n",
        "                    if fn.lower().endswith(\".npz\"):\n",
        "                        paths.append(os.path.join(dirpath, fn))\n",
        "        else:\n",
        "            for fn in os.listdir(root):\n",
        "                if fn.lower().endswith(\".npz\"):\n",
        "                    paths.append(os.path.join(root, fn))\n",
        "        return paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.npz_files)\n",
        "\n",
        "    def load_single_file(self, file_path: str) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            complete: torch.FloatTensor [1, D, H, W] with {0,1}\n",
        "        \"\"\"\n",
        "        with np.load(file_path) as data:\n",
        "            if \"complete\" not in data:\n",
        "                raise ValueError(f\"NPZ file {file_path} must contain 'complete' array\")\n",
        "            complete = torch.from_numpy(data[\"complete\"]).float()\n",
        "        if complete.dim() == 3:\n",
        "            complete = complete.unsqueeze(0)    # [1,D,H,W]\n",
        "        elif complete.dim() == 4:\n",
        "            pass                                # already [C,D,H,W]\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected 'complete' shape in {file_path}: {tuple(complete.shape)}\")\n",
        "        complete = (complete > 0).float()\n",
        "        return complete\n",
        "\n",
        "    def get_voxel_grid(self, index: int) -> torch.Tensor:\n",
        "        if index < 0 or index >= len(self.npz_files):\n",
        "            raise IndexError(f\"Index {index} out of range (0..{len(self.npz_files)-1})\")\n",
        "        return self.load_single_file(self.npz_files[index])\n",
        "\n",
        "\n",
        "class VoxelDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Directory-based Dataset: yields only the 'complete' occupancy grid\n",
        "    as float tensor [1, D, H, W].\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir: str, transform=None, recursive: bool = True):\n",
        "        self.files = VoxelFileLister(root_dir, recursive=recursive)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        complete = self.files.get_voxel_grid(idx)  # [1,D,H,W], float {0,1}\n",
        "        if self.transform:\n",
        "            complete = self.transform(complete)\n",
        "        return complete\n",
        "\n",
        "def create_data_loaders_from_dir(\n",
        "    root_dir: str,\n",
        "    batch_size: int = 1,\n",
        "    shuffle: bool = True,\n",
        "    num_workers: int = 4,              # you can now use workers safely\n",
        "    seed: int = 42,\n",
        "    recursive: bool = True,\n",
        "    persistent_workers: bool = True,\n",
        "    prefetch_factor: int = 2,\n",
        "):\n",
        "    \"\"\"\n",
        "    Builds train/val/test DataLoaders from an UNZIPPED directory of NPZ files.\n",
        "    \"\"\"\n",
        "    dataset = VoxelDataset(root_dir, recursive=recursive)\n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "    n = len(dataset)\n",
        "    indices = list(range(n))\n",
        "    random.Random(seed).shuffle(indices)\n",
        "\n",
        "    n_trainval = int(n * 0.8)\n",
        "    trainval_indices = indices[:n_trainval]\n",
        "    test_indices     = indices[n_trainval:]\n",
        "\n",
        "    n_train = int(len(trainval_indices) * 0.8)\n",
        "    train_indices = trainval_indices[:n_train]\n",
        "    val_indices   = trainval_indices[n_train:]\n",
        "\n",
        "    common_kwargs = dict(\n",
        "        batch_size=batch_size,\n",
        "        # num_workers=num_workers,\n",
        "        num_workers= 0,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=persistent_workers if num_workers > 0 else False,\n",
        "        prefetch_factor=prefetch_factor if num_workers > 0 else None,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(Subset(dataset, train_indices), shuffle=shuffle, **common_kwargs)\n",
        "    val_loader   = DataLoader(Subset(dataset, val_indices),   shuffle=False,  **common_kwargs)\n",
        "    test_loader  = DataLoader(Subset(dataset, test_indices),  shuffle=False,  **common_kwargs)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "class CachedVoxelSubset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Load a subset of VoxelDataset into RAM once, so __getitem__\n",
        "    is just a tensor lookup (no np.load during training).\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dataset: VoxelDataset, indices):\n",
        "        self.data = []\n",
        "        for i in indices:\n",
        "            self.data.append(base_dataset[i])   # this calls np.load ONCE per file\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# ------------------------------\n",
        "# Positional encoding (fixed for [B, D, H, W, d_model])\n",
        "# ------------------------------\n",
        "class RelPosBias(nn.Module):\n",
        "    \"\"\"\n",
        "    Learned 3D relative attention bias for a fixed window size.\n",
        "    - One bias per (dz,dy,dx) offset per head.\n",
        "    - Shape during use: [1, num_heads, 1, ws^3] so it can be added to attention scores.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads: int, window_size: int):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.num_positions = window_size ** 3  # matches flattening order used in LocalAttention\n",
        "        self.bias = nn.Parameter(torch.zeros(num_heads, self.num_positions))\n",
        "        nn.init.trunc_normal_(self.bias, std=0.02)\n",
        "\n",
        "    def forward(self):\n",
        "        # return [1, h, 1, ws^3] broadcastable to [B, h, 1, ws^3]\n",
        "        return self.bias.unsqueeze(0).unsqueeze(2)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Local attention (your class, unchanged)\n",
        "# ------------------------------\n",
        "class LocalAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int = 4, window_size: int = 3):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        assert d_model % num_heads == 0\n",
        "        self.window_size = window_size\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "    def forward(self, target_embedding, neighbor_embeddings, mask, rel_pos_bias=None):\n",
        "        \"\"\"\n",
        "        target_embedding:    [B, d_model]\n",
        "        neighbor_embeddings: [B, ws, ws, ws, d_model]\n",
        "        mask:                [B, ws, ws, ws] float {0,1}\n",
        "        \"\"\"\n",
        "        B = target_embedding.shape[0]\n",
        "        ws = self.window_size\n",
        "\n",
        "        neighbor_flat = neighbor_embeddings.contiguous().view(B, ws*ws*ws, self.d_model)\n",
        "        mask_flat = (mask > 0.5).contiguous().view(B, ws*ws*ws)\n",
        "\n",
        "        q = self.q_proj(target_embedding.unsqueeze(1))                         # [B,1,C]\n",
        "        k = self.k_proj(neighbor_flat)                                         # [B,ws^3,C]\n",
        "        v = self.v_proj(neighbor_flat)\n",
        "\n",
        "        q = q.view(B, 1, self.num_heads, self.head_dim).transpose(1, 2)        # [B,h,1,d]\n",
        "        k = k.view(B, ws*ws*ws, self.num_heads, self.head_dim).transpose(1, 2) # [B,h,ws^3,d]\n",
        "        v = v.view(B, ws*ws*ws, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale             # [B,h,1,ws^3]\n",
        "\n",
        "        if rel_pos_bias is not None:\n",
        "            # rel_pos_bias shape: [1,h,1,ws^3] -> broadcast over batch\n",
        "            scores = scores + rel_pos_bias\n",
        "\n",
        "        mask_expanded = mask_flat.unsqueeze(1).unsqueeze(2).expand(-1, self.num_heads, 1, -1)\n",
        "        scores = scores.masked_fill(~mask_expanded, float('-inf'))\n",
        "\n",
        "        # === Dangerous when all neighbors are masked ===\n",
        "        # all-false safety: avoid NaNs when no known neighbors\n",
        "        # all_false = ~mask_expanded.any(dim=-1, keepdim=True)                   # [B,h,1,1]\n",
        "        # scores = torch.where(all_false, torch.zeros_like(scores), scores)\n",
        "\n",
        "        # attn_weights = F.softmax(scores, dim=-1)\n",
        "        # ===\n",
        "\n",
        "        # === NEW CODE STARTS HERE ===\n",
        "        # Detect samples that have no valid (attendable) neighbors\n",
        "        valid_counts = mask_flat.sum(dim=1)       # [B]\n",
        "        no_ctx = (valid_counts == 0)              # [B] bool\n",
        "        no_ctx_broadcast = no_ctx.view(-1, 1, 1, 1)   # [B,1,1,1] for broadcasting\n",
        "\n",
        "        # If there is no context, avoid softmax(-inf,...,-inf): set scores to 0 beforehand.\n",
        "        # (This yields a uniform softmax, which we immediately zero out below.)\n",
        "        scores = torch.where(no_ctx_broadcast, torch.zeros_like(scores), scores)\n",
        "\n",
        "        # attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Zero the weights for no-context samples (no evidence => no attention contribution)\n",
        "        attn_weights = torch.where(no_ctx_broadcast, torch.zeros_like(attn_weights), attn_weights)\n",
        "\n",
        "        # attention output\n",
        "        out = torch.matmul(attn_weights, v)                       # [B,h,1,d]\n",
        "        out = out.transpose(1, 2).contiguous().view(B, 1, self.d_model).squeeze(1)  # [B,d_model]\n",
        "\n",
        "        # Project, THEN zero for no-context to prevent Linear bias from leaking in\n",
        "        proj = self.out_proj(out)                                 # [B,d_model]\n",
        "        proj = torch.where(no_ctx.unsqueeze(1), torch.zeros_like(proj), proj)\n",
        "\n",
        "        return proj\n",
        "\n",
        "# ------------------------------\n",
        "# Voxel transformer layer that uses LocalAttention per voxel\n",
        "# ------------------------------\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int = 8, window_size: int = 3, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attn = LocalAttention(d_model, num_heads, window_size)\n",
        "        self.rel_bias = RelPosBias(num_heads, window_size)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * 4, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.window_size = window_size\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x, neighborhood_fn, mask_fn):\n",
        "        \"\"\"\n",
        "        x: [B, ws, ws, ws, d_model]  (patch embeddings)\n",
        "        neighborhood_fn: (grid, dd, hh, ww, ws) -> [B, ws, ws, ws, d_model]\n",
        "        mask_fn:         (Dp,Hp,Wp, dd,hh,ww, ws) -> [B, ws, ws, ws]  (0/1 numeric; known-mask)\n",
        "        Returns:\n",
        "            x_center: [B, d_model]  (updated center embedding only)\n",
        "        \"\"\"\n",
        "        B, Dp, Hp, Wp, C = x.shape\n",
        "        r = self.window_size // 2\n",
        "        # target (center) token\n",
        "        target = x[:, r, r, r, :]  # [B, d_model]\n",
        "\n",
        "        # neighborhood embeddings and known mask for the center position\n",
        "        neighbors = neighborhood_fn(x, r, r, r, self.window_size)  # [B, ws, ws, ws, d_model]\n",
        "        mask      = mask_fn(Dp, Hp, Wp, r, r, r, self.window_size) # [B, ws, ws, ws] (0/1)\n",
        "\n",
        "        # attention (center queries neighbors)\n",
        "        tgt = self.norm1(target)\n",
        "        attn_out = self.attn(tgt, neighbors, mask, rel_pos_bias=self.rel_bias())                 # [B, d_model]\n",
        "        target = target + self.dropout(attn_out)\n",
        "\n",
        "        # ffn on center\n",
        "        tgt2 = self.norm2(target)\n",
        "        ffn_out = self.ffn(tgt2)                                   # [B, d_model]\n",
        "        target = target + ffn_out\n",
        "\n",
        "        return target  # [B, d_model]\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Stack layers\n",
        "# ------------------------------\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers: int, d_model: int, num_heads: int, window_size: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model, num_heads, window_size, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, neighborhood_fn, mask_fn):\n",
        "        \"\"\"\n",
        "        x: [B, D, H, W, d_model]\n",
        "        \"\"\"\n",
        "        center = None\n",
        "        for layer in self.layers:\n",
        "            center = layer(x, neighborhood_fn, mask_fn)\n",
        "            x = x.clone()\n",
        "            r = x.shape[1] // 2\n",
        "            x[:, r, r, r, :] = center\n",
        "        return center\n",
        "\n",
        "# ------------------------------\n",
        "# neighborhood_raw: returns raw patch (channel-first) and known_mask\n",
        "# ------------------------------\n",
        "def neighborhood_raw(occ_grid, known_grid, b, d, h, w, window_size):\n",
        "    \"\"\"\n",
        "    occ_grid:   [B,1,D,H,W] float {0,1}\n",
        "    known_grid: [B,1,D,H,W] float {0,1}\n",
        "\n",
        "    Returns:\n",
        "        patch_occ:   [1,1,ws,ws,ws] float {0,1}\n",
        "        patch_known: [1,  ws,ws,ws] float {0,1}\n",
        "    \"\"\"\n",
        "    _, C, D, H, W = occ_grid.shape\n",
        "    assert C == 1\n",
        "    r = window_size // 2\n",
        "\n",
        "    d0, d1 = max(0, d-r), min(D, d+r+1)\n",
        "    h0, h1 = max(0, h-r), min(H, h+r+1)\n",
        "    w0, w1 = max(0, w-r), min(W, w+r+1)\n",
        "\n",
        "    patch_occ   = occ_grid[b:b+1, :, d0:d1, h0:h1, w0:w1]          # [1,1,dp,hp,wp]\n",
        "    patch_known = known_grid[b:b+1, :, d0:d1, h0:h1, w0:w1]        # [1,1,dp,hp,wp]\n",
        "\n",
        "    # pads: (W_left, W_right, H_top, H_bottom, D_front, D_back)\n",
        "    pad = (\n",
        "        max(0, r - w),               max(0, (w + r + 1) - W),\n",
        "        max(0, r - h),               max(0, (h + r + 1) - H),\n",
        "        max(0, r - d),               max(0, (d + r + 1) - D),\n",
        "    )\n",
        "\n",
        "    if any(p > 0 for p in pad):\n",
        "        patch_occ   = F.pad(patch_occ,   pad, value=0.0)  # unknown outside -> 0 (empty value; but also unknown)\n",
        "        patch_known = F.pad(patch_known, pad, value=0.0)  # outside is unknown => 0\n",
        "\n",
        "    # squeeze channel for known to [1,ws,ws,ws]\n",
        "    patch_known = patch_known[:, 0]\n",
        "    return patch_occ.contiguous(), patch_known.contiguous()\n",
        "\n",
        "class IterativeVoxelModel(nn.Module):\n",
        "    def __init__(self, d_model: int = 64, num_heads: int = 4, num_layers: int = 3,\n",
        "                 window_size: int = 3, max_grid_size: int = 16, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        This model expects per-call patch input:\n",
        "            neighbors_patch: [B, 1, ws, ws, ws]  (binary occupancy values or zeros)\n",
        "            known_mask:      [B,    ws, ws, ws]  (1 = this voxel is known/visible, 0 = unknown/masked)\n",
        "\n",
        "        - Input is projected via Conv3d(2 -> d_model):\n",
        "              channel 0 = occupancy (0/1)\n",
        "              channel 1 = known_flag (0/1)\n",
        "\n",
        "        - We also add a learned 3D positional embedding of shape [1, ws, ws, ws, d_model]\n",
        "          on top of the projected features before feeding the transformer.\n",
        "\n",
        "        - Transformer returns center embedding -> output_head -> scalar logit.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.window_size = window_size\n",
        "\n",
        "        # 2-channel input: occupancy + known_flag\n",
        "        self.input_proj = nn.Conv3d(2, d_model, kernel_size=1)\n",
        "\n",
        "        # same transformer as before\n",
        "        self.transformer = Transformer(\n",
        "            num_layers=num_layers,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            window_size=window_size,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        # learned absolute 3D positional embedding inside the window\n",
        "        # shape: [1, ws, ws, ws, d_model]\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, window_size, window_size, window_size, d_model)\n",
        "        )\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "        # output head: from d_model to one logit\n",
        "        self.output_head = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, neighbors_patch, known_mask):\n",
        "        \"\"\"\n",
        "        neighbors_patch: [B, 1, ws, ws, ws] float {0,1}\n",
        "        known_mask:      [B,    ws, ws, ws] float {0,1} (1 = known, 0 = unknown)\n",
        "        \"\"\"\n",
        "        B = neighbors_patch.shape[0]\n",
        "        ws = self.window_size\n",
        "        r = ws // 2\n",
        "\n",
        "        # Hide the true center occupancy during training / inference\n",
        "        # (so model must infer it from neighbors + known_mask)\n",
        "        neighbors_patch = neighbors_patch.clone()\n",
        "        neighbors_patch[:, :, r, r, r] = 0.0\n",
        "\n",
        "        # Build 2-channel input: [occupancy, known_flag]\n",
        "        # known_mask is [B, ws, ws, ws] -> [B,1,ws,ws,ws]\n",
        "        known_flag = known_mask.unsqueeze(1)\n",
        "        x_in = torch.cat([neighbors_patch, known_flag], dim=1)  # [B,2,ws,ws,ws]\n",
        "\n",
        "        # Project to d_model and permute to [B,ws,ws,ws,d_model]\n",
        "        emb = self.input_proj(x_in)               # [B,d_model,ws,ws,ws]\n",
        "        emb = emb.permute(0, 2, 3, 4, 1).contiguous()  # [B,ws,ws,ws,d_model]\n",
        "\n",
        "        # Add learned absolute 3D positional embedding inside the window\n",
        "        emb = emb + self.pos_embed  # auto-broadcast [1,ws,ws,ws,d_model]\n",
        "\n",
        "        # Neighborhood + mask functions (same semantics as before)\n",
        "        def neighborhood_fn_patch(grid, dd, hh, ww, window_size):\n",
        "            B2, Dp, Hp, Wp, C = grid.shape\n",
        "            r = window_size // 2\n",
        "            d0, d1 = max(0, dd - r), min(Dp, dd + r + 1)\n",
        "            h0, h1 = max(0, hh - r), min(Hp, hh + r + 1)\n",
        "            w0, w1 = max(0, ww - r), min(Wp, ww + r + 1)\n",
        "            patch_local = grid[:, d0:d1, h0:h1, w0:w1, :]   # [B,dp,hp,wp,C]\n",
        "\n",
        "            pd0 = max(0, r - dd); pd1 = max(0, (dd + r + 1) - Dp)\n",
        "            ph0 = max(0, r - hh); ph1 = max(0, (hh + r + 1) - Hp)\n",
        "            pw0 = max(0, r - ww); pw1 = max(0, (ww + r + 1) - Wp)\n",
        "            if any([pd0, pd1, ph0, ph1, pw0, pw1]):\n",
        "                tmp = patch_local.permute(0, 4, 1, 2, 3)   # [B,C,d,h,w]\n",
        "                tmp = F.pad(tmp, (pw0, pw1, ph0, ph1, pd0, pd1))\n",
        "                patch_local = tmp.permute(0, 2, 3, 4, 1).contiguous()\n",
        "            return patch_local                             # [B,ws,ws,ws,C]\n",
        "\n",
        "        def mask_fn_patch(Dp, Hp, Wp, dd, hh, ww, window_size):\n",
        "            # Here, known_mask already encodes which voxels are known / visible (1) vs unknown (0),\n",
        "            # with padding / masked-out neighbors already set to 0.\n",
        "            # We use it directly as the attention \"attend mask\".\n",
        "            return known_mask  # [B,ws,ws,ws], numeric 0/1\n",
        "\n",
        "        center_emb = self.transformer(emb, neighborhood_fn_patch, mask_fn_patch)  # [B, d_model]\n",
        "        logits = self.output_head(center_emb)  # [B,1]\n",
        "        return logits\n",
        "\n",
        "def _extract_patch_with_pad_complete(complete_batch, b, d, h, w, ws):\n",
        "    \"\"\"\n",
        "    complete_batch: [B,1,D,H,W] float {0,1}\n",
        "    Returns:\n",
        "      patch_occ:   [1,1,ws,ws,ws]\n",
        "      base_mask:   [1,ws,ws,ws]  (1=in-bounds, 0=pad); center set to 0 later by caller\n",
        "    \"\"\"\n",
        "    _, _, D, H, W = complete_batch.shape\n",
        "    r = ws // 2\n",
        "\n",
        "    d0, d1 = max(0, d-r), min(D, d+r+1)\n",
        "    h0, h1 = max(0, h-r), min(H, h+r+1)\n",
        "    w0, w1 = max(0, w-r), min(W, w+r+1)\n",
        "\n",
        "    patch_occ = complete_batch[b:b+1, :, d0:d1, h0:h1, w0:w1]  # [1,1,dp,hp,wp]\n",
        "    base_mask = torch.ones_like(patch_occ, dtype=patch_occ.dtype)  # [1,1,dp,hp,wp]\n",
        "\n",
        "    # pad to ws on (W,H,D)\n",
        "    pad = (\n",
        "        max(0, r - (w - w0)), max(0, (w + r + 1) - w1),   # W_left, W_right\n",
        "        max(0, r - (h - h0)), max(0, (h + r + 1) - h1),   # H_top, H_bottom\n",
        "        max(0, r - (d - d0)), max(0, (d + r + 1) - d1),   # D_front, D_back\n",
        "    )\n",
        "    if any(pad):\n",
        "        patch_occ  = F.pad(patch_occ,  pad, value=0.0)  # padded voxels are empty values\n",
        "        base_mask  = F.pad(base_mask,  pad, value=0.0)  # padded positions cannot attend\n",
        "\n",
        "    base_mask = base_mask[:, 0]  # [1,ws,ws,ws]\n",
        "    return patch_occ.contiguous(), base_mask.contiguous()\n",
        "\n",
        "def _valid_positions_from_base(base_mask):\n",
        "    \"\"\"Return a list of (z,y,x) valid positions (in-bounds, non-center).\"\"\"\n",
        "    _, ws, _, _ = base_mask.shape\n",
        "    r = ws // 2\n",
        "    valid = (base_mask > 0.5).clone()  # [1,ws,ws,ws]\n",
        "    valid[:, r, r, r] = False          # center never attends\n",
        "    idxs = torch.nonzero(valid[0], as_tuple=False)  # [M,3]\n",
        "    return idxs  # on same device\n",
        "\n",
        "def _attend_mask_from_combo(base_mask, valid_idxs, combo_indices):\n",
        "    \"\"\"\n",
        "    base_mask: [1,ws,ws,ws] with 1=in-bounds\n",
        "    valid_idxs: [M,3] positions eligible to attend/mask\n",
        "    combo_indices: iterable (tuple/list) or 1D LongTensor of indices into valid_idxs to MASK OUT\n",
        "    Returns attend_mask: [1,ws,ws,ws] with 1=attend, 0=masked, center=0\n",
        "    \"\"\"\n",
        "    attend = (base_mask > 0.5).float()  # start with in-bounds as 1\n",
        "\n",
        "    # normalize indices -> 1D LongTensor\n",
        "    if combo_indices is not None:\n",
        "        if isinstance(combo_indices, torch.Tensor):\n",
        "            idx = combo_indices.to(device=valid_idxs.device, dtype=torch.long)\n",
        "        else:\n",
        "            # tuple/list from itertools / random.sample\n",
        "            idx = torch.as_tensor(combo_indices, device=valid_idxs.device, dtype=torch.long)\n",
        "\n",
        "        if idx.numel() > 0:\n",
        "            sel = valid_idxs.index_select(0, idx)  # [k,3]\n",
        "            attend[0, sel[:, 0], sel[:, 1], sel[:, 2]] = 0.0\n",
        "\n",
        "    # force center to 0\n",
        "    ws = base_mask.shape[1]\n",
        "    r  = ws // 2\n",
        "    attend[:, r, r, r] = 0.0\n",
        "    return attend\n",
        "\n",
        "def build_masked_window_batch_complete(\n",
        "    complete_batch: torch.Tensor,   # [B,1,D,H,W], float {0,1}\n",
        "    window_size: int,\n",
        "    *,\n",
        "    stride: int = 1,\n",
        "    mask_ratio: float = 0.2,\n",
        "    max_masks_per_center: int = 8,\n",
        "    max_centers_per_sample: int = None, # Caps how many centers per volume to take (AFTER skipping empty windows)\n",
        "    rng: random.Random = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    SAMPLE-ONLY variant with **per-volume random center sampling**:\n",
        "      - For each volume b:\n",
        "          1) Scan all centers (d,h,w) with given stride.\n",
        "          2) For each center, extract a window; if the window is fully empty (occ_count==0), skip it.\n",
        "          3) Collect all remaining centers into a candidate list.\n",
        "          4) Randomly sample up to `max_centers_per_sample` centers from this list.\n",
        "          5) For each chosen center, sample up to `max_masks_per_center` UNIQUE masking\n",
        "             combinations of size k = round(mask_ratio * M), where M = number of valid neighbors\n",
        "             (non-center, in-bounds).\n",
        "             - If k == 0, emits a single unmasked attend map (center still forced to 0 downstream).\n",
        "\n",
        "    Returns:\n",
        "      patches:      [N,1,ws,ws,ws]\n",
        "      attend_masks: [N,  ws,ws,ws]  (1=attend, 0=masked; center forced 0)\n",
        "      labels:       [N,1]           (center GT from `complete_batch`)\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = random\n",
        "\n",
        "    device = complete_batch.device\n",
        "    B, _, D, H, W = complete_batch.shape\n",
        "    ws = window_size\n",
        "    patches, masks, labels = [], [], []\n",
        "\n",
        "    for b in range(B):\n",
        "        # 1) Gather candidate centers (non-empty windows) for this volume\n",
        "        candidate_centers = []  # list of (d,h,w)\n",
        "        for d in range(0, D, stride):\n",
        "            for h in range(0, H, stride):\n",
        "                for w in range(0, W, stride):\n",
        "                    # Extract window and base mask just to check occ_count\n",
        "                    patch_occ, base_mask = _extract_patch_with_pad_complete(\n",
        "                        complete_batch, b, d, h, w, ws\n",
        "                    )\n",
        "                    occ_count = patch_occ.sum().item()\n",
        "\n",
        "                    # Skip fully empty windows\n",
        "                    if occ_count == 0:\n",
        "                        continue\n",
        "\n",
        "                    candidate_centers.append((d, h, w))\n",
        "\n",
        "        # 2) Randomly subsample candidate centers if max_centers_per_sample is set\n",
        "        if max_centers_per_sample is not None and max_centers_per_sample > 0:\n",
        "            if len(candidate_centers) > max_centers_per_sample:\n",
        "                # random.sample uses Python's RNG; rng can be a Random instance\n",
        "                if isinstance(rng, random.Random):\n",
        "                    chosen_centers = rng.sample(candidate_centers, max_centers_per_sample)\n",
        "                else:\n",
        "                    # fallback to global random if rng is the module\n",
        "                    chosen_centers = random.sample(candidate_centers, max_centers_per_sample)\n",
        "            else:\n",
        "                chosen_centers = candidate_centers\n",
        "        else:\n",
        "            chosen_centers = candidate_centers\n",
        "\n",
        "        # 3) For each chosen center, build patches + masks + labels\n",
        "        for (d, h, w) in chosen_centers:\n",
        "            # Re-extract window & base mask for this center\n",
        "            patch_occ, base_mask = _extract_patch_with_pad_complete(\n",
        "                complete_batch, b, d, h, w, ws\n",
        "            )\n",
        "\n",
        "            # Center label from complete GT (kept as [1,1])\n",
        "            label = complete_batch[b:b+1, :, d:d+1, h:h+1, w:w+1].view(1, 1)  # {0,1}\n",
        "\n",
        "            # Valid neighbor indices (non-center, in-bounds)\n",
        "            valid_idxs = _valid_positions_from_base(base_mask)  # [M,3]\n",
        "            M = valid_idxs.shape[0]\n",
        "            k = int(round(mask_ratio * M))\n",
        "            k = max(0, min(k, M))  # clamp\n",
        "\n",
        "            # Handle k == 0: single attend map with no extra masking\n",
        "            if k == 0:\n",
        "                attend_mask = _attend_mask_from_combo(base_mask, valid_idxs, [])\n",
        "                patches.append(patch_occ)\n",
        "                masks.append(attend_mask)\n",
        "                labels.append(label)\n",
        "                continue\n",
        "\n",
        "            # 4) Sample-only: up to `max_masks_per_center` UNIQUE k-subsets\n",
        "            if max_masks_per_center is None or max_masks_per_center <= 0:\n",
        "                max_masks_per_center = 1\n",
        "\n",
        "            seen = set()\n",
        "            trials = 0\n",
        "            max_trials = max_masks_per_center * 10  # soft cap on retries\n",
        "\n",
        "            while (len(seen) < max_masks_per_center) and (trials < max_trials):\n",
        "                combo = tuple(sorted(rng.sample(range(M), k))) if k > 0 else tuple()\n",
        "                if combo not in seen:\n",
        "                    seen.add(combo)\n",
        "                    attend_mask = _attend_mask_from_combo(base_mask, valid_idxs, combo)\n",
        "                    patches.append(patch_occ)\n",
        "                    masks.append(attend_mask)\n",
        "                    labels.append(label)\n",
        "                trials += 1\n",
        "\n",
        "    # 4) Collate across all volumes\n",
        "    if len(patches) == 0:\n",
        "        return None, None, None\n",
        "\n",
        "    patches = torch.cat(patches, dim=0).to(device)  # [N,1,ws,ws,ws]\n",
        "    masks   = torch.cat(masks,   dim=0).to(device)  # [N,ws,ws,ws]\n",
        "    labels  = torch.cat(labels,  dim=0).to(device)  # [N,1]\n",
        "    return patches, masks, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ca8cd74",
      "metadata": {
        "id": "0ca8cd74"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "baf9ef81",
      "metadata": {
        "id": "baf9ef81"
      },
      "outputs": [],
      "source": [
        "def run_validation(\n",
        "    model: nn.Module,\n",
        "    val_set,\n",
        "    device,\n",
        "    *,\n",
        "    window_size: int = 5,\n",
        "    stride: int = 1,\n",
        "    mask_ratio: float = 0.2,\n",
        "    masks_per_center: int = 4,      # smaller than train for speed\n",
        "    max_centers: int = 64,          # fewer centers per volume than train\n",
        "    pos_weight: float = 1.0,\n",
        "    batch_size: int = 4,            # smaller BS is fine for val\n",
        "    max_batches: int = 50,          # limit how many val volumes you touch\n",
        "):\n",
        "    \"\"\"\n",
        "    Fast validation:\n",
        "      - Uses the same build_masked_window_batch_complete as training.\n",
        "      - But with smaller max_centers, fewer masks_per_center, and a cap on val batches.\n",
        "      - Computes loss, overall acc, fill_acc, empty_acc on a subset of val_set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True\n",
        "    )\n",
        "\n",
        "    pw = torch.tensor([pos_weight], device=device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pw)\n",
        "\n",
        "    loss_sum = 0.0\n",
        "    sample_count = 0\n",
        "    correct_total = 0\n",
        "    total_total = 0\n",
        "\n",
        "    filled_correct = 0\n",
        "    filled_total   = 0\n",
        "    empty_correct  = 0\n",
        "    empty_total    = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i_batch, complete_batch in enumerate(val_loader):\n",
        "            if max_batches is not None and i_batch >= max_batches:\n",
        "                break\n",
        "\n",
        "            # Ensure shapes [B,1,D,H,W] and keep on CPU\n",
        "            if complete_batch.dim() == 4:\n",
        "                complete_batch = complete_batch.unsqueeze(1)\n",
        "            complete_batch = complete_batch.float()\n",
        "\n",
        "            # Build masked-window batch from COMPLETE volumes (CPU tensors)\n",
        "            patches, attend_masks, labels = build_masked_window_batch_complete(\n",
        "                complete_batch,\n",
        "                window_size=window_size,\n",
        "                stride=stride,\n",
        "                mask_ratio=mask_ratio,\n",
        "                max_masks_per_center=masks_per_center,\n",
        "                max_centers_per_sample=max_centers,\n",
        "            )\n",
        "\n",
        "            if patches is None:\n",
        "                continue\n",
        "\n",
        "            PATCH_BATCH = 512\n",
        "            N = patches.size(0)\n",
        "\n",
        "            for start in range(0, N, PATCH_BATCH):\n",
        "                end = min(start + PATCH_BATCH, N)\n",
        "\n",
        "                p = patches[start:end].to(device, non_blocking=True)\n",
        "                m = attend_masks[start:end].to(device, non_blocking=True)\n",
        "                y = labels[start:end].to(device, non_blocking=True)\n",
        "\n",
        "                logits = model(p, m)\n",
        "                loss = criterion(logits, y)\n",
        "\n",
        "                n_chunk = y.numel()\n",
        "                loss_sum += loss.item() * n_chunk\n",
        "                sample_count += n_chunk\n",
        "\n",
        "                # --- accuracy aggregation ---\n",
        "                probs = torch.sigmoid(logits)\n",
        "                preds = (probs >= 0.5).float()\n",
        "\n",
        "                correct_total += (preds == y).sum().item()\n",
        "                total_total   += n_chunk\n",
        "\n",
        "                labels_flat = y.view(-1)\n",
        "                preds_flat  = preds.view(-1)\n",
        "\n",
        "                mask_filled = (labels_flat == 1)\n",
        "                mask_empty  = (labels_flat == 0)\n",
        "\n",
        "                if mask_filled.any():\n",
        "                    filled_correct += (preds_flat[mask_filled] == labels_flat[mask_filled]).sum().item()\n",
        "                    filled_total   += mask_filled.sum().item()\n",
        "\n",
        "                if mask_empty.any():\n",
        "                    empty_correct += (preds_flat[mask_empty] == labels_flat[mask_empty]).sum().item()\n",
        "                    empty_total   += mask_empty.sum().item()\n",
        "\n",
        "    if sample_count == 0:\n",
        "        return {\n",
        "            \"val_loss\": float(\"nan\"),\n",
        "            \"val_acc\": float(\"nan\"),\n",
        "            \"val_fill_acc\": float(\"nan\"),\n",
        "            \"val_empty_acc\": float(\"nan\"),\n",
        "        }\n",
        "\n",
        "    val_loss = loss_sum / sample_count\n",
        "    val_acc  = correct_total / total_total if total_total > 0 else 0.0\n",
        "    val_fill_acc  = filled_correct / filled_total if filled_total > 0 else float('nan')\n",
        "    val_empty_acc = empty_correct  / empty_total  if empty_total  > 0 else float('nan')\n",
        "\n",
        "    return {\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_acc\": val_acc,\n",
        "        \"val_fill_acc\": val_fill_acc,\n",
        "        \"val_empty_acc\": val_empty_acc,\n",
        "    }\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_set,\n",
        "    val_set=None,\n",
        "    *,\n",
        "    num_epochs: int = 20,\n",
        "    batch_size: int = 8,\n",
        "    window_size: int = 5,\n",
        "    lr: float = 1e-4,\n",
        "    weight_decay: float = 0.0,\n",
        "    pos_weight: float = 1.0,\n",
        "    amp: bool = True,\n",
        "    seed: int = 42,\n",
        "    progress: bool = True,\n",
        "    stride: int = 1,\n",
        "    masks_per_center: int = 16,\n",
        "    max_centers: int = None,\n",
        "    mask_ratio: float = 0.2,\n",
        "    # --- new optional args ---\n",
        "    checkpoint_dir: str = None,\n",
        "    val_fast: bool = True,\n",
        "    val_max_centers: int = 64,\n",
        "    val_masks_per_center: int = 4,\n",
        "    val_max_batches: int = 50,\n",
        "):\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"---------------\")\n",
        "    print(f\"Using {max_centers} centers \\n {masks_per_center} masks\")\n",
        "    print(\"Started training...\")\n",
        "    print(device)\n",
        "    print()\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True\n",
        "    )\n",
        "    pw = torch.tensor([pos_weight], device=device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pw)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scaler = GradScaler(enabled=amp)\n",
        "\n",
        "    if checkpoint_dir is not None:\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_ckpt_path = None\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "\n",
        "        # --- epoch-level accumulators (sample-weighted) ---\n",
        "        loss_sum = 0.0\n",
        "        sample_count = 0\n",
        "        correct_total = 0\n",
        "        total_total = 0\n",
        "\n",
        "        filled_correct = 0\n",
        "        filled_total   = 0\n",
        "        empty_correct  = 0\n",
        "        empty_total    = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\", disable=not progress)\n",
        "        for complete_batch in pbar:\n",
        "            # Ensure shapes [B,1,D,H,W] and keep on CPU\n",
        "            if complete_batch.dim() == 4:\n",
        "                complete_batch = complete_batch.unsqueeze(1)\n",
        "            complete_batch = complete_batch.float()\n",
        "\n",
        "            # Build masked-window batch from COMPLETE volumes (CPU tensors)\n",
        "            patches, attend_masks, labels = build_masked_window_batch_complete(\n",
        "                complete_batch,\n",
        "                window_size=window_size,\n",
        "                stride=stride,\n",
        "                mask_ratio=mask_ratio,\n",
        "                max_masks_per_center=masks_per_center,\n",
        "                max_centers_per_sample=max_centers,\n",
        "            )\n",
        "\n",
        "            if patches is None:\n",
        "                continue\n",
        "\n",
        "            PATCH_BATCH = 512  # how many patches you want in GPU at once\n",
        "            N = patches.size(0)\n",
        "\n",
        "            for start in range(0, N, PATCH_BATCH):\n",
        "                end = min(start + PATCH_BATCH, N)\n",
        "\n",
        "                p = patches[start:end].to(device, non_blocking=True)\n",
        "                m = attend_masks[start:end].to(device, non_blocking=True)\n",
        "                y = labels[start:end].to(device, non_blocking=True)\n",
        "\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                with autocast(device_type='cuda', enabled=amp):\n",
        "                    logits = model(p, m)              # [patch_batch, 1]\n",
        "                    loss = criterion(logits, y)       # scalar for this patch chunk\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                # --- sample-weighted loss aggregation ---\n",
        "                n_chunk = y.numel()\n",
        "                loss_sum += loss.item() * n_chunk\n",
        "                sample_count += n_chunk\n",
        "\n",
        "                # --- accuracy aggregation ---\n",
        "                with torch.no_grad():\n",
        "                    probs = torch.sigmoid(logits)\n",
        "                    preds = (probs >= 0.5).float()\n",
        "\n",
        "                    correct_total += (preds == y).sum().item()\n",
        "                    total_total   += n_chunk\n",
        "\n",
        "                    labels_flat = y.view(-1)\n",
        "                    preds_flat  = preds.view(-1)\n",
        "\n",
        "                    mask_filled = (labels_flat == 1)\n",
        "                    mask_empty  = (labels_flat == 0)\n",
        "\n",
        "                    if mask_filled.any():\n",
        "                        filled_correct += (preds_flat[mask_filled] == labels_flat[mask_filled]).sum().item()\n",
        "                        filled_total   += mask_filled.sum().item()\n",
        "\n",
        "                    if mask_empty.any():\n",
        "                        empty_correct += (preds_flat[mask_empty] == labels_flat[mask_empty]).sum().item()\n",
        "                        empty_total   += mask_empty.sum().item()\n",
        "\n",
        "            # --- update tqdm ONCE per complete_batch, using epoch-level stats so far ---\n",
        "            avg_loss = loss_sum / max(sample_count, 1)\n",
        "            avg_acc  = correct_total / max(total_total, 1)\n",
        "            fill_acc = filled_correct / filled_total if filled_total > 0 else float('nan')\n",
        "            empty_acc = empty_correct / empty_total if empty_total > 0 else float('nan')\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                \"loss\": f\"{avg_loss:.4f}\",\n",
        "                \"acc\": f\"{avg_acc:.3f}\",\n",
        "                \"fill_acc\": f\"{fill_acc:.3f}\",\n",
        "                \"empty_acc\": f\"{empty_acc:.3f}\",\n",
        "            })\n",
        "\n",
        "        # --- end of epoch: compute final train metrics ---\n",
        "        if sample_count > 0:\n",
        "            epoch_loss = loss_sum / sample_count\n",
        "            epoch_acc  = correct_total / total_total if total_total > 0 else 0.0\n",
        "            fill_acc   = filled_correct / filled_total if filled_total > 0 else float('nan')\n",
        "            empty_acc  = empty_correct / empty_total if empty_total > 0 else float('nan')\n",
        "        else:\n",
        "            epoch_loss = float(\"nan\")\n",
        "            epoch_acc  = float(\"nan\")\n",
        "            fill_acc   = float(\"nan\")\n",
        "            empty_acc  = float(\"nan\")\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch}: \"\n",
        "            f\"train_loss = {epoch_loss:.4f} | \"\n",
        "            f\"train_acc = {epoch_acc:.4f} | \"\n",
        "            f\"fill_acc = {fill_acc:.4f} | \"\n",
        "            f\"empty_acc = {empty_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # --- quick validation ---\n",
        "        val_stats = None\n",
        "        if val_set is not None and val_fast:\n",
        "            val_stats = run_validation(\n",
        "                model,\n",
        "                val_set,\n",
        "                device,\n",
        "                window_size=window_size,\n",
        "                stride=stride,\n",
        "                mask_ratio=mask_ratio,\n",
        "                masks_per_center=val_masks_per_center,\n",
        "                max_centers=val_max_centers,\n",
        "                pos_weight=pos_weight,\n",
        "                batch_size=batch_size,      # reuse train BS, or make smaller\n",
        "                max_batches=val_max_batches,\n",
        "            )\n",
        "            print(\n",
        "                f\"  [VAL] loss = {val_stats['val_loss']:.4f} | \"\n",
        "                f\"acc = {val_stats['val_acc']:.4f} | \"\n",
        "                f\"fill_acc = {val_stats['val_fill_acc']:.4f} | \"\n",
        "                f\"empty_acc = {val_stats['val_empty_acc']:.4f}\"\n",
        "            )\n",
        "\n",
        "        # --- checkpointing ---\n",
        "        if checkpoint_dir is not None:\n",
        "            ckpt = {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"scaler_state_dict\": scaler.state_dict(),\n",
        "                \"train_loss\": epoch_loss,\n",
        "                \"train_acc\": epoch_acc,\n",
        "                \"train_fill_acc\": fill_acc,\n",
        "                \"train_empty_acc\": empty_acc,\n",
        "            }\n",
        "            if val_stats is not None:\n",
        "                ckpt.update(val_stats)\n",
        "\n",
        "            ckpt_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "            torch.save(ckpt, ckpt_path)\n",
        "            print(f\"Saved checkpoint to {ckpt_path}\")\n",
        "\n",
        "            # Track best by val_loss (if val is available)\n",
        "            if val_stats is not None:\n",
        "                if val_stats[\"val_loss\"] < best_val_loss:\n",
        "                    best_val_loss = val_stats[\"val_loss\"]\n",
        "                    best_ckpt_path = os.path.join(checkpoint_dir, \"best_checkpoint.pth\")\n",
        "                    torch.save(ckpt, best_ckpt_path)\n",
        "                    print(f\"Updated best checkpoint: {best_ckpt_path}\")\n",
        "\n",
        "    if checkpoint_dir is not None and best_ckpt_path is not None:\n",
        "        print(f\"Best checkpoint: {best_ckpt_path} with val_loss = {best_val_loss:.4f}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca85b063",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca85b063",
        "outputId": "33da3228-dad6-42b0-8b06-405fd76cf640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unzip complete. New/updated files extracted: 0\n",
            "Total dataset size: 256571\n",
            "Caching train subset into RAM...\n",
            "Caching val subset into RAM...\n",
            "Training samples: 10262 | Validation samples: 2566\n",
            "Train List Length: 10262\n",
            "Saved training file list to: ./train_list.txt\n",
            "Training samples: 10262 | Validation samples: 2566\n",
            "---------------\n",
            "Using 128 centers \n",
            " 8 masks\n",
            "Started training...\n",
            "cuda\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/4: 100%|| 1283/1283 [1:32:47<00:00,  4.34s/it, loss=0.0678, acc=0.973, fill_acc=0.852, empty_acc=0.986]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: train_loss = 0.0678 | train_acc = 0.9731 | fill_acc = 0.8516 | empty_acc = 0.9862\n",
            "  [VAL] loss = 0.0492 | acc = 0.9808 | fill_acc = 0.9229 | empty_acc = 0.9865\n",
            "Saved checkpoint to ./checkpoints_it/checkpoint_epoch_1.pth\n",
            "Updated best checkpoint: ./checkpoints_it/best_checkpoint.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/4:  13%|        | 168/1283 [12:13<1:20:30,  4.33s/it, loss=0.0526, acc=0.979, fill_acc=0.902, empty_acc=0.988]"
          ]
        }
      ],
      "source": [
        "# ------------------------------\n",
        "# Example main (instantiate and run)\n",
        "# ------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    ZIP_PATH   = \"/content/drive/MyDrive/AUB_masters/thesis/data/chunk_data_16_flood_fill_rm_20.zip\"\n",
        "    UNZIP_PATH = \"./chunk_data_16_flood_fill_rm_20\"\n",
        "    train_list = \"./train_list.txt\"\n",
        "    extracted = unzip_to_dir(ZIP_PATH, UNZIP_PATH, overwrite=False)\n",
        "    print(f\"Unzip complete. New/updated files extracted: {extracted}\")\n",
        "\n",
        "    # 2) Build dataset/loaders from the directory (fast & worker-safe)\n",
        "    dataset = VoxelDataset(UNZIP_PATH)\n",
        "    print(f\"Total dataset size: {len(dataset)}\")\n",
        "\n",
        "    n = len(dataset)\n",
        "    indices = list(range(n))\n",
        "    random.Random(42).shuffle(indices)\n",
        "\n",
        "    DATA_PER   = 0.05\n",
        "    TRAIN_FRAC = 0.8    # within that DATA_PER%, use 80% for training, 20% for validation\n",
        "\n",
        "    # Take a subset of the dataset = 5% of total\n",
        "    n_subset = max(1, int(n * DATA_PER))\n",
        "    subset_indices = indices[:n_subset]\n",
        "\n",
        "    # Split that subset into train / val\n",
        "    n_train = max(1, int(n_subset * TRAIN_FRAC))\n",
        "    train_idx = subset_indices[:n_train]\n",
        "    val_idx   = subset_indices[n_train:]\n",
        "\n",
        "    # (Optional) the rest of the dataset could be considered \"test\"\n",
        "    test_idx = indices[n_subset:]\n",
        "\n",
        "    # train_set = torch.utils.data.Subset(dataset, train_idx)\n",
        "    # val_set   = torch.utils.data.Subset(dataset, val_idx)\n",
        "    print(\"Caching train subset into RAM...\")\n",
        "    train_set = CachedVoxelSubset(dataset, train_idx)\n",
        "    print(\"Caching val subset into RAM...\")\n",
        "    val_set   = CachedVoxelSubset(dataset, val_idx)\n",
        "\n",
        "    print(f\"Training samples: {len(train_idx)} | Validation samples: {len(val_idx)}\")\n",
        "\n",
        "    # === Save list of training file names ===\n",
        "    npz_files = dataset.files.npz_files\n",
        "    train_files = [npz_files[i] for i in train_idx]\n",
        "\n",
        "    os.makedirs(os.path.dirname(train_list), exist_ok=True)\n",
        "    with open(train_list, \"w\") as f:\n",
        "        for name in train_files:\n",
        "            f.write(name + \"\\n\")\n",
        "\n",
        "    print(f\"Train List Length: {len(train_files)}\")\n",
        "    print(f\"Saved training file list to: {train_list}\")\n",
        "    print(f\"Training samples: {len(train_idx)} | Validation samples: {len(val_idx)}\")\n",
        "    # ========================================\n",
        "\n",
        "    # === Model parameters ===\n",
        "    WINDOW_SIZE = 11\n",
        "    D_MODEL = 64\n",
        "    NUM_HEADS = 4\n",
        "    NUM_LAYERS = 4\n",
        "    DROPOUT = 0.1\n",
        "    GRID_SIZE = 16\n",
        "    # ========================\n",
        "    model = IterativeVoxelModel(\n",
        "        d_model=D_MODEL,\n",
        "        num_heads=NUM_HEADS,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        window_size=WINDOW_SIZE,\n",
        "        dropout=DROPOUT,\n",
        "        max_grid_size=GRID_SIZE,\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # === Training parameters ===\n",
        "    EPOCHS = 4\n",
        "    BATCH_SIZE = 8\n",
        "    MASKS_PER_CENTER = 8\n",
        "    MAX_CENTERS = 128\n",
        "    # ===========================\n",
        "    checkpoint_dir = \"./checkpoints_it\"\n",
        "\n",
        "    # train(\n",
        "    #     model,\n",
        "    #     train_set,\n",
        "    #     num_epochs=EPOCHS,\n",
        "    #     batch_size=BATCH_SIZE,\n",
        "    #     window_size=WINDOW_SIZE,\n",
        "    #     masks_per_center=MASKS_PER_CENTER,\n",
        "    #     max_centers=MAX_CENTERS,\n",
        "    # )\n",
        "\n",
        "    train(\n",
        "    model,\n",
        "    train_set,\n",
        "    val_set=val_set,\n",
        "    num_epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    window_size=WINDOW_SIZE,\n",
        "    masks_per_center=MASKS_PER_CENTER,\n",
        "    max_centers=MAX_CENTERS,\n",
        "    pos_weight=1.0,               # or >1 later\n",
        "    checkpoint_dir=checkpoint_dir,\n",
        "    val_fast=True,\n",
        "    val_max_centers=64,           # cheap val\n",
        "    val_masks_per_center=4,\n",
        "    val_max_batches=50,           # at most 50 val batches per epoch\n",
        ")\n",
        "\n",
        "    MODEL_SAVE_PATH = \"./it.pth\"\n",
        "    torch.save({'model_state_dict': model.state_dict()}, MODEL_SAVE_PATH)\n",
        "    print(f\"Model saved to {MODEL_SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UkAGx138Ky0p",
      "metadata": {
        "id": "UkAGx138Ky0p"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "3e95cfa5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_numpy(x):\n",
        "    if isinstance(x, np.ndarray):\n",
        "        return x\n",
        "    return x.detach().cpu().numpy()\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "\n",
        "def to_numpy(x):\n",
        "    \"\"\"Convert PyTorch tensor  NumPy array safely.\"\"\"\n",
        "    if isinstance(x, np.ndarray):\n",
        "        return x\n",
        "    return x.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def _plot_voxels(grid, colors, out_path, title=None):\n",
        "    \"\"\"\n",
        "    Internal helper to plot a 3D voxel grid with per-voxel RGBA colors.\n",
        "    grid   : [D,H,W] boolean or 0/1 occupancy (NumPy)\n",
        "    colors : [D,H,W,4] RGBA numeric NumPy\n",
        "    \"\"\"\n",
        "    grid = to_numpy(grid)\n",
        "    colors = to_numpy(colors)\n",
        "\n",
        "    D, H, W = grid.shape\n",
        "\n",
        "    fig = plt.figure(figsize=(6, 6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.voxels(grid.astype(bool), facecolors=colors, edgecolor='k')\n",
        "\n",
        "    ax.set_xlim(0, D)\n",
        "    ax.set_ylim(0, H)\n",
        "    ax.set_zlim(0, W)\n",
        "\n",
        "    try:\n",
        "        ax.set_box_aspect([1, 1, 1])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"[VIS] Saved {out_path}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) FULL OBJECT (GT)\n",
        "# ------------------------------------------------------------\n",
        "def visualize_full_object(voxel_grid, out_dir, name=\"gt\"):\n",
        "    \"\"\"\n",
        "    voxel_grid: [D,H,W] or [1,1,D,H,W] or [1,D,H,W]\n",
        "    All GT filled voxels shown in blue.\n",
        "    \"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    vg = to_numpy(voxel_grid)\n",
        "\n",
        "    if vg.ndim == 5:\n",
        "        vg = vg[0, 0]\n",
        "    elif vg.ndim == 4:\n",
        "        vg = vg[0]\n",
        "\n",
        "    gt_filled = vg > 0.5\n",
        "\n",
        "    colors = np.zeros((*vg.shape, 4))\n",
        "    colors[gt_filled] = [0.0, 0.7, 1.0, 1.0]  # blue\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"{name}_full.png\")\n",
        "    _plot_voxels(vg, colors, out_path, title=\"Full Object (GT)\")\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) MASKED OBJECT (masked voxels = red)\n",
        "# ------------------------------------------------------------\n",
        "def visualize_masked_object(complete_gt, masked_coords, out_dir, name=\"masked\"):\n",
        "    \"\"\"\n",
        "    complete_gt   : [1,1,D,H,W] or [1,D,H,W] or [D,H,W]\n",
        "    masked_coords : Tensor [N,3] or list of (z,y,x)\n",
        "\n",
        "    - GT voxels: blue\n",
        "    - Masked ON-object: solid red\n",
        "    - Masked OFF-object: very transparent dark red (halo)\n",
        "    \"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    gt = to_numpy(complete_gt)\n",
        "    if gt.ndim == 5:\n",
        "        gt = gt[0, 0]\n",
        "    elif gt.ndim == 4:\n",
        "        gt = gt[0]\n",
        "\n",
        "    D, H, W = gt.shape\n",
        "    gt_filled = gt > 0.5\n",
        "\n",
        "    # Mask map\n",
        "    is_masked = np.zeros((D, H, W), dtype=bool)\n",
        "    if isinstance(masked_coords, torch.Tensor):\n",
        "        coords = masked_coords.detach().cpu().numpy()\n",
        "    else:\n",
        "        coords = np.array(masked_coords, dtype=np.int64)\n",
        "    for zz, yy, xx in coords:\n",
        "        if 0 <= zz < D and 0 <= yy < H and 0 <= xx < W:\n",
        "            is_masked[int(zz), int(yy), int(xx)] = True\n",
        "\n",
        "    masked_on_object  = is_masked & gt_filled\n",
        "    masked_off_object = is_masked & ~gt_filled\n",
        "\n",
        "    # We want to show all GT filled + all masked centers\n",
        "    grid = gt_filled | is_masked\n",
        "\n",
        "    colors = np.zeros((D, H, W, 4))\n",
        "\n",
        "    # GT = blue\n",
        "    colors[gt_filled] = [0.0, 0.7, 1.0, 1.0]\n",
        "\n",
        "    # ON-object masked = solid red\n",
        "    colors[masked_on_object] = [1.0, 0.0, 0.0, 1.0]\n",
        "\n",
        "    # OFF-object masked = ultra transparent dark red (ONLY place with transparency)\n",
        "    colors[masked_off_object] = [0.3, 0.0, 0.0, 0.003]\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"{name}_masked.png\")\n",
        "    _plot_voxels(grid, colors, out_path, title=\"Masked (GT=blue, masked red; halo faint)\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) PREDICTION QUALITY (correct=green, incorrect=red)\n",
        "# ------------------------------------------------------------\n",
        "def visualize_predictions(complete_gt, pred_grid, masked_coords, out_dir, name=\"pred_quality\"):\n",
        "    \"\"\"\n",
        "    complete_gt   : [1,1,D,H,W] or [1,D,H,W] or [D,H,W]\n",
        "    pred_grid     : same shape, binary prediction\n",
        "    masked_coords : Tensor [N,3] or list of (z,y,x)\n",
        "\n",
        "    - Unmasked GT voxels: blue\n",
        "    - Masked & predicted correctly (on or off object): solid green\n",
        "    - Masked & predicted incorrectly (on or off object): solid red\n",
        "    \"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    gt = to_numpy(complete_gt)\n",
        "    pr = to_numpy(pred_grid)\n",
        "\n",
        "    if gt.ndim == 5:\n",
        "        gt = gt[0, 0]\n",
        "        pr = pr[0, 0]\n",
        "    elif gt.ndim == 4:\n",
        "        gt = gt[0]\n",
        "        pr = pr[0]\n",
        "\n",
        "    D, H, W = gt.shape\n",
        "\n",
        "    gt_filled = gt > 0.5\n",
        "\n",
        "    # Masked centers\n",
        "    is_masked = np.zeros((D, H, W), dtype=bool)\n",
        "    if isinstance(masked_coords, torch.Tensor):\n",
        "        coords = masked_coords.detach().cpu().numpy()\n",
        "    else:\n",
        "        coords = np.array(masked_coords, dtype=np.int64)\n",
        "    for zz, yy, xx in coords:\n",
        "        if 0 <= zz < D and 0 <= yy < H and 0 <= xx < W:\n",
        "            is_masked[int(zz), int(yy), int(xx)] = True\n",
        "\n",
        "    correct   = (gt == pr)\n",
        "    incorrect = ~correct\n",
        "\n",
        "    masked_correct = is_masked & correct\n",
        "    masked_wrong   = is_masked & incorrect\n",
        "\n",
        "    # Show union of GT and prediction to see full shape\n",
        "    grid = ((gt + pr) > 0).astype(bool)\n",
        "\n",
        "    colors = np.zeros((D, H, W, 4))\n",
        "\n",
        "    # Base GT structure = blue\n",
        "    colors[gt_filled] = [0.0, 0.7, 1.0, 1.0]\n",
        "\n",
        "    # Masked centers (on or off object)  solid, no transparency\n",
        "    colors[masked_correct] = [0.0, 1.0, 0.0, 1.0]  # green\n",
        "    colors[masked_wrong]   = [1.0, 0.0, 0.0, 1.0]  # red\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"{name}_pred_quality.png\")\n",
        "    _plot_voxels(\n",
        "        grid,\n",
        "        colors,\n",
        "        out_path,\n",
        "        title=\"Prediction Quality (blue=GT, green=correct, red=wrong masked)\",\n",
        "    )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "sxGVW3cZKwug",
      "metadata": {
        "id": "sxGVW3cZKwug"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1) Utils: Tensor -> numpy\n",
        "# =========================\n",
        "def to_numpy(x):\n",
        "    if isinstance(x, np.ndarray):\n",
        "        return x\n",
        "    return x.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 2) Visualization helpers\n",
        "# =========================\n",
        "def _plot_voxels(grid, colors, out_path, title=None):\n",
        "    \"\"\"\n",
        "    grid   : [D,H,W] occupancy, NumPy or Tensor\n",
        "    colors : [D,H,W,4] RGBA, NumPy or Tensor\n",
        "    \"\"\"\n",
        "    grid = to_numpy(grid)\n",
        "    colors = to_numpy(colors)\n",
        "\n",
        "    D, H, W = grid.shape\n",
        "\n",
        "    fig = plt.figure(figsize=(6, 6))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "    # very light grey borders instead of black\n",
        "    ax.voxels(grid.astype(bool), facecolors=colors, edgecolor=(0.85, 0.85, 0.85, 0.4))\n",
        "\n",
        "    ax.set_xlim(0, D)\n",
        "    ax.set_ylim(0, H)\n",
        "    ax.set_zlim(0, W)\n",
        "\n",
        "    try:\n",
        "        ax.set_box_aspect([1, 1, 1])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"[VIS] Saved {out_path}\")\n",
        "\n",
        "def visualize_gt_vs_pred_side_by_side(complete_gt, pred_grid, out_dir, name=\"gt_vs_pred\"):\n",
        "    \"\"\"\n",
        "    Save a single PNG with:\n",
        "      - Left:  Ground truth (only GT filled voxels in blue)\n",
        "      - Right: Prediction\n",
        "          * Predicted voxels = blue\n",
        "          * Voxels where GT != pred = yellow\n",
        "\n",
        "    No transparent voxels in either view.\n",
        "    \"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    gt = to_numpy(complete_gt)\n",
        "    pr = to_numpy(pred_grid)\n",
        "\n",
        "    if gt.ndim == 5:\n",
        "        gt = gt[0, 0]\n",
        "        pr = pr[0, 0]\n",
        "    elif gt.ndim == 4:\n",
        "        gt = gt[0]\n",
        "        pr = pr[0]\n",
        "\n",
        "    D, H, W = gt.shape\n",
        "\n",
        "    gt_filled = gt > 0.5\n",
        "    pr_filled = pr > 0.5\n",
        "    diff = (gt_filled != pr_filled)\n",
        "\n",
        "    # --- GT view: only GT voxels drawn ---\n",
        "    grid_gt = gt_filled.copy()\n",
        "    colors_gt = np.zeros((D, H, W, 4))\n",
        "    colors_gt[gt_filled] = [0.0, 0.7, 1.0, 1.0]    # blue\n",
        "\n",
        "    # --- Pred view: only predicted/diff voxels drawn ---\n",
        "    grid_pr = pr_filled | diff\n",
        "    colors_pr = np.zeros((D, H, W, 4))\n",
        "    colors_pr[pr_filled] = [0.0, 0.7, 1.0, 1.0]    # blue\n",
        "    colors_pr[diff]      = [1.0, 1.0, 0.0, 1.0]    # yellow\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Left: GT\n",
        "    ax1 = fig.add_subplot(121, projection=\"3d\")\n",
        "    ax1.voxels(\n",
        "        grid_gt.astype(bool),\n",
        "        facecolors=colors_gt,\n",
        "        edgecolor=(0.85, 0.85, 0.85, 0.4),\n",
        "    )\n",
        "    ax1.set_title(\"Ground Truth\")\n",
        "    ax1.set_xlim(0, D); ax1.set_ylim(0, H); ax1.set_zlim(0, W)\n",
        "    try:\n",
        "        ax1.set_box_aspect([1, 1, 1])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Right: Prediction\n",
        "    ax2 = fig.add_subplot(122, projection=\"3d\")\n",
        "    ax2.voxels(\n",
        "        grid_pr.astype(bool),\n",
        "        facecolors=colors_pr,\n",
        "        edgecolor=(0.85, 0.85, 0.85, 0.4),\n",
        "    )\n",
        "    ax2.set_title(\"Prediction (yellow = difference)\")\n",
        "    ax2.set_xlim(0, D); ax2.set_ylim(0, H); ax2.set_zlim(0, W)\n",
        "    try:\n",
        "        ax2.set_box_aspect([1, 1, 1])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    plt.tight_layout()\n",
        "    out_path = os.path.join(out_dir, f\"{name}_gt_vs_pred.png\")\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"[VIS] Saved {out_path}\")\n",
        "\n",
        "\n",
        "def visualize_full_object(voxel_grid, out_dir, name=\"gt\"):\n",
        "    \"\"\"\n",
        "    voxel_grid: [D,H,W] or [1,1,D,H,W] or [1,D,H,W]\n",
        "    GT voxels always blue.\n",
        "    \"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    vg = to_numpy(voxel_grid)\n",
        "\n",
        "    if vg.ndim == 5:\n",
        "        vg = vg[0, 0]\n",
        "    elif vg.ndim == 4:\n",
        "        vg = vg[0]\n",
        "\n",
        "    colors = np.zeros((*vg.shape, 4))\n",
        "    # GT full voxels in blue\n",
        "    colors[vg > 0.5] = [0.0, 0.7, 1.0, 1.0]\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"{name}_full.png\")\n",
        "    _plot_voxels(vg, colors, out_path, title=\"Full Object (GT)\")\n",
        "\n",
        "\n",
        "def visualize_masked_object(complete_gt, masked_coords, out_dir, name=\"masked\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    gt = to_numpy(complete_gt)\n",
        "    if gt.ndim == 5:\n",
        "        gt = gt[0, 0]\n",
        "    elif gt.ndim == 4:\n",
        "        gt = gt[0]\n",
        "\n",
        "    D, H, W = gt.shape\n",
        "\n",
        "    # Mask array\n",
        "    is_masked = np.zeros((D, H, W), dtype=bool)\n",
        "    coords = masked_coords.detach().cpu().numpy() if isinstance(masked_coords, torch.Tensor) else np.array(masked_coords)\n",
        "    for zz, yy, xx in coords:\n",
        "        is_masked[int(zz), int(yy), int(xx)] = True\n",
        "\n",
        "    gt_filled = gt > 0.5\n",
        "    masked_on_object  = is_masked & gt_filled\n",
        "    masked_off_object = is_masked & ~gt_filled\n",
        "\n",
        "    grid = gt_filled | is_masked\n",
        "\n",
        "    colors = np.zeros((D, H, W, 4))\n",
        "\n",
        "    # GT = blue\n",
        "    colors[gt_filled] = [0.0, 0.7, 1.0, 1.0]\n",
        "\n",
        "    # ON-object masked = solid red\n",
        "    colors[masked_on_object] = [1.0, 0.0, 0.0, 1.0]\n",
        "\n",
        "    # OFF-object masked = ULTRA transparent dark red\n",
        "    colors[masked_off_object] = [0.3, 0.0, 0.0, 0.003]   # <--- fix\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"{name}_masked.png\")\n",
        "    _plot_voxels(grid, colors, out_path, title=\"Masked Object\")\n",
        "\n",
        "def visualize_predictions(complete_gt, pred_grid, masked_coords, out_dir, name=\"pred_quality\"):\n",
        "    \"\"\"\n",
        "    complete_gt   : [1,1,D,H,W] or [1,D,H,W] or [D,H,W]\n",
        "    pred_grid     : same shape, binary prediction\n",
        "    masked_coords : Tensor [N,3] or list of (z,y,x)\n",
        "\n",
        "    - Unmasked GT voxels: blue\n",
        "    - Masked ON-object & predicted correctly (GT=1, pred=1): green\n",
        "    - Masked ON-object & predicted incorrectly (GT=1, pred=0): red\n",
        "    - Masked OFF-object & predicted incorrectly (GT=0, pred=1): red\n",
        "    - Masked OFF-object & predicted correctly (GT=0, pred=0): not drawn\n",
        "    \"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    gt = to_numpy(complete_gt)\n",
        "    pr = to_numpy(pred_grid)\n",
        "\n",
        "    if gt.ndim == 5:\n",
        "        gt = gt[0, 0]\n",
        "        pr = pr[0, 0]\n",
        "    elif gt.ndim == 4:\n",
        "        gt = gt[0]\n",
        "        pr = pr[0]\n",
        "\n",
        "    D, H, W = gt.shape\n",
        "\n",
        "    gt_filled = gt > 0.5\n",
        "    pred_filled = pr > 0.5\n",
        "\n",
        "    # Masked centers\n",
        "    is_masked = np.zeros((D, H, W), dtype=bool)\n",
        "    if isinstance(masked_coords, torch.Tensor):\n",
        "        coords = masked_coords.detach().cpu().numpy()\n",
        "    else:\n",
        "        coords = np.array(masked_coords, dtype=np.int64)\n",
        "    for zz, yy, xx in coords:\n",
        "        if 0 <= zz < D and 0 <= yy < H and 0 <= xx < W:\n",
        "            is_masked[int(zz), int(yy), int(xx)] = True\n",
        "\n",
        "    correct   = (gt_filled == pred_filled)\n",
        "    incorrect = ~correct\n",
        "\n",
        "    # Split masked into on/off object\n",
        "    masked_on_object  = is_masked & gt_filled\n",
        "    masked_off_object = is_masked & ~gt_filled\n",
        "\n",
        "    # Apply your rules\n",
        "    masked_correct_on  = masked_on_object  & correct           # GT=1, pred=1\n",
        "    masked_wrong_on    = masked_on_object  & incorrect         # GT=1, pred=0\n",
        "    masked_wrong_off   = masked_off_object & incorrect         # GT=0, pred=1\n",
        "    # masked_correct_off (GT=0, pred=0)  invisible, not drawn\n",
        "\n",
        "    # Voxels we actually draw:\n",
        "    # - any GT filled voxel\n",
        "    # - any masked center that is on-object (correct or wrong)\n",
        "    # - any masked OFF-object with a wrong prediction\n",
        "    grid = gt_filled | masked_correct_on | masked_wrong_on | masked_wrong_off\n",
        "\n",
        "    colors = np.zeros((D, H, W, 4))\n",
        "\n",
        "    # Base: GT structure = blue\n",
        "    colors[gt_filled] = [0.0, 0.7, 1.0, 1.0]\n",
        "\n",
        "    # Masked ON-object:\n",
        "    colors[masked_correct_on] = [0.0, 1.0, 0.0, 1.0]  # green\n",
        "    colors[masked_wrong_on]   = [1.0, 0.0, 0.0, 1.0]  # red\n",
        "\n",
        "    # Masked OFF-object, wrong (GT=0, pred=1): red\n",
        "    colors[masked_wrong_off]  = [1.0, 0.0, 0.0, 1.0]\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"{name}_pred_quality.png\")\n",
        "    _plot_voxels(\n",
        "        grid,\n",
        "        colors,\n",
        "        out_path,\n",
        "        title=\"Prediction Quality (blue=GT, green=correct, red=wrong)\",\n",
        "    )\n",
        "\n",
        "\n",
        "# =======================================\n",
        "# 3) Rebuild full predicted voxel grid\n",
        "# =======================================\n",
        "def reconstruct_predicted_grid(complete_gt, masked_coords, logits_for_masked):\n",
        "    \"\"\"\n",
        "    complete_gt      : [1,1,D,H,W] tensor\n",
        "    masked_coords    : Tensor [N,3] or list of (z,y,x)\n",
        "    logits_for_masked: [N,1] tensor (raw logits from model)\n",
        "    Returns:\n",
        "        pred_grid [1,1,D,H,W] float tensor in {0,1}\n",
        "    \"\"\"\n",
        "    pred_grid = complete_gt.clone()\n",
        "\n",
        "    if isinstance(masked_coords, torch.Tensor):\n",
        "        coords = masked_coords.detach().cpu().numpy()\n",
        "    else:\n",
        "        coords = np.array(masked_coords, dtype=np.int64)\n",
        "\n",
        "    probs = torch.sigmoid(logits_for_masked).view(-1).detach().cpu()\n",
        "    preds = (probs >= 0.5).float().numpy()  # [N]\n",
        "\n",
        "    for i, (zz, yy, xx) in enumerate(coords):\n",
        "        pred_grid[0, 0, int(zz), int(yy), int(xx)] = float(preds[i])\n",
        "\n",
        "    return pred_grid\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 4) Inference: random objects, mask near-object voxels\n",
        "# ======================================================\n",
        "def run_inference_random_mask_from_zip_with_viz(\n",
        "    model: torch.nn.Module,\n",
        "    zip_path: str,\n",
        "    train_list_path: str,\n",
        "    window_size: int,\n",
        "    mask_fraction: float = 0.2,\n",
        "    max_files: int = 3,\n",
        "    patch_batch_size: int = 512,\n",
        "    device: torch.device = None,\n",
        "    out_dir: str = \"./vis_infer\",\n",
        "):\n",
        "    \"\"\"\n",
        "    - Randomly samples NPZs from `zip_path` whose basenames are NOT in `train_list_path`.\n",
        "    - For each selected volume:\n",
        "        * Loads 'complete' [D,H,W].\n",
        "        * Builds complete_gt [1,1,D,H,W].\n",
        "        * Finds voxels \"near the object\": centers whose ws^3 window has at least 1 filled voxel.\n",
        "        * Randomly masks `mask_fraction` of those centers:\n",
        "              occ -> 0, known -> 0.\n",
        "        * Predicts all masked voxels using model(neighbors_patch, known_mask).\n",
        "        * Reconstructs full predicted grid `pred_grid`.\n",
        "        * Visualizes:\n",
        "              - full GT (blue),\n",
        "              - masked object (GT blue, masked red),\n",
        "              - prediction quality (blue, green, red as described).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # --- read train basenames ---\n",
        "    train_basenames = set()\n",
        "    with open(train_list_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            p = line.strip()\n",
        "            if not p:\n",
        "                continue\n",
        "            base = os.path.basename(p)\n",
        "            train_basenames.add(base)\n",
        "    print(f\"[INF] Loaded {len(train_basenames)} train basenames from {train_list_path}\")\n",
        "\n",
        "    # --- scan zip ---\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        all_members = [m for m in zf.infolist() if m.filename.lower().endswith(\".npz\")]\n",
        "\n",
        "        test_members = []\n",
        "        for m in all_members:\n",
        "            base = os.path.basename(m.filename)\n",
        "            if base not in train_basenames:\n",
        "                test_members.append(m)\n",
        "\n",
        "        print(f\"[INF] Found {len(test_members)} NPZ files in zip not in train list\")\n",
        "\n",
        "        # randomize order so we don't always use the same files\n",
        "        random.shuffle(test_members)\n",
        "\n",
        "        if max_files is not None and len(test_members) > max_files:\n",
        "            test_members = test_members[:max_files]\n",
        "            print(f\"[INF] Randomly sampling {max_files} test files\")\n",
        "\n",
        "        global_correct = 0\n",
        "        global_count   = 0\n",
        "        global_fill_correct = 0\n",
        "        global_fill_total   = 0\n",
        "        global_empty_correct = 0\n",
        "        global_empty_total   = 0\n",
        "\n",
        "        r = window_size // 2\n",
        "\n",
        "        for idx, member in enumerate(test_members):\n",
        "            print(f\"\\n[INF] File {idx+1}/{len(test_members)}: {member.filename}\")\n",
        "            base_name = os.path.splitext(os.path.basename(member.filename))[0]\n",
        "\n",
        "            with zf.open(member, \"r\") as f:\n",
        "                data = np.load(f)\n",
        "                if \"complete\" not in data:\n",
        "                    print(\"  [WARN] 'complete' key missing, skipping\")\n",
        "                    continue\n",
        "                complete_np = data[\"complete\"]\n",
        "\n",
        "            complete = torch.from_numpy(complete_np).float()\n",
        "            if complete.dim() == 3:\n",
        "                complete = complete.unsqueeze(0)  # [1,D,H,W]\n",
        "            if complete.dim() == 4:\n",
        "                if complete.shape[0] != 1:\n",
        "                    print(f\"  [WARN] Unexpected channels {complete.shape}, skipping\")\n",
        "                    continue\n",
        "                complete = complete.unsqueeze(0)  # [1,1,D,H,W]\n",
        "            elif complete.dim() != 5:\n",
        "                print(f\"  [WARN] Unexpected shape {complete.shape}, skipping\")\n",
        "                continue\n",
        "\n",
        "            complete = (complete > 0.5).float()\n",
        "            complete_gt = complete.clone()  # [1,1,D,H,W]\n",
        "            _, _, D, H, W = complete.shape\n",
        "\n",
        "            # working occ + known\n",
        "            occ_grid   = complete.clone()\n",
        "            known_grid = torch.ones_like(occ_grid)\n",
        "\n",
        "            # ---- find candidate centers \"near object\" ----\n",
        "            gt_np = complete_gt[0, 0].cpu().numpy()\n",
        "            candidate_coords = []\n",
        "            for zz in range(D):\n",
        "                for yy in range(H):\n",
        "                    for xx in range(W):\n",
        "                        d0 = max(0, zz - r); d1 = min(D, zz + r + 1)\n",
        "                        h0 = max(0, yy - r); h1 = min(H, yy + r + 1)\n",
        "                        w0 = max(0, xx - r); w1 = min(W, xx + r + 1)\n",
        "                        local = gt_np[d0:d1, h0:h1, w0:w1]\n",
        "                        if local.sum() > 0:\n",
        "                            candidate_coords.append((zz, yy, xx))\n",
        "\n",
        "            if len(candidate_coords) == 0:\n",
        "                print(\"  [WARN] No near-object centers found, skipping file\")\n",
        "                continue\n",
        "\n",
        "            num_candidates = len(candidate_coords)\n",
        "            num_mask = max(1, int(mask_fraction * num_candidates))\n",
        "            idxs = np.random.choice(num_candidates, size=num_mask, replace=False)\n",
        "            masked_coords = np.array([candidate_coords[i] for i in idxs], dtype=np.int64)\n",
        "            masked_coords_t = torch.from_numpy(masked_coords).long()\n",
        "\n",
        "            # apply mask: occ->0, known->0 for masked centers\n",
        "            for (zz, yy, xx) in masked_coords:\n",
        "                occ_grid[0, 0, zz, yy, xx]   = 0.0\n",
        "                known_grid[0, 0, zz, yy, xx] = 0.0\n",
        "\n",
        "            all_logits = []\n",
        "            all_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for start in range(0, masked_coords_t.shape[0], patch_batch_size):\n",
        "                    end = min(start + patch_batch_size, masked_coords_t.shape[0])\n",
        "                    batch_coords = masked_coords_t[start:end]  # [B',3]\n",
        "\n",
        "                    patch_list = []\n",
        "                    known_list = []\n",
        "                    label_list = []\n",
        "\n",
        "                    for (zz, yy, xx) in batch_coords.tolist():\n",
        "                        patch_occ, patch_known = neighborhood_raw(\n",
        "                            occ_grid, known_grid, b=0, d=zz, h=yy, w=xx, window_size=window_size\n",
        "                        )  # patch_occ: [1,1,ws,ws,ws], patch_known: [1,ws,ws,ws]\n",
        "\n",
        "                        patch_list.append(patch_occ)\n",
        "                        known_list.append(patch_known)\n",
        "                        label = complete_gt[0, 0, zz, yy, xx].view(1, 1)  # [1,1]\n",
        "                        label_list.append(label)\n",
        "\n",
        "                    patches = torch.cat(patch_list, dim=0).to(device)   # [B',1,ws,ws,ws]\n",
        "                    knowns  = torch.cat(known_list, dim=0).to(device)   # [B',ws,ws,ws]\n",
        "                    labels  = torch.cat(label_list, dim=0).to(device)   # [B',1]\n",
        "\n",
        "                    logits = model(patches, knowns)                     # [B',1]\n",
        "\n",
        "                    all_logits.append(logits.cpu())\n",
        "                    all_labels.append(labels.cpu())\n",
        "\n",
        "            if len(all_logits) == 0:\n",
        "                print(\"  [WARN] No patches built, skipping stats/vis\")\n",
        "                continue\n",
        "\n",
        "            if len(all_logits) == 1:\n",
        "                logits_all = all_logits[0]\n",
        "                labels_all = all_labels[0]\n",
        "            else:\n",
        "                logits_all = torch.cat(all_logits, dim=0)\n",
        "                labels_all = torch.cat(all_labels, dim=0)\n",
        "\n",
        "            probs = torch.sigmoid(logits_all)\n",
        "            preds = (probs >= 0.5).float()\n",
        "\n",
        "            labels_flat = labels_all.view(-1)\n",
        "            preds_flat  = preds.view(-1)\n",
        "\n",
        "            correct = (preds_flat == labels_flat).sum().item()\n",
        "            count   = labels_flat.numel()\n",
        "\n",
        "            mask_filled = (labels_flat == 1)\n",
        "            mask_empty  = (labels_flat == 0)\n",
        "\n",
        "            if mask_filled.any():\n",
        "                file_fill_correct = (preds_flat[mask_filled] == labels_flat[mask_filled]).sum().item()\n",
        "                file_fill_total   = mask_filled.sum().item()\n",
        "            else:\n",
        "                file_fill_correct = 0\n",
        "                file_fill_total   = 0\n",
        "\n",
        "            if mask_empty.any():\n",
        "                file_empty_correct = (preds_flat[mask_empty] == labels_flat[mask_empty]).sum().item()\n",
        "                file_empty_total   = mask_empty.sum().item()\n",
        "            else:\n",
        "                file_empty_correct = 0\n",
        "                file_empty_total   = 0\n",
        "\n",
        "            file_acc      = correct / count\n",
        "            file_fill_acc = file_fill_correct / file_fill_total if file_fill_total > 0 else float(\"nan\")\n",
        "            file_empty_acc = file_empty_correct / file_empty_total if file_empty_total > 0 else float(\"nan\")\n",
        "\n",
        "            print(\n",
        "                f\"  [FILE STATS] acc={file_acc:.4f} | \"\n",
        "                f\"fill_acc={file_fill_acc:.4f} | empty_acc={file_empty_acc:.4f}\"\n",
        "            )\n",
        "\n",
        "            # --- accumulate global stats ---\n",
        "            global_correct += correct\n",
        "            global_count   += count\n",
        "            global_fill_correct += file_fill_correct\n",
        "            global_fill_total   += file_fill_total\n",
        "            global_empty_correct += file_empty_correct\n",
        "            global_empty_total   += file_empty_total\n",
        "\n",
        "            # --- reconstruct full predicted grid & visualize ---\n",
        "            pred_grid = reconstruct_predicted_grid(\n",
        "                complete_gt=complete_gt,\n",
        "                masked_coords=masked_coords_t,\n",
        "                logits_for_masked=logits_all,\n",
        "            )\n",
        "\n",
        "            name = f\"file{idx+1}_{base_name}\"\n",
        "            visualize_full_object(complete_gt, out_dir, name=name)\n",
        "            visualize_masked_object(complete_gt, masked_coords_t, out_dir, name=name)\n",
        "            visualize_predictions(complete_gt, pred_grid, masked_coords_t, out_dir, name=name)\n",
        "            visualize_gt_vs_pred_side_by_side(complete_gt, pred_grid, out_dir, name=name)\n",
        "\n",
        "\n",
        "        if global_count > 0:\n",
        "            g_acc = global_correct / global_count\n",
        "            g_fill_acc = global_fill_correct / global_fill_total if global_fill_total > 0 else float(\"nan\")\n",
        "            g_empty_acc = global_empty_correct / global_empty_total if global_empty_total > 0 else float(\"nan\")\n",
        "\n",
        "            print(\"\\n[GLOBAL INFERENCE STATS]\")\n",
        "            print(f\"  acc       = {g_acc:.4f}\")\n",
        "            print(f\"  fill_acc  = {g_fill_acc:.4f}\")\n",
        "            print(f\"  empty_acc = {g_empty_acc:.4f}\")\n",
        "        else:\n",
        "            print(\"[INF] No samples processed in inference.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "2bc7f0e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model from ./checkpoint_epoch_1.pth, epoch 1\n",
            "[!] Deleted Previous Files\n",
            "[INF] Loaded 10262 train basenames from ./train_list.txt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     34\u001b[39m         os.remove(file_path)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[!] Deleted Previous Files\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mrun_inference_random_mask_from_zip_with_viz\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mZIP_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_list_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAIN_LIST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWINDOW_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_fraction\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# 20% of near-object voxels\u001b[39;49;00m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_files\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# change if you want more objects\u001b[39;49;00m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatch_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVIS_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 341\u001b[39m, in \u001b[36mrun_inference_random_mask_from_zip_with_viz\u001b[39m\u001b[34m(model, zip_path, train_list_path, window_size, mask_fraction, max_files, patch_batch_size, device, out_dir)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[INF] Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_basenames)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m train basenames from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_list_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# --- scan zip ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zf:\n\u001b[32m    342\u001b[39m     all_members = [m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m zf.infolist() \u001b[38;5;28;01mif\u001b[39;00m m.filename.lower().endswith(\u001b[33m\"\u001b[39m\u001b[33m.npz\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m    344\u001b[39m     test_members = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/zipfile/__init__.py:1385\u001b[39m, in \u001b[36mZipFile.__init__\u001b[39m\u001b[34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[39m\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1384\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1386\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1387\u001b[39m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[32m   1388\u001b[39m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[32m   1389\u001b[39m         \u001b[38;5;28mself\u001b[39m._didModify = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/zipfile/__init__.py:1510\u001b[39m, in \u001b[36mZipFile._RealGetContents\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1507\u001b[39m x._raw_time = t\n\u001b[32m   1508\u001b[39m x.date_time = ( (d>>\u001b[32m9\u001b[39m)+\u001b[32m1980\u001b[39m, (d>>\u001b[32m5\u001b[39m)&\u001b[32m0xF\u001b[39m, d&\u001b[32m0x1F\u001b[39m,\n\u001b[32m   1509\u001b[39m                 t>>\u001b[32m11\u001b[39m, (t>>\u001b[32m5\u001b[39m)&\u001b[32m0x3F\u001b[39m, (t&\u001b[32m0x1F\u001b[39m) * \u001b[32m2\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1510\u001b[39m \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_decodeExtra\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_filename_crc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m x.header_offset = x.header_offset + concat\n\u001b[32m   1512\u001b[39m \u001b[38;5;28mself\u001b[39m.filelist.append(x)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/zipfile/__init__.py:523\u001b[39m, in \u001b[36mZipInfo._decodeExtra\u001b[39m\u001b[34m(self, filename_crc)\u001b[39m\n\u001b[32m    520\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m:\n\u001b[32m    521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.filename.encode(\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m), \u001b[38;5;28mself\u001b[39m.flag_bits | _MASK_UTF_FILENAME\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decodeExtra\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_crc):\n\u001b[32m    524\u001b[39m     \u001b[38;5;66;03m# Try to decode the extra field.\u001b[39;00m\n\u001b[32m    525\u001b[39m     extra = \u001b[38;5;28mself\u001b[39m.extra\n\u001b[32m    526\u001b[39m     unpack = struct.unpack\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "ZIP_PATH = \"/home/raedfidawi/Documents/thesis/3DLLM/chunk_data_16_flood_fill_rm_20.zip\"\n",
        "VIS_PATH = \"./vis_infer_16\"\n",
        "\n",
        "WINDOW_SIZE = 11\n",
        "D_MODEL = 64\n",
        "NUM_HEADS = 4\n",
        "NUM_LAYERS = 4\n",
        "DROPOUT = 0.1\n",
        "GRID_SIZE = 16\n",
        "\n",
        "model = IterativeVoxelModel(\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    window_size=WINDOW_SIZE,\n",
        "    dropout=DROPOUT,\n",
        "    max_grid_size=GRID_SIZE,\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "TRAIN_LIST = \"./train_list.txt\"\n",
        "MODEL_CKPT = \"./checkpoint_epoch_1.pth\"\n",
        "\n",
        "ckpt = torch.load(MODEL_CKPT, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "print(f\"Loaded model from {MODEL_CKPT}, epoch {ckpt.get('epoch', '?')}\")\n",
        "\n",
        "\n",
        "for filename in os.listdir(VIS_PATH):\n",
        "    if filename.endswith(\".png\"):\n",
        "        file_path = os.path.join(VIS_PATH, filename)\n",
        "        os.remove(file_path)\n",
        "\n",
        "print(f\"[!] Deleted Previous Files\")\n",
        "\n",
        "run_inference_random_mask_from_zip_with_viz(\n",
        "    model,\n",
        "    zip_path=ZIP_PATH,\n",
        "    train_list_path=TRAIN_LIST,\n",
        "    window_size=WINDOW_SIZE,\n",
        "    mask_fraction=0.2,        # 20% of near-object voxels\n",
        "    max_files=20,              # change if you want more objects\n",
        "    patch_batch_size=512,\n",
        "    device=device,\n",
        "    out_dir=VIS_PATH,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
