{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92c6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to fix the iterative model code.\n",
    "from typing import Tuple, List\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "import zipfile\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import json\n",
    "\n",
    "# ------------------------------\n",
    "# Data loader / dataset (unchanged)\n",
    "# ------------------------------\n",
    "class VoxelDataLoader:\n",
    "    \"\"\"Loads and processes NPZ voxel data directly from a zip file (no extraction)\"\"\"\n",
    "    def __init__(self, zip_path: str):\n",
    "        self.zip_path = zip_path\n",
    "        self.zip_file = zipfile.ZipFile(zip_path, 'r')\n",
    "        self.npz_files = [f for f in self.zip_file.namelist() if f.endswith('.npz')]\n",
    "        print(f\"Found {len(self.npz_files)} total NPZ files in zip: {zip_path}\")\n",
    "        if len(self.npz_files) == 0:\n",
    "            raise ValueError(f\"No NPZ files found in zip file {zip_path}\")\n",
    "        self.npz_files.sort()\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            self.zip_file.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def load_single_file(self, file_path: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # file_path is the internal path in the zip\n",
    "        with self.zip_file.open(file_path) as f:\n",
    "            data = np.load(f)\n",
    "            if 'complete' not in data or 'partial' not in data:\n",
    "                raise ValueError(f\"NPZ file {file_path} must contain both 'complete' and 'partial' arrays\")\n",
    "            complete = torch.from_numpy(data['complete']).float()\n",
    "            partial = torch.from_numpy(data['partial']).float()\n",
    "            if complete.shape != partial.shape:\n",
    "                raise ValueError(f\"Shape mismatch in {file_path}: complete {complete.shape} vs partial {partial.shape}\")\n",
    "            return complete, partial\n",
    "\n",
    "    def get_all_data(self) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        all_data = []\n",
    "        for file_path in self.npz_files:\n",
    "            complete, partial = self.load_single_file(file_path)\n",
    "            all_data.append((complete, partial))\n",
    "        return all_data\n",
    "\n",
    "    def get_voxel_grids(self, index: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if index >= len(self.npz_files):\n",
    "            raise IndexError(f\"Index {index} out of range. Only {len(self.npz_files)} files available.\")\n",
    "        return self.load_single_file(self.npz_files[index])\n",
    "\n",
    "class VoxelDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"PyTorch Dataset for voxel completion\"\"\"\n",
    "    def __init__(self, zip_path: str, transform=None):\n",
    "        self.data_loader = VoxelDataLoader(zip_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader.npz_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        complete, partial = self.data_loader.get_voxel_grids(idx)\n",
    "        # Binarize to 0/1\n",
    "        complete = (complete > 0).float()\n",
    "        partial = (partial > 0).float()\n",
    "        if self.transform:\n",
    "            complete, partial = self.transform(complete, partial)\n",
    "        return complete, partial\n",
    "\n",
    "def create_data_loaders(zip_path, batch_size=1, shuffle=True, num_workers=0, seed=42):\n",
    "    dataset = VoxelDataset(zip_path)\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    # simple split as before\n",
    "    n = len(dataset)\n",
    "    indices = list(range(n))\n",
    "    random.Random(seed).shuffle(indices)\n",
    "    n_trainval = int(n * 0.8)\n",
    "    n_test = n - n_trainval\n",
    "    trainval_indices = indices[:n_trainval]\n",
    "    test_indices = indices[n_trainval:]\n",
    "    n_train = int(len(trainval_indices) * 0.8)\n",
    "    train_indices = trainval_indices[:n_train]\n",
    "    val_indices = trainval_indices[n_train:]\n",
    "    train_loader = DataLoader(Subset(dataset, train_indices), batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    val_loader = DataLoader(Subset(dataset, val_indices), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(Subset(dataset, test_indices), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ------------------------------\n",
    "# Positional encoding (fixed for [B, D, H, W, d_model])\n",
    "# ------------------------------\n",
    "class PositionalEncoding3D(nn.Module):\n",
    "    def __init__(self, d_model: int, max_grid_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_grid_size = max_grid_size\n",
    "        # stored as (D, H, W, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(max_grid_size, max_grid_size, max_grid_size, d_model))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, D, H, W, d_model]\n",
    "        B, D, H, W, _ = x.shape\n",
    "        pos = self.pos_embed[:D, :H, :W, :].unsqueeze(0)  # [1, D, H, W, d_model]\n",
    "        return x + pos\n",
    "\n",
    "    def get_encoding(self, D, H, W):\n",
    "        return self.pos_embed[:D, :H, :W, :]\n",
    "\n",
    "# ------------------------------\n",
    "# Local attention (your class, unchanged)\n",
    "# ------------------------------\n",
    "class LocalAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int = 4, window_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.window_size = window_size\n",
    "        assert d_model % num_heads == 0\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, target_embedding, neighbor_embeddings, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            target_embedding: [B, d_model] - Embedding of the voxel to predict.\n",
    "            neighbor_embeddings: [B, ws, ws, ws, d_model] - Embeddings of the neighborhood.\n",
    "            mask: [B, ws, ws, ws] - Boolean mask (True for known, False for unknown).\n",
    "        \"\"\"\n",
    "        B = target_embedding.shape[0]\n",
    "        ws = self.window_size\n",
    "\n",
    "        neighbor_flat = neighbor_embeddings.view(B, ws * ws * ws, self.d_model)\n",
    "        mask_flat = mask.view(B, ws * ws * ws)  # [B, ws^3]\n",
    "\n",
    "        # Query from target embedding: [B, 1, d_model]\n",
    "        q = self.q_proj(target_embedding.unsqueeze(1))  # [B, 1, d_model]\n",
    "        # Keys and values from neighbors: [B, ws^3, d_model]\n",
    "        k = self.k_proj(neighbor_flat)\n",
    "        v = self.v_proj(neighbor_flat)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, 1, self.num_heads, self.head_dim).transpose(1, 2)  # [B, num_heads, 1, head_dim]\n",
    "        k = k.view(B, ws * ws * ws, self.num_heads, self.head_dim).transpose(1, 2)  # [B, num_heads, ws^3, head_dim]\n",
    "        v = v.view(B, ws * ws * ws, self.num_heads, self.head_dim).transpose(1, 2)  # [B, num_heads, ws^3, head_dim]\n",
    "\n",
    "        # Attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [B, num_heads, 1, ws^3]\n",
    "\n",
    "        # Apply mask (True means allowed)\n",
    "        mask_expanded = mask_flat.unsqueeze(1).unsqueeze(2).expand(-1, self.num_heads, 1, -1)\n",
    "        scores = scores.masked_fill(~mask_expanded, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = torch.where(torch.isnan(attn_weights), torch.zeros_like(attn_weights), attn_weights)\n",
    "\n",
    "        out = torch.matmul(attn_weights, v)  # [B, num_heads, 1, head_dim]\n",
    "        out = out.transpose(1, 2).contiguous().view(B, 1, self.d_model).squeeze(1)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "# ------------------------------\n",
    "# Voxel transformer layer that uses LocalAttention per voxel\n",
    "# ------------------------------\n",
    "class VoxelTransformerLayer3D(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int = 8, window_size: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.window_size = window_size\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.attention = LocalAttention(d_model, num_heads, window_size)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, neighborhood_fn, mask_fn):\n",
    "        \"\"\"\n",
    "        x: [B, D, H, W, d_model]\n",
    "        neighborhood_fn: callable(grid, d, h, w, window_size) -> [B, ws, ws, ws, d_model]\n",
    "        mask_fn: callable(D, H, W, d, h, w, window_size) -> [ws,ws,ws] or [1,ws,ws,ws] or [B,ws,ws,ws]\n",
    "        \"\"\"\n",
    "        B, D, H, W, C = x.shape\n",
    "        out = torch.zeros_like(x)\n",
    "\n",
    "        for dd in range(D):\n",
    "            for hh in range(H):\n",
    "                for ww in range(W):\n",
    "                    target = x[:, dd, hh, ww, :]  # [B, d_model]\n",
    "                    neighbors = neighborhood_fn(x, dd, hh, ww, self.window_size)  # [B, ws, ws, ws, d_model]\n",
    "\n",
    "                    # === robust mask handling ===\n",
    "                    mask = mask_fn(D, H, W, dd, hh, ww, self.window_size)\n",
    "                    # mask can be one of: [ws,ws,ws], [1,ws,ws,ws], [B,ws,ws,ws]\n",
    "                    if mask.dim() == 3:\n",
    "                        # [ws,ws,ws] -> [1,ws,ws,ws]\n",
    "                        mask = mask.unsqueeze(0)\n",
    "                    if mask.shape[0] == 1 and B > 1:\n",
    "                        # [1,ws,ws,ws] -> [B,ws,ws,ws]\n",
    "                        mask = mask.expand(B, -1, -1, -1).contiguous()\n",
    "                    # now mask is guaranteed to be [B, ws, ws, ws]\n",
    "\n",
    "                    # attention (per-voxel)\n",
    "                    tgt_norm = self.norm1(target)\n",
    "                    attn_out = self.attention(tgt_norm, neighbors, mask)\n",
    "                    target = target + self.dropout(attn_out)\n",
    "\n",
    "                    # ffn\n",
    "                    tgt_norm = self.norm2(target)\n",
    "                    ffn_out = self.ffn(tgt_norm)\n",
    "                    target = target + ffn_out\n",
    "\n",
    "                    out[:, dd, hh, ww, :] = target\n",
    "        return out\n",
    "\n",
    "# ------------------------------\n",
    "# Stack layers\n",
    "# ------------------------------\n",
    "class VoxelTransformer3D(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, window_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            VoxelTransformerLayer3D(d_model, num_heads, window_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, neighborhood_fn, mask_fn):\n",
    "        \"\"\"\n",
    "        x: [B, D, H, W, d_model]\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, neighborhood_fn, mask_fn)\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# Helper functions for voxel candidate selection\n",
    "# ------------------------------\n",
    "def get_voxel_candidates(complete_grid, partial_grid, max_voxels: int = 256):\n",
    "    \"\"\"\n",
    "    Returns a balanced *sample* of candidate voxels to predict.\n",
    "    \n",
    "    Args:\n",
    "        complete_grid: [B, 1, D, H, W]\n",
    "        partial_grid: [B, 1, D, H, W]\n",
    "        max_voxels: maximum number of candidate voxels per batch element.\n",
    "    Returns:\n",
    "        list of voxel coords: (b, d, h, w, label)\n",
    "    \"\"\"\n",
    "    B, _, D, H, W = complete_grid.shape\n",
    "    candidates = []\n",
    "\n",
    "    for b in range(B):\n",
    "        filled = ((complete_grid[b,0]==1) & (partial_grid[b,0]==0)).nonzero(as_tuple=False)\n",
    "        empty  = ((complete_grid[b,0]==0) & (partial_grid[b,0]==0)).nonzero(as_tuple=False)\n",
    "\n",
    "        # balance between filled and empty\n",
    "        k = max_voxels // 2\n",
    "        filled_k = min(len(filled), k)\n",
    "        empty_k  = min(len(empty),  k)\n",
    "\n",
    "        if filled_k > 0:\n",
    "            filled_idx = torch.randperm(len(filled))[:filled_k]\n",
    "            filled = filled[filled_idx]\n",
    "        else:\n",
    "            filled = []\n",
    "\n",
    "        if empty_k > 0:\n",
    "            empty_idx = torch.randperm(len(empty))[:empty_k]\n",
    "            empty = empty[empty_idx]\n",
    "        else:\n",
    "            empty = []\n",
    "\n",
    "        for f in filled:\n",
    "            candidates.append((b, f[0].item(), f[1].item(), f[2].item(), 1))\n",
    "        for e in empty:\n",
    "            candidates.append((b, e[0].item(), e[1].item(), e[2].item(), 0))\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def compute_density(grid, d, h, w, window_size):\n",
    "    \"\"\"Counts filled neighbors around voxel. grid: [D,H,W]\"\"\"\n",
    "    D, H, W = grid.shape\n",
    "    r = window_size // 2\n",
    "    d0, d1 = max(0, d-r), min(D, d+r+1)\n",
    "    h0, h1 = max(0, h-r), min(H, h+r+1)\n",
    "    w0, w1 = max(0, w-r), min(W, w+r+1)\n",
    "    patch = grid[d0:d1, h0:h1, w0:w1]\n",
    "    return float(patch.sum().item())\n",
    "\n",
    "def sort_voxels(candidates, complete_grid, window_size):\n",
    "    \"\"\"Sorts voxel list by density then by distance to origin.\"\"\"\n",
    "    sorted_list = []\n",
    "    for (b,d,h,w,label) in candidates:\n",
    "        density = compute_density(complete_grid[b,0], d,h,w, window_size)\n",
    "        dist = d + h + w  # Manhattan distance to origin\n",
    "        sorted_list.append(((b,d,h,w,label), density, dist))\n",
    "    sorted_list.sort(key=lambda x: (-x[1], x[2]))  # high density first, then close to origin\n",
    "    return [item[0] for item in sorted_list]\n",
    "\n",
    "# ------------------------------\n",
    "# neighborhood_raw: returns occ patch (1 channel) + boolean known_mask\n",
    "# ------------------------------\n",
    "def neighborhood_raw(occ_grid, known_grid, b, d, h, w, window_size):\n",
    "    \"\"\"\n",
    "    occ_grid:   [B, 1, D, H, W]  (partial occupancy: 1=occupied, 0=empty or unknown)\n",
    "    known_grid: [B, 1, D, H, W]  (observability:   1=known (observed/safe evidence), 0=unknown)\n",
    "    returns:\n",
    "        patch:      [1, 1, ws, ws, ws]  (channel-first, ready for Conv3d)\n",
    "        known_mask: [1, ws, ws, ws]     (bool: True where neighbor is known/observed)\n",
    "    \"\"\"\n",
    "    # occupancy patch for the numeric input\n",
    "    patch_occ   = neighborhood_fn(occ_grid,   b, d, h, w, window_size)   # [1,1,ws,ws,ws]\n",
    "    # known patch for the attention mask\n",
    "    patch_known = neighborhood_fn(known_grid, b, d, h, w, window_size)   # [1,1,ws,ws,ws]\n",
    "\n",
    "    known_mask = (patch_known[:, 0] == 1)  # -> [1,ws,ws,ws] boolean\n",
    "    return patch_occ.contiguous(), known_mask.contiguous()\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# IterativeVoxelModel: projection + positional encoding + small transformer on the patch\n",
    "# ------------------------------\n",
    "class IterativeVoxelModel(nn.Module):\n",
    "    def __init__(self, d_model: int = 64, num_heads: int = 4, num_layers: int = 3,\n",
    "                 window_size: int = 3, max_grid_size: int = 32, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        This model expects per-call patch input:\n",
    "            neighbors_patch: [B, 1, ws, ws, ws]  (binary observed values or zeros)\n",
    "            known_mask: [B, ws, ws, ws] (boolean)\n",
    "        The model:\n",
    "            - projects neighbors -> d_model via Conv3d(1 -> d_model)\n",
    "            - permutes to [B, Dp, Hp, Wp, d_model], adds positional encoding\n",
    "            - runs VoxelTransformer3D on the patch\n",
    "            - extracts center voxel embedding and returns a scalar logit (per batch)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.window_size = window_size\n",
    "        self.input_proj = nn.Conv3d(1, d_model, kernel_size=1)  # works on [B,1,ws,ws,ws]\n",
    "        self.pos_encoding = PositionalEncoding3D(d_model, max_grid_size=max_grid_size)\n",
    "        # use the same transformer as defined above, but it expects patch in [B, D, H, W, d_model]\n",
    "        self.transformer = VoxelTransformer3D(num_layers=num_layers, d_model=d_model,\n",
    "                                              num_heads=num_heads, window_size=window_size,\n",
    "                                              dropout=dropout)\n",
    "        # output head: from d_model to one logit\n",
    "        self.output_head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, neighbors_patch, known_mask):\n",
    "        \"\"\"\n",
    "        neighbors_patch: [B, 1, ws, ws, ws]\n",
    "        known_mask: [B, ws, ws, ws] boolean - True where neighbor is observed/known\n",
    "        Returns:\n",
    "            logits: [B, 1] (logit for center voxel occupancy)\n",
    "        \"\"\"\n",
    "        B = neighbors_patch.shape[0]\n",
    "        ws = self.window_size\n",
    "        assert neighbors_patch.shape[2:] == (ws, ws, ws), f\"neighbors size mismatch {neighbors_patch.shape}\"\n",
    "        # project\n",
    "        emb = self.input_proj(neighbors_patch)  # [B, d_model, ws, ws, ws]\n",
    "        # permute to [B, D, H, W, d_model]\n",
    "        emb = emb.permute(0, 2, 3, 4, 1).contiguous()\n",
    "        # add positional encoding\n",
    "        emb = self.pos_encoding(emb)  # [B, ws, ws, ws, d_model]\n",
    "\n",
    "        # define patch-local neighborhood_fn and mask_fn used by VoxelTransformerLayer3D\n",
    "        def neighborhood_fn_patch(grid, dd, hh, ww, window_size):\n",
    "            # grid: [B, Dp, Hp, Wp, d_model] where Dp=ws\n",
    "            B2, Dp, Hp, Wp, C = grid.shape\n",
    "            r = window_size // 2\n",
    "            d0, d1 = max(0, dd - r), min(Dp, dd + r + 1)\n",
    "            h0, h1 = max(0, hh - r), min(Hp, hh + r + 1)\n",
    "            w0, w1 = max(0, ww - r), min(Wp, ww + r + 1)\n",
    "            patch_local = grid[:, d0:d1, h0:h1, w0:w1, :]  # may be smaller than ws on boundaries\n",
    "            # pad if necessary to shape [B, ws, ws, ws, d_model]\n",
    "            pd0 = max(0, r - dd); pd1 = max(0, (dd + r + 1) - Dp)\n",
    "            ph0 = max(0, r - hh); ph1 = max(0, (hh + r + 1) - Hp)\n",
    "            pw0 = max(0, r - ww); pw1 = max(0, (ww + r + 1) - Wp)\n",
    "            if any([pd0,pd1,ph0,ph1,pw0,pw1]):\n",
    "                # permute to channel-first temporarily to use F.pad on last 3 dims\n",
    "                # patch_local: [B, d_patch, h_patch, w_patch, C] -> permute to [B, C, d,h,w]\n",
    "                tmp = patch_local.permute(0, 4, 1, 2, 3).contiguous()\n",
    "                pad = (pw0, pw1, ph0, ph1, pd0, pd1)\n",
    "                tmp = F.pad(tmp, pad)\n",
    "                patch_local = tmp.permute(0, 2, 3, 4, 1).contiguous()\n",
    "            # ensure final shape is [B, ws, ws, ws, d_model]\n",
    "            return patch_local\n",
    "\n",
    "        def mask_fn_patch(Dp, Hp, Wp, dd, hh, ww, window_size):\n",
    "            r = window_size // 2\n",
    "            d0, d1 = max(0, dd - r), min(Dp, dd + r + 1)\n",
    "            h0, h1 = max(0, hh - r), min(Hp, hh + r + 1)\n",
    "            w0, w1 = max(0, ww - r), min(Wp, ww + r + 1)\n",
    "\n",
    "            km_local = known_mask[:, d0:d1, h0:h1, w0:w1]  # [B,d,h,w]  (BOOL: observed)\n",
    "            # clone so we don't write into the original known_mask\n",
    "            km_local = km_local.clone()\n",
    "\n",
    "            pd0 = max(0, r - dd); pd1 = max(0, (dd + r + 1) - Dp)\n",
    "            ph0 = max(0, r - hh); ph1 = max(0, (hh + r + 1) - Hp)\n",
    "            pw0 = max(0, r - ww); pw1 = max(0, (ww + r + 1) - Wp)\n",
    "            if any([pd0, pd1, ph0, ph1, pw0, pw1]):\n",
    "                km_local = F.pad(km_local, (pw0, pw1, ph0, ph1, pd0, pd1), value=False)\n",
    "\n",
    "            km_local[:, r, r, r] = False  # block center\n",
    "            return km_local  # [B, ws, ws, ws] (dtype=bool)\n",
    "\n",
    "\n",
    "        # Now run the transformer on emb patch\n",
    "        # emb: [B, ws, ws, ws, d_model]\n",
    "        # we must give neighborhood_fn_patch and mask_fn_patch to transformer\n",
    "        out_patch = self.transformer(emb, neighborhood_fn_patch, mask_fn_patch)  # [B, ws, ws, ws, d_model]\n",
    "\n",
    "        # extract center voxel index\n",
    "        center = ws // 2\n",
    "        center_emb = out_patch[:, center, center, center, :]  # [B, d_model]\n",
    "        logits = self.output_head(center_emb)  # [B, 1]\n",
    "        return logits\n",
    "\n",
    "\n",
    "def neighborhood_fn(grid, b, d, h, w, window_size):\n",
    "    \"\"\"\n",
    "    Extract a cubic neighborhood around voxel (d,h,w) from batch element b.\n",
    "    grid: [B, C, D, H, W]\n",
    "    Returns: [1, C, ws, ws, ws]\n",
    "    \"\"\"\n",
    "    assert grid.dim() == 5, f\"Expected 5D grid, got {grid.shape}\"\n",
    "    assert window_size % 2 == 1, \"window_size must be odd.\"\n",
    "    radius = window_size // 2\n",
    "\n",
    "    # Pad grid symmetrically\n",
    "    padded = F.pad(grid[b:b+1], (radius, radius, radius, radius, radius, radius), mode=\"constant\", value=0)\n",
    "\n",
    "    # shift indices because of padding\n",
    "    d, h, w = d + radius, h + radius, w + radius\n",
    "\n",
    "    # slice out the patch\n",
    "    patch = padded[:, :, d-radius:d+radius+1, h-radius:h+radius+1, w-radius:w+radius+1]\n",
    "    return patch  # [1, C, ws, ws, ws]\n",
    "\n",
    "\n",
    "# ====== Loss helpers ======\n",
    "def tversky_loss_from_logits(logits, labels, alpha=0.7, beta=0.3, eps=1e-6):\n",
    "    \"\"\"\n",
    "    logits: [N,1], labels: [N,1] in {0,1}\n",
    "    Penalizes FP more when alpha>beta. (1 - Tversky)\n",
    "    \"\"\"\n",
    "    p = torch.sigmoid(logits)\n",
    "    tp = (p * labels).sum()\n",
    "    fp = (p * (1 - labels)).sum()\n",
    "    fn = ((1 - p) * labels).sum()\n",
    "    tversky = (tp + eps) / (tp + alpha * fp + beta * fn + eps)\n",
    "    return 1.0 - tversky\n",
    "\n",
    "def batch_pos_weight(labels, min_pw=1.0, max_pw=10.0):\n",
    "    \"\"\"\n",
    "    Compute pos_weight = N_neg / N_pos, clamped to [min_pw, max_pw].\n",
    "    labels: [N,1]\n",
    "    \"\"\"\n",
    "    pos = labels.sum().item()\n",
    "    neg = labels.numel() - pos\n",
    "    if pos < 1:  # avoid inf; fall back\n",
    "        return torch.tensor([min_pw], dtype=labels.dtype, device=labels.device)\n",
    "    pw = max(min_pw, min(max_pw, neg / max(pos, 1e-6)))\n",
    "    return torch.tensor([pw], dtype=labels.dtype, device=labels.device)\n",
    "\n",
    "\n",
    "# ====== Metrics / validation helpers ======\n",
    "@torch.no_grad()\n",
    "def _gather_val_logits_labels(model, val_loader, window_size, device):\n",
    "    \"\"\"\n",
    "    Runs the model on the validation set and returns concatenated logits and labels.\n",
    "    Uses the SAME known_grid logic you added for training (no ambiguity).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "\n",
    "    for complete_grid, partial_grid in val_loader:\n",
    "        complete_grid = complete_grid.to(device)\n",
    "        partial_grid  = partial_grid.to(device)\n",
    "\n",
    "        # Ensure [B,1,D,H,W]\n",
    "        if complete_grid.dim() == 4: complete_grid = complete_grid.unsqueeze(1)\n",
    "        if partial_grid.dim()  == 4: partial_grid  = partial_grid.unsqueeze(1)\n",
    "\n",
    "        # known_grid from GT (training-time definition): unknown only where we deleted\n",
    "        missing_grid = ((complete_grid == 1) & (partial_grid == 0)).float()\n",
    "        known_grid   = (1.0 - missing_grid)\n",
    "\n",
    "        # Use the same candidate policy as training (balanced sample + sorting)\n",
    "        candidates = get_voxel_candidates(complete_grid, partial_grid, max_voxels=256)\n",
    "        candidates = sort_voxels(candidates, complete_grid, window_size)\n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "\n",
    "        patches, known_masks, labels = [], [], []\n",
    "        for (b, d, h, w, label) in candidates:\n",
    "            patch, known_mask = neighborhood_raw(partial_grid, known_grid, b, d, h, w, window_size)\n",
    "            patches.append(patch)\n",
    "            known_masks.append(known_mask)\n",
    "            labels.append(label)\n",
    "\n",
    "        patches     = torch.cat(patches, dim=0).to(device)        # [N,1,ws,ws,ws]\n",
    "        known_masks = torch.cat(known_masks, dim=0).to(device)     # [N,ws,ws,ws]\n",
    "        labels      = torch.tensor(labels, dtype=torch.float32, device=device).unsqueeze(1)  # [N,1]\n",
    "\n",
    "        logits = model(patches, known_masks)  # [N,1]\n",
    "        all_logits.append(logits.detach())\n",
    "        all_labels.append(labels.detach())\n",
    "\n",
    "    if len(all_logits) == 0:\n",
    "        # No candidates found (edge case)\n",
    "        return torch.empty(0,1, device=device), torch.empty(0,1, device=device)\n",
    "\n",
    "    return torch.cat(all_logits, dim=0), torch.cat(all_labels, dim=0)\n",
    "\n",
    "\n",
    "def _metrics_for_threshold(probs, labels, tau, eps=1e-8):\n",
    "    \"\"\"\n",
    "    probs, labels: [N,1], float in [0,1] and {0,1}\n",
    "    Returns dict of metrics for the positive class.\n",
    "    \"\"\"\n",
    "    preds = (probs >= tau).float()\n",
    "    tp = (preds * labels).sum()\n",
    "    fp = (preds * (1 - labels)).sum()\n",
    "    fn = ((1 - preds) * labels).sum()\n",
    "    tn = (((1 - preds) * (1 - labels))).sum()\n",
    "\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall    = tp / (tp + fn + eps)\n",
    "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
    "    iou       = tp / (tp + fp + fn + eps)\n",
    "    acc       = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    return dict(tau=tau, precision=precision.item(), recall=recall.item(),\n",
    "                f1=f1.item(), iou=iou.item(), acc=acc.item(),\n",
    "                tp=int(tp.item()), fp=int(fp.item()), fn=int(fn.item()), tn=int(tn.item()))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(model, val_loader, window_size, device, tau_grid=(0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90)):\n",
    "    \"\"\"\n",
    "    Runs validation, sweeps thresholds, and returns:\n",
    "      best (by IoU), full metrics list, and raw counts.\n",
    "    \"\"\"\n",
    "    logits, labels = _gather_val_logits_labels(model, val_loader, window_size, device)\n",
    "    if logits.numel() == 0:\n",
    "        return None, []  # nothing to evaluate\n",
    "\n",
    "    probs = torch.sigmoid(logits)\n",
    "    results = []\n",
    "    for tau in tau_grid:\n",
    "        results.append(_metrics_for_threshold(probs, labels, tau))\n",
    "\n",
    "    # pick best by IoU\n",
    "    best = max(results, key=lambda d: d[\"iou\"])\n",
    "    return best, results\n",
    "\n",
    "\n",
    "\n",
    "def train_model_voxelwise(\n",
    "    model: nn.Module,\n",
    "    train_set,\n",
    "    val_set,\n",
    "    num_epochs: int = 5,\n",
    "    batch_size: int = 8,\n",
    "    window_size: int = 5,\n",
    "    seed: int = 42\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True, unit='sample')\n",
    "        for complete_grid, partial_grid in pbar:\n",
    "\n",
    "            complete_grid = complete_grid.to(device)\n",
    "            partial_grid = partial_grid.to(device)\n",
    "\n",
    "            # Ensure 5D [B,1,D,H,W]\n",
    "            if complete_grid.dim() == 4:\n",
    "                complete_grid = complete_grid.unsqueeze(1)\n",
    "            if partial_grid.dim() == 4:\n",
    "                partial_grid = partial_grid.unsqueeze(1)\n",
    "\n",
    "            # ---- candidate voxels (implement separately) ----\n",
    "            candidates = get_voxel_candidates(complete_grid, partial_grid, max_voxels=256)\n",
    "            candidates = sort_voxels(candidates, complete_grid, window_size)\n",
    "\n",
    "            if len(candidates) == 0:\n",
    "                continue\n",
    "\n",
    "            patches, known_masks, labels = [], [], []\n",
    "\n",
    "            missing_grid = ((complete_grid == 1) & (partial_grid == 0)).float()  # [B,1,D,H,W]\n",
    "            known_grid   = (1.0 - missing_grid)   \n",
    "\n",
    "            for (b, d, h, w, label) in candidates:\n",
    "                patch, known_mask = neighborhood_raw(partial_grid, known_grid, b, d, h, w, window_size)\n",
    "                patches.append(patch)          # [1,1,ws,ws,ws]\n",
    "                known_masks.append(known_mask) # [1,ws,ws,ws] (bool)\n",
    "                labels.append(label)\n",
    "\n",
    "\n",
    "            # stack into tensors\n",
    "            patches = torch.cat(patches, dim=0).to(device)        # [N,1,ws,ws,ws]\n",
    "            known_masks = torch.cat(known_masks, dim=0).to(device)  # [N,ws,ws,ws]\n",
    "            labels = torch.tensor(labels, dtype=torch.float32, device=device).unsqueeze(1)  # [N,1]\n",
    "\n",
    "            # ---- forward ----\n",
    "            logits = model(patches, known_masks)  # [N,1]\n",
    "\n",
    "            # ---- BCE with dynamic pos_weight ----\n",
    "            pos_w = batch_pos_weight(labels)              # tensor([..], device=..)\n",
    "            bce   = F.binary_cross_entropy_with_logits(logits, labels, pos_weight=pos_w)\n",
    "\n",
    "            # ---- Tversky term (penalize FP more than FN) ----\n",
    "            tversky = tversky_loss_from_logits(logits, labels, alpha=0.7, beta=0.3)\n",
    "\n",
    "            # ---- Optional: sparsity regularizer (small) ----\n",
    "            # If you have densities for each candidate, you can add a mild penalty that discourages p=1 in very sparse zones.\n",
    "            # We can re-use your compute_density on the *partial* grid (evidence the model sees).\n",
    "            dens_list = []\n",
    "            ws_for_density = window_size  # same window as your heuristic; keep small\n",
    "            for (b_, d_, h_, w_, _) in candidates:\n",
    "                dens_list.append(compute_density(partial_grid[b_, 0], d_, h_, w_, ws_for_density))\n",
    "            dens = torch.tensor(dens_list, device=device, dtype=logits.dtype).unsqueeze(1)  # [N,1]\n",
    "            sparse_mask = (dens <= 1).float()   # mark very sparse centers\n",
    "            sparsity_reg = (torch.sigmoid(logits) * sparse_mask).mean()  # penalize confident 1s in free space\n",
    "\n",
    "            # ---- total loss (tune lambdas) ----\n",
    "            lambda_tversky = 0.20\n",
    "            lambda_sparse  = 0.05\n",
    "            loss = bce + lambda_tversky * tversky + lambda_sparse * sparsity_reg\n",
    "\n",
    "\n",
    "            # ---- backward ----\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "            avg_loss = total_loss / n_batches\n",
    "            pbar.set_postfix({'train_loss': f\"{avg_loss:.4f}\"})\n",
    "\n",
    "            best_overall = None  # will hold dict with best metrics across epochs\n",
    "            best_tau = None\n",
    "\n",
    "        # ====== VALIDATION at epoch end ======\n",
    "        best, sweep = validate_epoch(model, val_loader, window_size, device,\n",
    "                                    tau_grid=(0.50,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90))\n",
    "        if best is None:\n",
    "            print(f\"[Val] Epoch {epoch+1}: no candidates to evaluate.\")\n",
    "        else:\n",
    "            print(f\"[Val] Epoch {epoch+1}: \"\n",
    "                f\"best_tau={best['tau']:.2f} \"\n",
    "                f\"IoU={best['iou']:.4f} \"\n",
    "                f\"F1={best['f1']:.4f} \"\n",
    "                f\"Prec={best['precision']:.4f} \"\n",
    "                f\"Rec={best['recall']:.4f} \"\n",
    "                f\"Acc={best['acc']:.4f} \"\n",
    "                f\"TP={best['tp']} FP={best['fp']} FN={best['fn']} TN={best['tn']}\")\n",
    "\n",
    "            # keep global best across epochs\n",
    "            if (best_overall is None) or (best['iou'] > best_overall['iou']):\n",
    "                best_overall = dict(best)\n",
    "                best_tau = best['tau']\n",
    "\n",
    "        if best_overall is not None:\n",
    "            print(f\"\\n[Val] Best across all epochs: \"\n",
    "                f\"tau={best_tau:.2f} IoU={best_overall['iou']:.4f} \"\n",
    "                f\"F1={best_overall['f1']:.4f} Prec={best_overall['precision']:.4f} \"\n",
    "                f\"Rec={best_overall['recall']:.4f} Acc={best_overall['acc']:.4f}\")\n",
    "            # (Optional) save best tau for later inference\n",
    "            with open(\"best_threshold.json\", \"w\") as f:\n",
    "                json.dump({\"best_tau\": best_tau}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371e183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"iterative_model_new.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e5a03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 256571 total NPZ files in zip: /home/raedfidawi/Documents/thesis/3DLLM/chunk_data_16_flood_fill_rm_20.zip\n",
      "Found 61576 entries in: train_list.txt\n",
      "Loaded 61576 test samples from ZIP (out of 61576).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "zip_path        = \"/home/raedfidawi/Documents/thesis/3DLLM/chunk_data_16_flood_fill_rm_20.zip\"\n",
    "train_list_path = \"train_list.txt\"\n",
    "\n",
    "# Init dataset (uses zip internally)\n",
    "dataset = VoxelDataset(zip_path)\n",
    "\n",
    "# Read listed files (relative zip paths as written by you)\n",
    "with open(train_list_path, \"r\") as f:\n",
    "    listed_paths = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "print(f\"Found {len(listed_paths)} entries in: {train_list_path}\")\n",
    "\n",
    "# ZIP internal file list (exact paths stored in the zip)\n",
    "zip_paths = dataset.data_loader.npz_files  # e.g. ['chunk_data_16_flood_fill_rm_20/abc.npz', ...]\n",
    "# Build fast lookups\n",
    "full_to_idx = {p: i for i, p in enumerate(zip_paths)}\n",
    "base_to_idxs = {}\n",
    "for i, p in enumerate(zip_paths):\n",
    "    b = os.path.basename(p)\n",
    "    base_to_idxs.setdefault(b, []).append(i)\n",
    "\n",
    "# Build test samples from train_list\n",
    "test_samples = []\n",
    "missing = []\n",
    "ambiguous = []\n",
    "\n",
    "for rel in listed_paths:\n",
    "    rel_norm = rel.replace(\"\\\\\", \"/\")  # normalize just in case\n",
    "    idx = full_to_idx.get(rel_norm, None)\n",
    "\n",
    "    if idx is None:\n",
    "        # fallback to basename matching\n",
    "        base = os.path.basename(rel_norm)\n",
    "        cand = base_to_idxs.get(base)\n",
    "        if cand is None:\n",
    "            missing.append(rel)\n",
    "            continue\n",
    "        if len(cand) > 1:\n",
    "            # multiple files in the zip share this basename; report & pick the first\n",
    "            ambiguous.append((rel, [zip_paths[i] for i in cand]))\n",
    "            idx = cand[0]\n",
    "        else:\n",
    "            idx = cand[0]\n",
    "\n",
    "    complete, partial = dataset[idx]\n",
    "    test_samples.append((complete, partial))\n",
    "\n",
    "print(f\"Loaded {len(test_samples)} test samples from ZIP (out of {len(listed_paths)}).\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"⚠️ Missing in ZIP: {len(missing)}\")\n",
    "    for m in missing[:10]:\n",
    "        print(\"   \", m)\n",
    "\n",
    "if ambiguous:\n",
    "    print(f\"⚠️ Ambiguous basenames (picked first match): {len(ambiguous)}\")\n",
    "    for rel, candidates in ambiguous[:5]:\n",
    "        print(\"   \", rel, \"->\", candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "985b7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "@torch.inference_mode()\n",
    "def iterative_inference(model_path, test_samples, device,\n",
    "                        no_change_patience=100, prob_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Iterative inference that:\n",
    "      - treats ALL empty voxels as initial candidates\n",
    "      - initializes known grid from the current partial ones only (known=1 where partial==1)\n",
    "      - builds patches via neighborhood_raw(occ_grid, known_grid, ...)\n",
    "      - commits positives at p >= prob_threshold (sets occ=1 AND known=1)\n",
    "      - leaves negatives as unknown (known=0) so they can be reconsidered later\n",
    "    \"\"\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    sample_idx = random.randint(0, len(test_samples)-1)\n",
    "    complete, partial = test_samples[sample_idx]\n",
    "    print(f\"Selected sample index: {sample_idx}\")\n",
    "\n",
    "    # === Load model (match your training config!) ===\n",
    "    model = IterativeVoxelModel(\n",
    "        d_model=48,\n",
    "        num_heads=4,\n",
    "        num_layers=3,\n",
    "        max_grid_size=16,\n",
    "        window_size=5,   # make sure this matches the model you trained\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in checkpoint['model_state_dict'].items():\n",
    "        new_state_dict[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    # === Prepare occ & known ===\n",
    "    # occ starts as the partial; known starts as \"we only know those 1's are occupied\"\n",
    "    occ   = (partial.unsqueeze(0).unsqueeze(0).to(device) == 1).float()  # [1,1,D,H,W] float in {0,1}\n",
    "    known = (occ == 1)                                                   # [1,1,D,H,W] BOOL  (True where occ==1)\n",
    "                                            # known=1 where partial==1; 0 elsewhere\n",
    "\n",
    "    D, H, W = partial.shape\n",
    "    window  = int(model.window_size)\n",
    "\n",
    "    # --- neighbor density (on current occ) ---\n",
    "    def count_filled_neighbors(d, h, w, occ_grid, win):\n",
    "        r = win // 2\n",
    "        d0, d1 = max(0, d - r), min(D - 1, d + r)\n",
    "        h0, h1 = max(0, h - r), min(H - 1, h + r)\n",
    "        w0, w1 = max(0, w - r), min(W - 1, w + r)\n",
    "        g = occ_grid[0, 0]\n",
    "        cnt = 0\n",
    "        for dd in range(d0, d1 + 1):\n",
    "            for hh in range(h0, h1 + 1):\n",
    "                for ww in range(w0, w1 + 1):\n",
    "                    if dd == d and hh == h and ww == w:\n",
    "                        continue\n",
    "                    if g[dd, hh, ww] > 0:\n",
    "                        cnt += 1\n",
    "        return cnt\n",
    "\n",
    "    # === Candidates = ALL empty voxels initially ===\n",
    "    empty_coords = torch.nonzero(occ[0,0] == 0, as_tuple=False)  # [N,3] (d,h,w)\n",
    "\n",
    "    # score and sort (density desc, then manhattan distance asc)\n",
    "    voxel_scores = []\n",
    "    for d, h, w in empty_coords.tolist():\n",
    "        density = count_filled_neighbors(d, h, w, occ, window)\n",
    "        dist = d + h + w\n",
    "        voxel_scores.append((d, h, w, density, dist))\n",
    "    voxel_scores.sort(key=lambda t: (-t[3], t[4]))\n",
    "    sorted_candidates = [(d, h, w) for d, h, w, _, _ in voxel_scores]\n",
    "\n",
    "    # --- early stop tracker ---\n",
    "    no_change_run = 0\n",
    "\n",
    "    print(f\"Entering prediction on {len(sorted_candidates)} candidates...\")\n",
    "    for i, (d, h, w) in enumerate(sorted_candidates):\n",
    "        old_val = occ[0, 0, d, h, w].item()\n",
    "\n",
    "        # Build patch + mask that match TRAINING:\n",
    "        # neighborhood_raw expects (occ_grid, known_grid, b, d,h,w, window_size)\n",
    "        patch, mask_patch = neighborhood_raw(occ, known.float(), 0, d, h, w, window)  # patch:[1,1,ws,ws,ws], mask:[1,ws,ws,ws] bool\n",
    "        logits = model(patch.to(device), mask_patch.to(device))\n",
    "        p = torch.sigmoid(logits).squeeze().item()\n",
    "        print(p)\n",
    "        changed = False\n",
    "        if p >= prob_threshold:\n",
    "            # Commit positive: set occupancy AND mark it known\n",
    "            occ[0, 0, d, h, w] = 1.0\n",
    "            changed = (old_val != 1.0)\n",
    "        changed = (p >= prob_threshold) and (old_val != 1.0)\n",
    "\n",
    "        known[0, 0, d, h, w] = True\n",
    "        # Early stopping\n",
    "        if changed:\n",
    "            no_change_run = 0\n",
    "        else:\n",
    "            no_change_run += 1\n",
    "            if no_change_run >= no_change_patience:\n",
    "                print(f\"No change for {no_change_patience} consecutive voxels. Early stopping at i={i}.\")\n",
    "                break\n",
    "\n",
    "        if (i % 500) == 0:\n",
    "            print(f\"Predicted {i}/{len(sorted_candidates)} (p={p:.3f}, no-change run={no_change_run})\")\n",
    "\n",
    "    # Save outputs\n",
    "    output_voxels = occ.squeeze().detach().cpu().numpy()\n",
    "    np.save(\"output_voxel.npy\", output_voxels)\n",
    "    np.save(\"partial_voxel.npy\", partial.numpy())\n",
    "    np.save(\"complete_voxel.npy\", complete.numpy())\n",
    "    print(\"Inference complete. Voxels saved: output_voxel.npy, partial_voxel.npy, complete_voxel.npy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34977bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Selected sample index: 41546\n",
      "Entering prediction on 3990 candidates...\n",
      "1.0\n",
      "Predicted 0/3990 (p=1.000, no-change run=0)\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43miterative_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_SAVE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_change_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprob_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/thesis/3DLLM/llm/lib64/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36miterative_inference\u001b[39m\u001b[34m(model_path, test_samples, device, no_change_patience, prob_threshold)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Build patch + mask that match TRAINING:\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# neighborhood_raw expects (occ_grid, known_grid, b, d,h,w, window_size)\u001b[39;00m\n\u001b[32m     87\u001b[39m patch, mask_patch = neighborhood_raw(occ, known.float(), \u001b[32m0\u001b[39m, d, h, w, window)  \u001b[38;5;66;03m# patch:[1,1,ws,ws,ws], mask:[1,ws,ws,ws] bool\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_patch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m p = torch.sigmoid(logits).squeeze().item()\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(p)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/thesis/3DLLM/llm/lib64/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/thesis/3DLLM/llm/lib64/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 432\u001b[39m, in \u001b[36mIterativeVoxelModel.forward\u001b[39m\u001b[34m(self, neighbors_patch, known_mask)\u001b[39m\n\u001b[32m    426\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m km_local  \u001b[38;5;66;03m# [B, ws, ws, ws] (dtype=bool)\u001b[39;00m\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# Now run the transformer on emb patch\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# emb: [B, ws, ws, ws, d_model]\u001b[39;00m\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# we must give neighborhood_fn_patch and mask_fn_patch to transformer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m out_patch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighborhood_fn_patch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_fn_patch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, ws, ws, ws, d_model]\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# extract center voxel index\u001b[39;00m\n\u001b[32m    435\u001b[39m center = ws // \u001b[32m2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/thesis/3DLLM/llm/lib64/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/thesis/3DLLM/llm/lib64/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 252\u001b[39m, in \u001b[36mVoxelTransformer3D.forward\u001b[39m\u001b[34m(self, x, neighborhood_fn, mask_fn)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03mx: [B, D, H, W, d_model]\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighborhood_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/thesis/3DLLM/llm/lib64/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/thesis/3DLLM/llm/lib64/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 225\u001b[39m, in \u001b[36mVoxelTransformerLayer3D.forward\u001b[39m\u001b[34m(self, x, neighborhood_fn, mask_fn)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# now mask is guaranteed to be [B, ws, ws, ws]\u001b[39;00m\n\u001b[32m    222\u001b[39m \n\u001b[32m    223\u001b[39m \u001b[38;5;66;03m# attention (per-voxel)\u001b[39;00m\n\u001b[32m    224\u001b[39m tgt_norm = \u001b[38;5;28mself\u001b[39m.norm1(target)\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m attn_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m target = target + \u001b[38;5;28mself\u001b[39m.dropout(attn_out)\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# ffn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/thesis/3DLLM/llm/lib64/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/thesis/3DLLM/llm/lib64/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 156\u001b[39m, in \u001b[36mLocalAttention.forward\u001b[39m\u001b[34m(self, target_embedding, neighbor_embeddings, mask)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# Keys and values from neighbors: [B, ws^3, d_model]\u001b[39;00m\n\u001b[32m    155\u001b[39m k = \u001b[38;5;28mself\u001b[39m.k_proj(neighbor_flat)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m v = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Reshape for multi-head attention\u001b[39;00m\n\u001b[32m    159\u001b[39m q = q.view(B, \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_heads, \u001b[38;5;28mself\u001b[39m.head_dim).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# [B, num_heads, 1, head_dim]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/thesis/3DLLM/llm/lib64/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/thesis/3DLLM/llm/lib64/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/thesis/3DLLM/llm/lib64/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iterative_inference(MODEL_SAVE_PATH, test_samples, device=device, no_change_patience=200, prob_threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
