{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bea1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extension to agent copy 4.ipynb F1-score, pos weight\n",
    "from typing import Tuple, List\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "import zipfile\n",
    "import shutil\n",
    "import tempfile\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Subset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelDataLoader:\n",
    "    \"\"\"Loads and processes NPZ voxel data from a zip file\"\"\"\n",
    "\n",
    "    def __init__(self, zip_path: str):\n",
    "        # Create a temporary directory\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        print(f\"Created temporary directory: {self.temp_dir}\")\n",
    "\n",
    "        # Extract zip file\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.temp_dir)\n",
    "        print(f\"Extracted zip file to temporary directory\")\n",
    "\n",
    "        # Find all NPZ files\n",
    "        all_files = glob.glob(os.path.join(self.temp_dir, \"**/*.npz\"), recursive=True)\n",
    "        print(f\"Found {len(all_files)} total NPZ files\")\n",
    "\n",
    "        if len(all_files) == 0:\n",
    "            raise ValueError(f\"No NPZ files found in zip file\")\n",
    "\n",
    "        random.shuffle(all_files)  # Shuffle before splitting\n",
    "        cutoff = int(len(all_files))\n",
    "        self.npz_files = all_files[:cutoff]\n",
    "        print(f\"Using {len(self.npz_files)}\")\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup temporary directory when object is destroyed\"\"\"\n",
    "        try:\n",
    "            shutil.rmtree(self.temp_dir)\n",
    "            print(f\"Cleaned up temporary directory: {self.temp_dir}\")\n",
    "        except:\n",
    "            print(f\"Failed to clean up temporary directory: {self.temp_dir}\")\n",
    "\n",
    "    def load_single_file(self, file_path: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        data = np.load(file_path)\n",
    "\n",
    "        # More robust key checking\n",
    "        if 'complete' not in data or 'partial' not in data:\n",
    "            raise ValueError(f\"NPZ file {file_path} must contain both 'complete' and 'partial' arrays\")\n",
    "\n",
    "        complete = torch.from_numpy(data['complete']).float()\n",
    "        partial = torch.from_numpy(data['partial']).float()\n",
    "\n",
    "        # Verify shapes match\n",
    "        if complete.shape != partial.shape:\n",
    "            raise ValueError(f\"Shape mismatch in {file_path}: complete {complete.shape} vs partial {partial.shape}\")\n",
    "\n",
    "        return complete, partial\n",
    "\n",
    "    def get_all_data(self) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"Load all voxel pairs from all NPZ files\"\"\"\n",
    "        all_data = []\n",
    "        for file_path in self.npz_files:\n",
    "            complete, partial = self.load_single_file(file_path)\n",
    "            all_data.append((complete, partial))\n",
    "        return all_data\n",
    "\n",
    "    def get_voxel_grids(self, index: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns complete and partial voxel grids from a specific file\"\"\"\n",
    "        if index >= len(self.npz_files):\n",
    "            raise IndexError(f\"Index {index} out of range. Only {len(self.npz_files)} files available.\")\n",
    "        return self.load_single_file(self.npz_files[index])\n",
    "\n",
    "\n",
    "class VoxelDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"PyTorch Dataset for voxel completion\"\"\"\n",
    "\n",
    "    def __init__(self, zip_path: str, transform=None):\n",
    "        self.data_loader = VoxelDataLoader(zip_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader.npz_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        complete, partial = self.data_loader.get_voxel_grids(idx)\n",
    "        # Normalize to [0,1] if not already\n",
    "        complete = (complete > 0).float()\n",
    "        partial = (partial > 0).float()\n",
    "        if self.transform:\n",
    "            complete, partial = self.transform(complete, partial)\n",
    "        return complete, partial\n",
    "\n",
    "\n",
    "# Update data loader creation function\n",
    "def create_data_loader(zip_path: str, batch_size: int = 1, shuffle: bool = True, num_workers: int = 0):\n",
    "    \"\"\"Create a PyTorch DataLoader for training\"\"\"\n",
    "    dataset = VoxelDataset(zip_path)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        # pin_memory=True\n",
    "    )\n",
    "\n",
    "\n",
    "def split_dataset(dataset, train_ratio=0.8, val_ratio=0.2, seed=42):\n",
    "    n = len(dataset)\n",
    "    indices = list(range(n))\n",
    "    random.Random(seed).shuffle(indices)\n",
    "\n",
    "    # from dataset: 80% train 20% test \n",
    "    n_trainval = int(n * 0.8)\n",
    "    n_test = n - n_trainval\n",
    "    trainval_indices = indices[:n_trainval]\n",
    "    test_indices = indices[n_trainval:]\n",
    "    # from training data: 80% train 20% validation\n",
    "    n_train = int(len(trainval_indices) * 0.8)\n",
    "    train_indices = trainval_indices[:n_train]\n",
    "    val_indices = trainval_indices[n_train:]\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "def create_data_loaders(zip_path, batch_size=1, shuffle=True, num_workers=0, seed=42):\n",
    "    dataset = VoxelDataset(zip_path)\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    train_idx, val_idx, test_idx = split_dataset(dataset, seed=seed)\n",
    "    train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    val_loader = DataLoader(Subset(dataset, val_idx), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(Subset(dataset, test_idx), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "class SpatialAttention3D(nn.Module):\n",
    "    \"\"\"\n",
    "    True 3D spatial attention with proper windowing that maintains 3D structure throughout.\n",
    "    Modified to support autoregressive masking.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int = 4, window_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.head_dim = d_model // num_heads\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.qkv = nn.Conv3d(d_model, d_model * 3, kernel_size=1)\n",
    "        self.proj = nn.Conv3d(d_model, d_model, kernel_size=1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.attn_weights = None\n",
    "        \n",
    "        # 3D positional embeddings for window positions\n",
    "        self.use_pos_embed = True\n",
    "        if self.use_pos_embed:\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, 1, self.head_dim, 1, 1, 1, window_size, window_size, window_size)\n",
    "            )\n",
    "            nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def create_causal_mask_3d(self, D, H, W, device):\n",
    "        \"\"\"\n",
    "        Create a 3D causal mask for autoregressive generation.\n",
    "        Uses raster scan order: z -> y -> x (depth -> height -> width)\n",
    "        \"\"\"\n",
    "        total_positions = D * H * W\n",
    "        mask = torch.zeros(total_positions, total_positions, device=device)\n",
    "        \n",
    "        for i in range(total_positions):\n",
    "            # Convert flat index to 3D coordinates\n",
    "            z_i = i // (H * W)\n",
    "            remainder = i % (H * W)\n",
    "            y_i = remainder // W\n",
    "            x_i = remainder % W\n",
    "            \n",
    "            for j in range(total_positions):\n",
    "                z_j = j // (H * W)\n",
    "                remainder = j % (H * W)\n",
    "                y_j = remainder // W\n",
    "                x_j = remainder % W\n",
    "                \n",
    "                # Allow attention if j comes before or at i in raster scan order\n",
    "                if (z_j < z_i) or \\\n",
    "                   (z_j == z_i and y_j < y_i) or \\\n",
    "                   (z_j == z_i and y_j == y_i and x_j <= x_i):\n",
    "                    mask[i, j] = 1\n",
    "        \n",
    "        return mask\n",
    "\n",
    "    def forward(self, x, window_size=None, use_causal_mask=False):\n",
    "        B, C, D, H, W = x.shape\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, self.num_heads, self.head_dim, D, H, W)\n",
    "        k = k.view(B, self.num_heads, self.head_dim, D, H, W)\n",
    "        v = v.view(B, self.num_heads, self.head_dim, D, H, W)\n",
    "        \n",
    "        # Extract windows efficiently using unfold\n",
    "        ws = window_size if window_size is not None else self.window_size\n",
    "        pad = ws // 2\n",
    "        \n",
    "        # Pad the tensors\n",
    "        q_pad = F.pad(q, [pad]*6, mode='constant', value=0)\n",
    "        k_pad = F.pad(k, [pad]*6, mode='constant', value=0)\n",
    "        v_pad = F.pad(v, [pad]*6, mode='constant', value=0)\n",
    "        \n",
    "        # Extract windows - maintains 3D structure\n",
    "        def extract_windows(tensor):\n",
    "            windows = tensor.unfold(3, ws, 1).unfold(4, ws, 1).unfold(5, ws, 1)\n",
    "            return windows.contiguous()\n",
    "        \n",
    "        q_win = extract_windows(q_pad)\n",
    "        k_win = extract_windows(k_pad)\n",
    "        v_win = extract_windows(v_pad)\n",
    "        \n",
    "        # Get center query for each position\n",
    "        center = ws // 2\n",
    "        q_center = q_win[:, :, :, :, :, :, center, center, center]\n",
    "        \n",
    "        # Add positional embeddings to keys\n",
    "        if self.use_pos_embed:\n",
    "            k_win = k_win + self.pos_embed[:, :, :, :, :, :, :ws, :ws, :ws]\n",
    "        \n",
    "        # Expand q_center to match window dimensions\n",
    "        q_expanded = q_center.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        q_expanded = q_expanded.expand(-1, -1, -1, -1, -1, -1, ws, ws, ws)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_scores = (q_expanded * k_win).sum(dim=2) * self.scale\n",
    "        \n",
    "        # Apply causal mask if requested\n",
    "        if use_causal_mask:\n",
    "            # Create causal mask for the window\n",
    "            causal_mask = self.create_causal_mask_3d(ws, ws, ws, x.device)\n",
    "            causal_mask = causal_mask.view(1, 1, 1, 1, 1, ws, ws, ws)\n",
    "            attn_scores = attn_scores + (1 - causal_mask) * (-1e9)\n",
    "        \n",
    "        # Apply softmax\n",
    "        original_shape = attn_scores.shape\n",
    "        attn_scores_flat = attn_scores.view(B, self.num_heads, D, H, W, -1)\n",
    "        attn_weights = F.softmax(attn_scores_flat, dim=-1)\n",
    "        attn_weights_3d = attn_weights.view(original_shape)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attn_weights_expanded = attn_weights_3d.unsqueeze(2)\n",
    "        attn_out = (attn_weights_expanded * v_win).sum(dim=(-3, -2, -1))\n",
    "        \n",
    "        # Reshape back to original format\n",
    "        attn_out = attn_out.view(B, C, D, H, W)\n",
    "        \n",
    "        # Final projection\n",
    "        out = self.proj(attn_out)\n",
    "        \n",
    "        self.attn_weights = attn_weights\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class VoxelTransformerLayer3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer layer with causal masking support for autoregressive generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int = 8, window_size: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.norm1 = nn.GroupNorm(1, d_model)\n",
    "        self.norm2 = nn.GroupNorm(1, d_model)\n",
    "        \n",
    "        self.attention = SpatialAttention3D(d_model, num_heads, window_size)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Conv3d(d_model, d_model * 4, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout3d(dropout),\n",
    "            nn.Conv3d(d_model * 4, d_model, kernel_size=1),\n",
    "            nn.Dropout3d(dropout)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout3d(dropout)\n",
    "        \n",
    "    def forward(self, x, window_size=None, use_causal_mask=False):\n",
    "        # Attention block with residual connection\n",
    "        norm_x = self.norm1(x)\n",
    "        attn_out = self.attention(norm_x, window_size=window_size, use_causal_mask=use_causal_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        \n",
    "        # FFN block with residual connection\n",
    "        norm_x = self.norm2(x)\n",
    "        ffn_out = self.ffn(norm_x)\n",
    "        x = x + ffn_out\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned 3D positional encoding for voxel grids.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_grid_size: int = 16):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, d_model, max_grid_size, max_grid_size, max_grid_size)\n",
    "        )\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, D, H, W = x.shape\n",
    "        return self.pos_embed[:, :, :D, :H, :W]\n",
    "\n",
    "\n",
    "class AutoregressiveVoxelTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoregressive 3D transformer for voxel completion.\n",
    "    Predicts one voxel at a time in raster scan order.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int = 64, num_heads: int = 8, num_layers: int = 4,\n",
    "                 max_grid_size: int = 16, window_size: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Input projection - now takes 2 channels (current state + confidence/mask)\n",
    "        self.input_proj = nn.Conv3d(2, d_model, kernel_size=1)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding3D(d_model, max_grid_size)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            VoxelTransformerLayer3D(d_model, num_heads, window_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection - predicts single voxel probability\n",
    "        self.output_norm = nn.GroupNorm(1, d_model)\n",
    "        self.output_proj = nn.Conv3d(d_model, 1, kernel_size=1)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def get_next_unknown_position(self, current_grid, known_mask):\n",
    "        \"\"\"\n",
    "        Get the next unknown position in raster scan order (z->y->x).\n",
    "        Returns (z, y, x) coordinates or None if all positions are known.\n",
    "        \"\"\"\n",
    "        B, _, D, H, W = current_grid.shape\n",
    "        \n",
    "        for b in range(B):\n",
    "            for z in range(D):\n",
    "                for y in range(H):\n",
    "                    for x in range(W):\n",
    "                        if known_mask[b, 0, z, y, x] == 0:  # Unknown position\n",
    "                            return b, z, y, x\n",
    "        return None\n",
    "    \n",
    "    def forward_single_step(self, current_grid, known_mask, target_pos=None):\n",
    "        \"\"\"\n",
    "        Forward pass for a single autoregressive step.\n",
    "        Predicts the probability for the next unknown voxel.\n",
    "        \"\"\"\n",
    "        # Concatenate current grid and known mask as input\n",
    "        x = torch.cat([current_grid, known_mask], dim=1)  # [B, 2, D, H, W]\n",
    "        \n",
    "        # Project to model dimension\n",
    "        x = self.input_proj(x)  # [B, d_model, D, H, W]\n",
    "        x = x + self.pos_encoding(x)\n",
    "        \n",
    "        # Apply transformer layers with causal masking\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, window_size=self.window_size, use_causal_mask=True)\n",
    "        \n",
    "        x = self.output_norm(x)\n",
    "        x = self.output_proj(x)  # [B, 1, D, H, W]\n",
    "        \n",
    "        # If target position is specified, return only that position's prediction\n",
    "        if target_pos is not None:\n",
    "            b, z, y, x_coord = target_pos\n",
    "            return x[b, 0, z, y, x_coord].unsqueeze(0)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def generate_autoregressive(self, partial_grid, max_steps=None, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate complete voxel grid autoregressively.\n",
    "        \"\"\"\n",
    "        device = partial_grid.device\n",
    "        B, _, D, H, W = partial_grid.shape\n",
    "        \n",
    "        if max_steps is None:\n",
    "            max_steps = D * H * W\n",
    "        \n",
    "        # Initialize current state\n",
    "        current_grid = partial_grid.clone()\n",
    "        known_mask = (partial_grid > 0).float()  # 1 where voxels are known, 0 where unknown\n",
    "        \n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for step in range(max_steps):\n",
    "                # Find next unknown position\n",
    "                next_pos = self.get_next_unknown_position(current_grid, known_mask)\n",
    "                if next_pos is None:\n",
    "                    break  # All positions are known\n",
    "                \n",
    "                # Predict next voxel\n",
    "                logit = self.forward_single_step(current_grid, known_mask, next_pos)\n",
    "                \n",
    "                # Sample from distribution (with temperature)\n",
    "                prob = torch.sigmoid(logit / temperature)\n",
    "                prediction = torch.bernoulli(prob)\n",
    "                \n",
    "                # Update grid and mask\n",
    "                b, z, y, x = next_pos\n",
    "                current_grid[b, 0, z, y, x] = prediction\n",
    "                known_mask[b, 0, z, y, x] = 1\n",
    "        \n",
    "        return current_grid\n",
    "    \n",
    "    def forward(self, partial_grid, target_grid=None, teacher_forcing_ratio=1.0):\n",
    "        \"\"\"\n",
    "        Forward pass for training with teacher forcing.\n",
    "        \"\"\"\n",
    "        if target_grid is None:\n",
    "            # Inference mode - generate autoregressively\n",
    "            return self.generate_autoregressive(partial_grid)\n",
    "        \n",
    "        # Training mode with teacher forcing\n",
    "        device = partial_grid.device\n",
    "        B, _, D, H, W = partial_grid.shape\n",
    "        \n",
    "        # Initialize\n",
    "        current_grid = partial_grid.clone()\n",
    "        known_mask = (partial_grid > 0).float()\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        positions = []\n",
    "        \n",
    "        # Collect all unknown positions in raster scan order\n",
    "        unknown_positions = []\n",
    "        for b in range(B):\n",
    "            for z in range(D):\n",
    "                for y in range(H):\n",
    "                    for x in range(W):\n",
    "                        if known_mask[b, 0, z, y, x] == 0:\n",
    "                            unknown_positions.append((b, z, y, x))\n",
    "        \n",
    "        # Process each unknown position\n",
    "        for pos in unknown_positions:\n",
    "            b, z, y, x = pos\n",
    "            \n",
    "            # Predict current position\n",
    "            logit = self.forward_single_step(current_grid, known_mask, pos)\n",
    "            predictions.append(logit)\n",
    "            targets.append(target_grid[b, 0, z, y, x])\n",
    "            positions.append(pos)\n",
    "            \n",
    "            # Teacher forcing: use ground truth or prediction\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                # Use ground truth\n",
    "                current_grid[b, 0, z, y, x] = target_grid[b, 0, z, y, x]\n",
    "            else:\n",
    "                # Use prediction\n",
    "                prob = torch.sigmoid(logit)\n",
    "                prediction = torch.bernoulli(prob)\n",
    "                current_grid[b, 0, z, y, x] = prediction\n",
    "            \n",
    "            # Update known mask\n",
    "            known_mask[b, 0, z, y, x] = 1\n",
    "        \n",
    "        if predictions:\n",
    "            predictions = torch.stack(predictions)\n",
    "            targets = torch.stack(targets)\n",
    "            return predictions, targets, positions\n",
    "        else:\n",
    "            return torch.tensor([]), torch.tensor([]), []\n",
    "\n",
    "\n",
    "def autoregressive_loss(predictions, targets, criterion):\n",
    "    \"\"\"\n",
    "    Compute loss for autoregressive predictions.\n",
    "    \"\"\"\n",
    "    if len(predictions) == 0:\n",
    "        return torch.tensor(0.0, requires_grad=True)\n",
    "    \n",
    "    return criterion(predictions, targets.float())\n",
    "\n",
    "\n",
    "def train_autoregressive_model(\n",
    "    model: nn.Module,\n",
    "    train_set,\n",
    "    val_set,\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 1,\n",
    "    window_size: int = 3,\n",
    "    teacher_forcing_ratio: float = 0.8,\n",
    "    seed: int = 42\n",
    "):\n",
    "    \"\"\"Training loop for the autoregressive voxel completion model.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Use balanced BCE loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        num_samples_processed = 0\n",
    "        \n",
    "        model.train()\n",
    "        epoch_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=True)\n",
    "        \n",
    "        for batch_idx, (complete_grid, partial_grid) in enumerate(epoch_pbar):\n",
    "            complete_grid = complete_grid.to(device, non_blocking=True)\n",
    "            partial_grid = partial_grid.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Ensure correct dimensions\n",
    "            if partial_grid.dim() == 4:\n",
    "                partial_grid = partial_grid.unsqueeze(1)\n",
    "            if complete_grid.dim() == 4:\n",
    "                complete_grid = complete_grid.unsqueeze(1)\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "                # Forward pass with teacher forcing\n",
    "                predictions, targets, positions = model(\n",
    "                    partial_grid, complete_grid, teacher_forcing_ratio\n",
    "                )\n",
    "                \n",
    "                if len(predictions) > 0:\n",
    "                    loss = autoregressive_loss(predictions, targets, criterion)\n",
    "                else:\n",
    "                    loss = torch.tensor(0.0, requires_grad=True, device=device)\n",
    "            \n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_samples_processed += 1\n",
    "            \n",
    "            # Clean up\n",
    "            del complete_grid, partial_grid, predictions, targets, loss\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            epoch_pbar.set_postfix({\n",
    "                'train_loss': f'{total_loss/num_samples_processed:.4f}',\n",
    "                'samples': num_samples_processed\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = total_loss / max(num_samples_processed, 1)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for complete_grid, partial_grid in val_loader:\n",
    "                complete_grid = complete_grid.to(device, non_blocking=True)\n",
    "                partial_grid = partial_grid.to(device, non_blocking=True)\n",
    "                \n",
    "                if partial_grid.dim() == 4:\n",
    "                    partial_grid = partial_grid.unsqueeze(1)\n",
    "                if complete_grid.dim() == 4:\n",
    "                    complete_grid = complete_grid.unsqueeze(1)\n",
    "                \n",
    "                predictions, targets, positions = model(\n",
    "                    partial_grid, complete_grid, teacher_forcing_ratio=1.0\n",
    "                )\n",
    "                \n",
    "                if len(predictions) > 0:\n",
    "                    loss = autoregressive_loss(predictions, targets, criterion)\n",
    "                    val_loss += loss.item()\n",
    "                \n",
    "                val_samples += 1\n",
    "                \n",
    "                del complete_grid, partial_grid, predictions, targets, loss\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_val_loss = val_loss / max(val_samples, 1)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Time: {timedelta(seconds=int(epoch_time))}, \"\n",
    "              f\"Train loss: {avg_train_loss:.4f}, Val loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"\\nTraining completed in {timedelta(seconds=int(total_time))}\")\n",
    "    print(f\"Average time per epoch: {timedelta(seconds=int(total_time/num_epochs))}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def create_autoregressive_model(grid_size=16):\n",
    "    \"\"\"Create an autoregressive voxel completion model.\"\"\"\n",
    "    model = AutoregressiveVoxelTransformer(\n",
    "        d_model=64,\n",
    "        num_heads=8,\n",
    "        num_layers=4,\n",
    "        max_grid_size=grid_size,\n",
    "        window_size=3,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Generate complete voxel grid from partial input\n",
    "def generate_completion(model, partial_grid, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate a complete voxel grid from a partial one using autoregressive generation.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained AutoregressiveVoxelTransformer\n",
    "        partial_grid: Input partial voxel grid [B, 1, D, H, W]\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "    \n",
    "    Returns:\n",
    "        Complete voxel grid [B, 1, D, H, W]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        completed_grid = model.generate_autoregressive(partial_grid, temperature=temperature)\n",
    "    return completed_grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e30d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-auth-oauthlib\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# DRIVE_PATH = \"/content/drive/MyDrive/AUB_masters/thesis/data/partial_data_16.zip\"  # Adjust this path to match your Drive structure\n",
    "# LOCAL_PATH = \"/content/partial_data\"\n",
    "# !mkdir -p {LOCAL_PATH}\n",
    "\n",
    "# print(\"Copying data from Drive to local storage...\")\n",
    "# !cp \"{DRIVE_PATH}\" \"{LOCAL_PATH}/data.zip\"\n",
    "# zip_path = f\"{LOCAL_PATH}/data.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = \"../../chunk_data_16_flood_fill_rm_40.zip\"\n",
    "dataset = VoxelDataset(zip_path)\n",
    "\n",
    "print(f\"Total dataset size: {len(dataset)}\")\n",
    "\n",
    "train_idx, val_idx, test_idx = split_dataset(dataset, seed=42)\n",
    "train_set = torch.utils.data.Subset(dataset, train_idx)\n",
    "val_set = torch.utils.data.Subset(dataset, val_idx)\n",
    "test_set = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = VoxelCompletionTransformer(\n",
    "    d_model=48,        \n",
    "    num_heads=6,       \n",
    "    num_layers=6,      \n",
    "    window_size=3,    \n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "torch.cuda.empty_cache()\n",
    "# best model before this was num_epochs=2\n",
    "train_model(model, train_set, val_set, num_epochs=5, batch_size=2, lambda_consistency=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec58e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"../../models/model_rm_40_new_att/trained_model_rm_40_dmodel_96_ws_3_new_att.pth\"\n",
    "# os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "# }, MODEL_SAVE_PATH)\n",
    "# print(f\"Model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d3323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporary directory: /tmp/tmpiqt353g7\n",
      "Extracted zip file to temporary directory\n",
      "Found 256571 total NPZ files\n",
      "Using 256571\n",
      "Loaded 51315 test samples from ../../test_data/test_data_rm_40_new_att/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import json \n",
    "\n",
    "# load_dir = \"../../test_data/test_data_rm_40/\"\n",
    "# test_samples = []\n",
    "\n",
    "# for file in sorted(glob.glob(os.path.join(load_dir, \"test_*.npz\"))):\n",
    "#     data = np.load(file)\n",
    "#     complete = torch.from_numpy(data['complete']).float()\n",
    "#     partial = torch.from_numpy(data['partial']).float()\n",
    "#     test_samples.append((complete, partial))\n",
    "    \n",
    "zip_path = \"../../chunk_data_16_flood_fill_rm_40.zip\"\n",
    "dataset = VoxelDataset(zip_path)\n",
    "\n",
    "test_dir = \"../../test_data/test_data_rm_40_new_att/\"\n",
    "test_indices_file = os.path.join(test_dir, \"test_indices.json\")\n",
    "\n",
    "with open(test_indices_file, \"r\") as f:\n",
    "    test_idx = json.load(f)\n",
    "\n",
    "test_samples = []\n",
    "for idx in test_idx:\n",
    "    complete, partial = dataset[idx]\n",
    "    test_samples.append((complete, partial))\n",
    "\n",
    "print(f\"Loaded {len(test_samples)} test samples from {test_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4fb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Updated test_model to use test_set ---\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "def test_model(model_path, test_set, sample_idx=0, threshold=0.5, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VoxelCompletionTransformer(\n",
    "        d_model=96,        \n",
    "        num_heads=6,       \n",
    "        num_layers=6,      \n",
    "        window_size=3,    \n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    # If using multi-gpu\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "    # Remove 'module.' prefix if present (from DataParallel)\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace('module.', '') if k.startswith('module.') else k\n",
    "        new_state_dict[new_key] = v\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    \n",
    "    # if using one gpu\n",
    "    # checkpoint = torch.load(model_path, map_location=device)\n",
    "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    model.eval()\n",
    "    complete, partial = test_set[sample_idx]\n",
    "\n",
    "    filled_complete = (complete > 0).sum()\n",
    "    # Of those, how many are also filled in partial?\n",
    "    filled_partial = ((complete > 0) & (partial > 0)).sum()\n",
    "    \n",
    "    missing_percent = (filled_complete - filled_partial) / filled_complete\n",
    "\n",
    "    # if missing_percent > 0.3:\n",
    "    print(\"Missing Percentage: \", missing_percent)\n",
    "\n",
    "    ptl = partial\n",
    "    partial = partial.unsqueeze(0).unsqueeze(0).to(device)  # [1, 1, D, H, W]\n",
    "    with torch.no_grad():\n",
    "\n",
    "        output = model(partial)\n",
    "        \n",
    "        with open(\"model_summary.txt\", \"w\") as f:\n",
    "            f.write(str(summary(model, input_size=(1, 1, 16, 16, 16))))\n",
    "\n",
    "        output = torch.sigmoid(output)\n",
    "        # output[0, 0][ptl == 1] = 1.0\n",
    "        output = output.squeeze().cpu()\n",
    "    print(\"Inference complete.\")\n",
    "    print(\"Partial shape:\", partial.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            for k in range(output.shape[2]):\n",
    "                if output[i, j, k] > threshold:\n",
    "                    output[i, j, k] = 1.0\n",
    "                else:\n",
    "                    output[i, j, k] = 0.0\n",
    "    out_path = \"output_voxel.npy\"\n",
    "    complete_path = \"complete_voxel.npy\"\n",
    "    partial_path = \"partial_voxel.npy\"\n",
    "    np.save(out_path, output.numpy())\n",
    "    np.save(complete_path, complete)\n",
    "    np.save(partial_path, ptl)\n",
    "    print(\"Sample Index: \", sample_idx)\n",
    "    print(f\"Output saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fe5480df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Percentage:  tensor(0.3994)\n",
      "Inference complete.\n",
      "Partial shape: torch.Size([1, 1, 16, 16, 16])\n",
      "Output shape: torch.Size([16, 16, 16])\n",
      "Sample Index:  0\n",
      "Output saved to output_voxel.npy\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_model(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    test_set=test_samples,\n",
    "    sample_idx=\n",
    "    0,\n",
    "    # random.randint(0, len(test_samples) - 1),\n",
    "    # 27183,\n",
    "    threshold=0.7,\n",
    "    device=device\n",
    ")\n",
    "# 1306, 4159, 49191, 42677, \n",
    "# 49173, 8676, 22131\n",
    "# 40904, 646\n",
    "\n",
    "# 6429\n",
    "\n",
    "# new (saved npz dmodel 48)\n",
    "# 12625, 5997\n",
    "# 8874 (doesnt fill all)\n",
    "\n",
    "# new attn\n",
    "# 39466\n",
    "# 25371\n",
    "# 10393\n",
    "# 43919\n",
    "# 14104\n",
    "# 7881\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c26db6",
   "metadata": {},
   "source": [
    "# Test Function w/ Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f086b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Updated test_model_metrics to compute CD, IoU, F1, HD ---\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist, directed_hausdorff\n",
    "\n",
    "def test_model_metrics(model_path, test_set, sample_idx=0, threshold=0.5, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VoxelCompletionTransformer(\n",
    "        d_model=96,        \n",
    "        num_heads=6,       \n",
    "        num_layers=6,      \n",
    "        window_size=3,    \n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    # checkpoint = torch.load(model_path, map_location=device)\n",
    "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace('module.', '') if k.startswith('module.') else k\n",
    "        new_state_dict[new_key] = v\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "    model.eval()\n",
    "    complete, partial = test_set[sample_idx]\n",
    "\n",
    "    filled_complete = (complete > 0).sum()\n",
    "    filled_partial = ((complete > 0) & (partial > 0)).sum()\n",
    "    missing_percent = (filled_complete - filled_partial) / filled_complete\n",
    "    print(\"Missing Percentage: \", missing_percent)\n",
    "\n",
    "    ptl = partial\n",
    "    partial = partial.unsqueeze(0).unsqueeze(0).to(device)  # [1, 1, D, H, W]\n",
    "    with torch.no_grad():\n",
    "        output = model(partial)\n",
    "        output = torch.sigmoid(output)\n",
    "        # output[0, 0][ptl == 1] = 1.0\n",
    "        output = output.squeeze().cpu()\n",
    "\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            for k in range(output.shape[2]):\n",
    "                if output[i, j, k] > threshold:\n",
    "                    output[i, j, k] = 1.0\n",
    "                else:\n",
    "                    output[i, j, k] = 0.0\n",
    "    print(\"Inference complete.\")\n",
    "    print(\"Partial shape:\", partial.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "    # Binarize output\n",
    "    pred = (output > threshold).float()\n",
    "    gt = (complete > 0).float()\n",
    "\n",
    "    # IoU\n",
    "    intersection = ((pred == 1) & (gt == 1)).sum().item()\n",
    "    union = ((pred == 1) | (gt == 1)).sum().item()\n",
    "    iou = intersection / (union + 1e-8)\n",
    "    print(f\"IoU: {iou:.4f}\")\n",
    "\n",
    "    # F1 Score\n",
    "    tp = ((pred == 1) & (gt == 1)).sum().item()\n",
    "    fp = ((pred == 1) & (gt == 0)).sum().item()\n",
    "    fn = ((pred == 0) & (gt == 1)).sum().item()\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Chamfer Distance (CD)\n",
    "    def get_points(voxel):\n",
    "        return np.argwhere(voxel.numpy() > 0.5)\n",
    "    pred_points = get_points(pred)\n",
    "    gt_points = get_points(gt)\n",
    "    if len(pred_points) > 0 and len(gt_points) > 0:\n",
    "        dist_pred_to_gt = cdist(pred_points, gt_points)\n",
    "        dist_gt_to_pred = cdist(gt_points, pred_points)\n",
    "        chamfer = np.mean(np.min(dist_pred_to_gt, axis=1)) + np.mean(np.min(dist_gt_to_pred, axis=1))\n",
    "        print(f\"Chamfer Distance: {chamfer:.4f}\")\n",
    "    else:\n",
    "        print(\"Chamfer Distance: N/A (empty prediction or ground truth)\")\n",
    "\n",
    "    # Hausdorff Distance (HD, UHD)\n",
    "    if len(pred_points) > 0 and len(gt_points) > 0:\n",
    "        hd_pred_gt = directed_hausdorff(pred_points, gt_points)[0]\n",
    "        hd_gt_pred = directed_hausdorff(gt_points, pred_points)[0]\n",
    "        hausdorff = max(hd_pred_gt, hd_gt_pred)\n",
    "        print(f\"Hausdorff Distance: {hausdorff:.4f}\")\n",
    "        print(f\"Unidirectional HD (pred→gt): {hd_pred_gt:.4f}, (gt→pred): {hd_gt_pred:.4f}\")\n",
    "    else:\n",
    "        print(\"Hausdorff Distance: N/A (empty prediction or ground truth)\")\n",
    "\n",
    "    out_path = \"output_voxel.npy\"\n",
    "    complete_path = \"complete_voxel.npy\"\n",
    "    partial_path = \"partial_voxel.npy\"\n",
    "    np.save(out_path, output.numpy())\n",
    "    np.save(complete_path, complete)\n",
    "    np.save(partial_path, ptl)\n",
    "    print(\"Sample Index: \", sample_idx)\n",
    "    print(f\"Output saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f356fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Percentage:  tensor(0.4010)\n",
      "Inference complete.\n",
      "Partial shape: torch.Size([1, 1, 16, 16, 16])\n",
      "Output shape: torch.Size([16, 16, 16])\n",
      "IoU: 0.6912\n",
      "F1 Score: 0.8174\n",
      "Chamfer Distance: 0.4287\n",
      "Hausdorff Distance: 5.3852\n",
      "Unidirectional HD (pred→gt): 5.3852, (gt→pred): 1.0000\n",
      "Sample Index:  37863\n",
      "Output saved to output_voxel.npy\n"
     ]
    }
   ],
   "source": [
    "test_model_metrics(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    test_set=test_samples,\n",
    "    sample_idx=random.randint(0, len(test_samples) - 1),\n",
    "    # sample_idx=31878,\n",
    "    threshold=0.5\n",
    ")\n",
    "# 25451\n",
    "# 31878\n",
    "# 41617"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde604aa",
   "metadata": {},
   "source": [
    "# Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def test_model_with_attention(model_path, test_set, sample_idx=0, threshold=0.5, device=None):\n",
    "    \"\"\"Modified inference function that captures attention weights\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = VoxelCompletionTransformer(\n",
    "        d_model=48,\n",
    "        num_heads=6,\n",
    "        num_layers=6,\n",
    "        window_size=3,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Hook to capture attention weights\n",
    "    attention_weights = []\n",
    "    \n",
    "    def attention_hook(module, input, output):\n",
    "        # Capture attention weights from each attention layer\n",
    "        if hasattr(module, 'attn_weights'):\n",
    "            attention_weights.append(module.attn_weights.detach().cpu())\n",
    "    \n",
    "    # Register hooks on attention layers\n",
    "    for name, module in model.named_modules():\n",
    "        if 'attention' in name.lower() or 'attn' in name.lower():\n",
    "            module.register_forward_hook(attention_hook)\n",
    "    \n",
    "    complete, partial = test_set[sample_idx]\n",
    "    filled_complete = (complete > 0).sum()\n",
    "    filled_partial = ((complete > 0) & (partial > 0)).sum()\n",
    "    missing_percent = (filled_complete - filled_partial) / filled_complete\n",
    "    \n",
    "    print(\"Missing Percentage: \", missing_percent)\n",
    "    ptl = partial\n",
    "    partial = partial.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(partial)\n",
    "    \n",
    "    output = torch.sigmoid(output)\n",
    "    output[0, 0][ptl == 1] = 1.0\n",
    "    output = output.squeeze().cpu()\n",
    "    \n",
    "    # Process output\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            for k in range(output.shape[2]):\n",
    "                if output[i, j, k] > threshold:\n",
    "                    output[i, j, k] = 1.0\n",
    "                else:\n",
    "                    output[i, j, k] = 0.0\n",
    "    \n",
    "    # Save outputs\n",
    "    out_path = \"output_voxel.npy\"\n",
    "    complete_path = \"complete_voxel.npy\"\n",
    "    partial_path = \"partial_voxel.npy\"\n",
    "    \n",
    "    np.save(out_path, output.numpy())\n",
    "    np.save(complete_path, complete)\n",
    "    np.save(partial_path, ptl)\n",
    "    \n",
    "    print(\"Sample Index: \", sample_idx)\n",
    "    print(f\"Output saved to {out_path}\")\n",
    "    \n",
    "    return output, attention_weights, partial.squeeze(), complete, ptl\n",
    "\n",
    "def visualize_attention_weights(attention_weights, layer_idx=0, head_idx=0, save_path=\"attention_viz.png\"):\n",
    "    \"\"\"Visualize attention weights for a specific layer and head\"\"\"\n",
    "    if len(attention_weights) == 0:\n",
    "        print(\"No attention weights captured. Make sure your model has attention layers with 'attention_weights' attribute.\")\n",
    "        return\n",
    "    \n",
    "    if layer_idx >= len(attention_weights):\n",
    "        print(f\"Layer {layer_idx} not found. Available layers: 0-{len(attention_weights)-1}\")\n",
    "        return\n",
    "    \n",
    "    attn = attention_weights[layer_idx]\n",
    "    print(f\"Attention weights shape: {attn.shape}\")\n",
    "    \n",
    "    # Handle different attention weight shapes\n",
    "    if len(attn.shape) == 6:  # [batch, heads, D, H, W, window_size]\n",
    "        attn = attn[0, head_idx]  # Select first batch, specific head -> [D, H, W, window_size]\n",
    "        # Reshape to 2D for visualization: [D*H*W, window_size]\n",
    "        attn = attn.reshape(-1, attn.shape[-1])\n",
    "    elif len(attn.shape) == 5:  # [heads, D, H, W, window_size]\n",
    "        attn = attn[head_idx]  # Select specific head -> [D, H, W, window_size]\n",
    "        attn = attn.reshape(-1, attn.shape[-1])\n",
    "    elif len(attn.shape) == 4:  # [batch, heads, seq_len, seq_len]\n",
    "        attn = attn[0, head_idx]  # Select first batch, specific head\n",
    "    elif len(attn.shape) == 3:  # [heads, seq_len, seq_len]\n",
    "        attn = attn[head_idx]  # Select specific head\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.heatmap(attn.numpy(), cmap='Blues', cbar=True, square=True)\n",
    "    plt.title(f'Attention Weights - Layer {layer_idx}, Head {head_idx}')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    \n",
    "    # Attention distribution histogram\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(attn.numpy().flatten(), bins=50, alpha=0.7, color='skyblue')\n",
    "    plt.title('Attention Weight Distribution')\n",
    "    plt.xlabel('Attention Weight')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Max attention per query\n",
    "    plt.subplot(2, 2, 3)\n",
    "    max_attn = torch.max(attn, dim=1)[0]\n",
    "    plt.plot(max_attn.numpy(), marker='o', linewidth=2, markersize=4)\n",
    "    plt.title('Maximum Attention per Query Position')\n",
    "    plt.xlabel('Query Position')\n",
    "    plt.ylabel('Max Attention Weight')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Attention entropy (attention spread)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    entropy = -torch.sum(attn * torch.log(attn + 1e-9), dim=1)\n",
    "    plt.plot(entropy.numpy(), marker='s', linewidth=2, markersize=4, color='red')\n",
    "    plt.title('Attention Entropy per Query Position')\n",
    "    plt.xlabel('Query Position')\n",
    "    plt.ylabel('Entropy')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_3d_attention(attention_weights, voxel_shape=(16, 16, 16), layer_idx=0, head_idx=0, \n",
    "                          query_pos=None, save_path=\"attention_3d.png\"):\n",
    "    \"\"\"Visualize 3D attention patterns for voxel data\"\"\"\n",
    "    if len(attention_weights) == 0:\n",
    "        print(\"No attention weights captured.\")\n",
    "        return\n",
    "    \n",
    "    attn = attention_weights[layer_idx]\n",
    "    print(f\"Original attention shape: {attn.shape}\")\n",
    "    \n",
    "    # Handle 6D attention weights [batch, heads, D, H, W, window_size]\n",
    "    if len(attn.shape) == 6:\n",
    "        attn = attn[0, head_idx]  # Select first batch and head -> [D, H, W, window_size]\n",
    "    elif len(attn.shape) == 5:\n",
    "        attn = attn[head_idx]  # Select head -> [D, H, W, window_size]\n",
    "    elif len(attn.shape) == 4:\n",
    "        attn = attn[0, head_idx]\n",
    "    elif len(attn.shape) == 3:\n",
    "        attn = attn[head_idx]\n",
    "    \n",
    "    print(f\"After selection: {attn.shape}\")\n",
    "    \n",
    "    # For windowed attention, we need to handle this differently\n",
    "    if len(attn.shape) == 4:  # [D, H, W, window_size]\n",
    "        D, H, W, window_size = attn.shape\n",
    "        \n",
    "        # Select a specific voxel position to visualize its attention\n",
    "        if query_pos is None:\n",
    "            query_d, query_h, query_w = D//2, H//2, W//2  # Center voxel\n",
    "        else:\n",
    "            query_d, query_h, query_w = query_pos\n",
    "        \n",
    "        # Get attention weights for this query position\n",
    "        attention_values = attn[query_d, query_h, query_w].numpy()  # [window_size]\n",
    "        \n",
    "        # Create 3D visualization showing attention within the local window\n",
    "        fig = plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Calculate window positions relative to query\n",
    "        window_radius = int(np.cbrt(window_size)) // 2  # Assuming cubic window\n",
    "        positions = []\n",
    "        \n",
    "        idx = 0\n",
    "        for dz in range(-window_radius, window_radius + 1):\n",
    "            for dy in range(-window_radius, window_radius + 1):\n",
    "                for dx in range(-window_radius, window_radius + 1):\n",
    "                    if idx < window_size:\n",
    "                        # Absolute positions\n",
    "                        abs_d = max(0, min(D-1, query_d + dz))\n",
    "                        abs_h = max(0, min(H-1, query_h + dy))\n",
    "                        abs_w = max(0, min(W-1, query_w + dx))\n",
    "                        positions.append([abs_w, abs_h, abs_d])  # x, y, z\n",
    "                        idx += 1\n",
    "        \n",
    "        positions = np.array(positions)\n",
    "        \n",
    "        # 3D scatter plot\n",
    "        ax1 = fig.add_subplot(131, projection='3d')\n",
    "        scatter = ax1.scatter(positions[:, 0], positions[:, 1], positions[:, 2], \n",
    "                             c=attention_values, cmap='viridis', s=50, alpha=0.8)\n",
    "        \n",
    "        # Highlight query position\n",
    "        query_3d = [query_w, query_h, query_d]\n",
    "        ax1.scatter(query_3d[0], query_3d[1], query_3d[2], \n",
    "                   c='red', s=200, marker='*', edgecolors='black', linewidth=2)\n",
    "        \n",
    "        ax1.set_xlabel('X (W)')\n",
    "        ax1.set_ylabel('Y (H)')\n",
    "        ax1.set_zlabel('Z (D)')\n",
    "        ax1.set_title(f'3D Attention Pattern\\nQuery at ({query_w}, {query_h}, {query_d})')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=ax1, shrink=0.5)\n",
    "        cbar.set_label('Attention Weight')\n",
    "        \n",
    "        # 2D projections\n",
    "        ax2 = fig.add_subplot(132)\n",
    "        scatter2 = ax2.scatter(positions[:, 0], positions[:, 1], c=attention_values, cmap='viridis', s=30)\n",
    "        ax2.scatter(query_3d[0], query_3d[1], c='red', s=100, marker='*', edgecolors='black')\n",
    "        ax2.set_xlabel('X (W)')\n",
    "        ax2.set_ylabel('Y (H)')\n",
    "        ax2.set_title('XY Projection')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax3 = fig.add_subplot(133)\n",
    "        scatter3 = ax3.scatter(positions[:, 0], positions[:, 2], c=attention_values, cmap='viridis', s=30)\n",
    "        ax3.scatter(query_3d[0], query_3d[2], c='red', s=100, marker='*', edgecolors='black')\n",
    "        ax3.set_xlabel('X (W)')\n",
    "        ax3.set_ylabel('Z (D)')\n",
    "        ax3.set_title('XZ Projection')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return query_d, query_h, query_w\n",
    "    \n",
    "    else:\n",
    "        # Fallback for other shapes\n",
    "        print(f\"Unexpected attention shape: {attn.shape}\")\n",
    "        return None\n",
    "\n",
    "def visualize_attention_across_layers(attention_weights, save_path=\"attention_layers.png\"):\n",
    "    \"\"\"Compare attention patterns across different layers\"\"\"\n",
    "    if len(attention_weights) == 0:\n",
    "        print(\"No attention weights captured.\")\n",
    "        return\n",
    "    \n",
    "    num_layers = len(attention_weights)\n",
    "    fig, axes = plt.subplots(2, (num_layers + 1) // 2, figsize=(4 * num_layers, 8))\n",
    "    \n",
    "    if num_layers == 1:\n",
    "        axes = [axes]\n",
    "    elif num_layers > 1:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, attn in enumerate(attention_weights):\n",
    "        # Handle 6D attention weights\n",
    "        if len(attn.shape) == 6:  # [batch, heads, D, H, W, window_size]\n",
    "            attn = attn[0, 0]  # First batch, first head -> [D, H, W, window_size]\n",
    "            # Average over spatial dimensions to get [window_size] pattern\n",
    "            attn = attn.mean(dim=(0, 1, 2))  # Average over D, H, W\n",
    "        elif len(attn.shape) == 5:  # [heads, D, H, W, window_size]\n",
    "            attn = attn[0]  # First head -> [D, H, W, window_size]\n",
    "            attn = attn.mean(dim=(0, 1, 2))\n",
    "        elif len(attn.shape) == 4:  # [batch, heads, seq_len, seq_len]\n",
    "            attn = attn[0, 0]  # First batch, first head\n",
    "        elif len(attn.shape) == 3:  # [heads, seq_len, seq_len]\n",
    "            attn = attn[0]  # First head\n",
    "        \n",
    "        ax = axes[i] if num_layers > 1 else axes\n",
    "        \n",
    "        # If we have a 1D attention pattern (from averaging), show as bar plot\n",
    "        if len(attn.shape) == 1:\n",
    "            ax.bar(range(len(attn)), attn.numpy(), alpha=0.7, color='skyblue')\n",
    "            ax.set_title(f'Layer {i} - Avg Attention Pattern')\n",
    "            ax.set_xlabel('Window Position')\n",
    "            ax.set_ylabel('Average Attention Weight')\n",
    "        else:\n",
    "            # Create heatmap for 2D attention\n",
    "            im = ax.imshow(attn.numpy(), cmap='Blues', aspect='auto')\n",
    "            ax.set_title(f'Layer {i}')\n",
    "            ax.set_xlabel('Key Position')\n",
    "            ax.set_ylabel('Query Position')\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(num_layers, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def analyze_windowed_attention(attention_weights, layer_idx=0, head_idx=0, save_path=\"windowed_attention.png\"):\n",
    "    \"\"\"Analyze attention patterns in windowed (local) attention\"\"\"\n",
    "    if len(attention_weights) == 0:\n",
    "        print(\"No attention weights captured.\")\n",
    "        return\n",
    "    \n",
    "    attn = attention_weights[layer_idx]\n",
    "    \n",
    "    # Handle 6D attention weights [batch, heads, D, H, W, window_size]\n",
    "    if len(attn.shape) == 6:\n",
    "        attn = attn[0, head_idx]  # Select first batch and head -> [D, H, W, window_size]\n",
    "    elif len(attn.shape) == 5:\n",
    "        attn = attn[head_idx]  # Select head -> [D, H, W, window_size]\n",
    "    \n",
    "    print(f\"Analyzing windowed attention with shape: {attn.shape}\")\n",
    "    \n",
    "    D, H, W, window_size = attn.shape\n",
    "    \n",
    "    # Convert to numpy for easier manipulation\n",
    "    attn_np = attn.numpy()\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Average attention pattern across all spatial positions\n",
    "    avg_attention = np.mean(attn_np, axis=(0, 1, 2))  # Average over D, H, W\n",
    "    axes[0, 0].bar(range(window_size), avg_attention, alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].set_title('Average Attention Pattern')\n",
    "    axes[0, 0].set_xlabel('Window Position')\n",
    "    axes[0, 0].set_ylabel('Average Attention Weight')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Attention variance across spatial positions\n",
    "    var_attention = np.var(attn_np, axis=(0, 1, 2))\n",
    "    axes[0, 1].bar(range(window_size), var_attention, alpha=0.7, color='lightcoral')\n",
    "    axes[0, 1].set_title('Attention Variance')\n",
    "    axes[0, 1].set_xlabel('Window Position')\n",
    "    axes[0, 1].set_ylabel('Variance')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Attention entropy distribution\n",
    "    # Calculate entropy for each spatial position\n",
    "    entropy_map = np.zeros((D, H, W))\n",
    "    for d in range(D):\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "                probs = attn_np[d, h, w]\n",
    "                probs = probs + 1e-9  # Add small epsilon to avoid log(0)\n",
    "                entropy_map[d, h, w] = -np.sum(probs * np.log(probs))\n",
    "    \n",
    "    # Show entropy for middle slice\n",
    "    middle_slice = entropy_map[D//2, :, :]\n",
    "    im1 = axes[0, 2].imshow(middle_slice, cmap='viridis', aspect='auto')\n",
    "    axes[0, 2].set_title(f'Attention Entropy (Slice D={D//2})')\n",
    "    axes[0, 2].set_xlabel('W')\n",
    "    axes[0, 2].set_ylabel('H')\n",
    "    plt.colorbar(im1, ax=axes[0, 2])\n",
    "    \n",
    "    # 4. Attention focus (max attention weight) across space\n",
    "    max_attention = np.max(attn_np, axis=-1)  # Max over window_size\n",
    "    middle_slice_max = max_attention[D//2, :, :]\n",
    "    im2 = axes[1, 0].imshow(middle_slice_max, cmap='Blues', aspect='auto')\n",
    "    axes[1, 0].set_title(f'Max Attention (Slice D={D//2})')\n",
    "    axes[1, 0].set_xlabel('W')\n",
    "    axes[1, 0].set_ylabel('H')\n",
    "    plt.colorbar(im2, ax=axes[1, 0])\n",
    "    \n",
    "    # 5. Attention distribution histogram\n",
    "    axes[1, 1].hist(attn_np.flatten(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[1, 1].set_title('Attention Weight Distribution')\n",
    "    axes[1, 1].set_xlabel('Attention Weight')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Window position preference\n",
    "    # Reshape window to 3D coordinates assuming cubic window\n",
    "    window_cube_size = int(np.cbrt(window_size))\n",
    "    if window_cube_size ** 3 == window_size:\n",
    "        window_3d = avg_attention.reshape(window_cube_size, window_cube_size, window_cube_size)\n",
    "        # Show middle slice of the window\n",
    "        middle_window_slice = window_3d[window_cube_size//2, :, :]\n",
    "        im3 = axes[1, 2].imshow(middle_window_slice, cmap='Reds', aspect='auto')\n",
    "        axes[1, 2].set_title(f'Window Attention Pattern (Middle Slice)')\n",
    "        axes[1, 2].set_xlabel('Window W')\n",
    "        axes[1, 2].set_ylabel('Window H')\n",
    "        plt.colorbar(im3, ax=axes[1, 2])\n",
    "    else:\n",
    "        # If not cubic, show as 1D\n",
    "        axes[1, 2].plot(avg_attention, marker='o', linewidth=2, markersize=4)\n",
    "        axes[1, 2].set_title('Window Attention Pattern')\n",
    "        axes[1, 2].set_xlabel('Window Position')\n",
    "        axes[1, 2].set_ylabel('Attention Weight')\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"\\nAttention Statistics:\")\n",
    "    print(f\"Average attention range: {avg_attention.min():.4f} - {avg_attention.max():.4f}\")\n",
    "    print(f\"Most attended window position: {np.argmax(avg_attention)}\")\n",
    "    print(f\"Least attended window position: {np.argmin(avg_attention)}\")\n",
    "    print(f\"Average entropy: {np.mean(entropy_map):.4f}\")\n",
    "    print(f\"Entropy range: {entropy_map.min():.4f} - {entropy_map.max():.4f}\")\n",
    "\n",
    "\n",
    "output, attention_weights, partial, complete, ptl = test_model_with_attention(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    test_set=test_set,\n",
    "    sample_idx=0\n",
    ")\n",
    "\n",
    "# Visualize attention weights\n",
    "if attention_weights:\n",
    "    analyze_windowed_attention(attention_weights, layer_idx=0, head_idx=0)\n",
    "    visualize_attention_weights(attention_weights, layer_idx=0, head_idx=0)\n",
    "    visualize_3d_attention(attention_weights, layer_idx=0, head_idx=0)\n",
    "    visualize_attention_across_layers(attention_weights)\n",
    "else:\n",
    "    print(\"No attention weights captured. You may need to modify your model to store attention weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a172dae8",
   "metadata": {},
   "source": [
    "# multi-step inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "01302aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_multi_step(\n",
    "    model_path, \n",
    "    test_set, \n",
    "    sample_idx=0, \n",
    "    start_threshold=0.7, \n",
    "    threshold_step=0.05, \n",
    "    num_steps=3, \n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run multi-step inference on a test sample, binarizing and feeding output as next partial.\n",
    "    Args:\n",
    "        model_path: Path to model checkpoint\n",
    "        test_set: List of (complete, partial) tuples\n",
    "        sample_idx: Index of test sample\n",
    "        start_threshold: Initial threshold for binarization\n",
    "        threshold_step: Amount to decrease threshold each step\n",
    "        num_steps: Number of thresholds to try\n",
    "        device: torch.device\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VoxelCompletionTransformer(\n",
    "        d_model=96,        \n",
    "        num_heads=6,       \n",
    "        num_layers=6,      \n",
    "        window_size=3,    \n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    # Remove 'module.' prefix if present (from DataParallel)\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace('module.', '') if k.startswith('module.') else k\n",
    "        new_state_dict[new_key] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "    complete, partial = test_set[sample_idx]\n",
    "    ptl = partial\n",
    "    partial = partial.unsqueeze(0).unsqueeze(0).to(device)  # [1, 1, D, H, W]\n",
    "    thresholds = [start_threshold - i * threshold_step for i in range(num_steps)]\n",
    "    with torch.no_grad():\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            output = model(partial)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.squeeze().cpu()\n",
    "            # Binarize output and use as next partial\n",
    "            for x in range(output.shape[0]):\n",
    "                for y in range(output.shape[1]):\n",
    "                    for z in range(output.shape[2]):\n",
    "                        if output[x, y, z] > threshold:\n",
    "                            output[x, y, z] = 1.0\n",
    "                        else:\n",
    "                            output[x, y, z] = 0.0\n",
    "            print(f\"\\nThreshold {threshold:.2f} (step {i+1}/{num_steps}):\")\n",
    "            print(f\"  Predicted filled voxels: {int(output.sum().item())}\")\n",
    "            # Prepare for next step\n",
    "            partial = output.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    out_path = \"output_voxel.npy\"\n",
    "    complete_path = \"complete_voxel.npy\"\n",
    "    partial_path = \"partial_voxel.npy\"\n",
    "    np.save(out_path, output.numpy())\n",
    "    np.save(complete_path, complete)\n",
    "    np.save(partial_path, ptl)\n",
    "    print(\"Sample Index: \", sample_idx)\n",
    "    print(f\"Output saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8a845fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold 0.70 (step 1/3):\n",
      "  Predicted filled voxels: 434\n",
      "\n",
      "Threshold 0.65 (step 2/3):\n",
      "  Predicted filled voxels: 1205\n",
      "\n",
      "Threshold 0.60 (step 3/3):\n",
      "  Predicted filled voxels: 2246\n",
      "Sample Index:  0\n",
      "Output saved to output_voxel.npy\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_model_multi_step(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    test_set=test_samples,\n",
    "    sample_idx=\n",
    "    0,\n",
    "    # random.randint(0, len(test_samples) - 1),\n",
    "    start_threshold=0.7, \n",
    "    threshold_step=0.05, \n",
    "    num_steps=3, \n",
    "    device=device\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
