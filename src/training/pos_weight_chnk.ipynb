{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bea1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extension to agent copy 4.ipynb F1-score, pos weight\n",
    "from typing import Tuple, List\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "import zipfile\n",
    "import shutil\n",
    "import tempfile\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Subset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89f6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelDataLoader:\n",
    "    \"\"\"Loads and processes NPZ voxel data from a zip file\"\"\"\n",
    "\n",
    "    def __init__(self, zip_path: str):\n",
    "        # Create a temporary directory\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        print(f\"Created temporary directory: {self.temp_dir}\")\n",
    "\n",
    "        # Extract zip file\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.temp_dir)\n",
    "        print(f\"Extracted zip file to temporary directory\")\n",
    "\n",
    "        # Find all NPZ files\n",
    "        all_files = glob.glob(os.path.join(self.temp_dir, \"**/*.npz\"), recursive=True)\n",
    "        print(f\"Found {len(all_files)} total NPZ files\")\n",
    "\n",
    "        if len(all_files) == 0:\n",
    "            raise ValueError(f\"No NPZ files found in zip file\")\n",
    "\n",
    "        random.shuffle(all_files)  # Shuffle before splitting\n",
    "        cutoff = int(len(all_files))\n",
    "        self.npz_files = all_files[:cutoff]\n",
    "        print(f\"Using {len(self.npz_files)}\")\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup temporary directory when object is destroyed\"\"\"\n",
    "        try:\n",
    "            shutil.rmtree(self.temp_dir)\n",
    "            print(f\"Cleaned up temporary directory: {self.temp_dir}\")\n",
    "        except:\n",
    "            print(f\"Failed to clean up temporary directory: {self.temp_dir}\")\n",
    "\n",
    "    def load_single_file(self, file_path: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        data = np.load(file_path)\n",
    "\n",
    "        # More robust key checking\n",
    "        if 'complete' not in data or 'partial' not in data:\n",
    "            raise ValueError(f\"NPZ file {file_path} must contain both 'complete' and 'partial' arrays\")\n",
    "\n",
    "        complete = torch.from_numpy(data['complete']).float()\n",
    "        partial = torch.from_numpy(data['partial']).float()\n",
    "\n",
    "        # Verify shapes match\n",
    "        if complete.shape != partial.shape:\n",
    "            raise ValueError(f\"Shape mismatch in {file_path}: complete {complete.shape} vs partial {partial.shape}\")\n",
    "\n",
    "        return complete, partial\n",
    "\n",
    "    def get_all_data(self) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"Load all voxel pairs from all NPZ files\"\"\"\n",
    "        all_data = []\n",
    "        for file_path in self.npz_files:\n",
    "            complete, partial = self.load_single_file(file_path)\n",
    "            all_data.append((complete, partial))\n",
    "        return all_data\n",
    "\n",
    "    def get_voxel_grids(self, index: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns complete and partial voxel grids from a specific file\"\"\"\n",
    "        if index >= len(self.npz_files):\n",
    "            raise IndexError(f\"Index {index} out of range. Only {len(self.npz_files)} files available.\")\n",
    "        return self.load_single_file(self.npz_files[index])\n",
    "\n",
    "\n",
    "class VoxelDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"PyTorch Dataset for voxel completion\"\"\"\n",
    "\n",
    "    def __init__(self, zip_path: str, transform=None):\n",
    "        self.data_loader = VoxelDataLoader(zip_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader.npz_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        complete, partial = self.data_loader.get_voxel_grids(idx)\n",
    "        # Normalize to [0,1] if not already\n",
    "        complete = (complete > 0).float()\n",
    "        partial = (partial > 0).float()\n",
    "        if self.transform:\n",
    "            complete, partial = self.transform(complete, partial)\n",
    "        return complete, partial\n",
    "\n",
    "\n",
    "# Update data loader creation function\n",
    "def create_data_loader(zip_path: str, batch_size: int = 1, shuffle: bool = True, num_workers: int = 0):\n",
    "    \"\"\"Create a PyTorch DataLoader for training\"\"\"\n",
    "    dataset = VoxelDataset(zip_path)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        # pin_memory=True\n",
    "    )\n",
    "\n",
    "\n",
    "def split_dataset(dataset, train_ratio=0.8, val_ratio=0.2, seed=42):\n",
    "    n = len(dataset)\n",
    "    indices = list(range(n))\n",
    "    random.Random(seed).shuffle(indices)\n",
    "\n",
    "    # from dataset: 80% train 20% test \n",
    "    n_trainval = int(n * 0.8)\n",
    "    n_test = n - n_trainval\n",
    "    trainval_indices = indices[:n_trainval]\n",
    "    test_indices = indices[n_trainval:]\n",
    "    # from training data: 80% train 20% validation\n",
    "    n_train = int(len(trainval_indices) * 0.8)\n",
    "    train_indices = trainval_indices[:n_train]\n",
    "    val_indices = trainval_indices[n_train:]\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "def create_data_loaders(zip_path, batch_size=1, shuffle=True, num_workers=0, seed=42):\n",
    "    dataset = VoxelDataset(zip_path)\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    train_idx, val_idx, test_idx = split_dataset(dataset, seed=seed)\n",
    "    train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    val_loader = DataLoader(Subset(dataset, val_idx), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(Subset(dataset, test_idx), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "class SpatialAttention3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient 3D spatial attention with proper windowing.\n",
    "    Maintains 3D structure throughout (never flattens below 3D).\n",
    "    At each level, attention looks in all 6 directions (behind, in front, left, right, above, under) via 3D windowing.\n",
    "    Now supports dynamic window size per forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int = 4, window_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.head_dim = d_model // num_heads\n",
    "        assert d_model % num_heads == 0\n",
    "        self.qkv = nn.Conv3d(d_model, d_model * 3, kernel_size=1)\n",
    "        self.proj = nn.Conv3d(d_model, d_model, kernel_size=1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x, window_size=None):\n",
    "        B, C, D, H, W = x.shape\n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x)  # [B, 3*C, D, H, W]\n",
    "        q, k, v = qkv.chunk(3, dim=1)  # Each: [B, C, D, H, W]\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, self.num_heads, self.head_dim, D, H, W)\n",
    "        k = k.view(B, self.num_heads, self.head_dim, D, H, W)\n",
    "        v = v.view(B, self.num_heads, self.head_dim, D, H, W)\n",
    "        # Extract windows efficiently using unfold\n",
    "        ws = window_size if window_size is not None else self.window_size\n",
    "        pad = ws // 2\n",
    "        # Pad the tensors\n",
    "        q_pad = F.pad(q, [pad]*6, mode='constant', value=0)\n",
    "        k_pad = F.pad(k, [pad]*6, mode='constant', value=0)\n",
    "        v_pad = F.pad(v, [pad]*6, mode='constant', value=0)\n",
    "        # Extract windows - much more efficient than conv3d approach\n",
    "        def extract_windows(tensor):\n",
    "            # tensor: [B, heads, head_dim, D_pad, H_pad, W_pad]\n",
    "            windows = tensor.unfold(3, ws, 1).unfold(4, ws, 1).unfold(5, ws, 1)\n",
    "            # Result: [B, heads, head_dim, D, H, W, ws, ws, ws]\n",
    "            return windows.contiguous()\n",
    "        q_win = extract_windows(q_pad)  # [B, heads, head_dim, D, H, W, ws, ws, ws]\n",
    "        k_win = extract_windows(k_pad)\n",
    "        v_win = extract_windows(v_pad)\n",
    "        # Get center query for each position\n",
    "        center = ws // 2\n",
    "        q_center = q_win[:, :, :, :, :, :, center, center, center]  # [B, heads, head_dim, D, H, W]\n",
    "        # Flatten spatial dimensions of windows\n",
    "        k_flat = k_win.view(B, self.num_heads, self.head_dim, D, H, W, ws*ws*ws)\n",
    "        v_flat = v_win.view(B, self.num_heads, self.head_dim, D, H, W, ws*ws*ws)\n",
    "        # Compute attention scores\n",
    "        q_center = q_center.permute(0, 1, 3, 4, 5, 2).unsqueeze(-1)\n",
    "        k_flat = k_flat.permute(0, 1, 3, 4, 5, 2, 6)\n",
    "        v_flat = v_flat.permute(0, 1, 3, 4, 5, 2, 6)\n",
    "        # Attention computation\n",
    "        attn_scores = (q_center * k_flat).sum(dim=-2) * self.scale  # [B, heads, D, H, W, wsÂ³]\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        # Apply attention to values\n",
    "        attn_out = (attn_weights.unsqueeze(-2) * v_flat).sum(dim=-1)  # [B, heads, D, H, W, head_dim]\n",
    "        # Reshape back to original format\n",
    "        attn_out = attn_out.permute(0, 1, 5, 2, 3, 4).contiguous()  # [B, heads, head_dim, D, H, W]\n",
    "        attn_out = attn_out.view(B, C, D, H, W)\n",
    "        # Final projection\n",
    "        out = self.proj(attn_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class VoxelTransformerLayer3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer layer with proper normalization and residuals.\n",
    "    Now supports dynamic window size for attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int = 8, window_size: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Layer normalization (adapted for 3D)\n",
    "        self.norm1 = nn.GroupNorm(1, d_model)  # GroupNorm works better for 3D than LayerNorm\n",
    "        self.norm2 = nn.GroupNorm(1, d_model)\n",
    "        \n",
    "        # Attention\n",
    "        self.attention = SpatialAttention3D(d_model, num_heads, window_size)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Conv3d(d_model, d_model * 4, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout3d(dropout),\n",
    "            nn.Conv3d(d_model * 4, d_model, kernel_size=1),\n",
    "            nn.Dropout3d(dropout)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout3d(dropout)\n",
    "        \n",
    "    def forward(self, x, window_size=None):\n",
    "        # Attention block with residual connection\n",
    "        norm_x = self.norm1(x)\n",
    "        attn_out = self.attention(norm_x, window_size=window_size)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        \n",
    "        # FFN block with residual connection\n",
    "        norm_x = self.norm2(x)\n",
    "        ffn_out = self.ffn(norm_x)\n",
    "        x = x + ffn_out\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned 3D positional encoding for voxel grids.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_grid_size: int = 16):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_grid_size = max_grid_size\n",
    "        # Learnable positional embedding for each voxel position\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, d_model, max_grid_size, max_grid_size, max_grid_size)\n",
    "        )\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, d_model, D, H, W]\n",
    "        _, _, D, H, W = x.shape\n",
    "        return self.pos_embed[:, :, :D, :H, :W]\n",
    "\n",
    "\n",
    "class VoxelCompletionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved 3D transformer for voxel completion.\n",
    "    Predicts in a single level at the given window size.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int = 64, num_heads: int = 8, num_layers: int = 4,\n",
    "                 max_grid_size: int = 16, window_size: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        # Input projection\n",
    "        self.input_proj = nn.Conv3d(1, d_model, kernel_size=1)\n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding3D(d_model, max_grid_size)\n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            VoxelTransformerLayer3D(d_model, num_heads, window_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        # Output projection\n",
    "        self.output_norm = nn.GroupNorm(1, d_model)\n",
    "        self.output_proj = nn.Conv3d(d_model, 1, kernel_size=1)\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def forward(self, x, window_size=None):\n",
    "        # x: [B, 1, D, H, W]\n",
    "        x = self.input_proj(x)  # [B, d_model, D, H, W]\n",
    "        x = x + self.pos_encoding(x)\n",
    "        ws = window_size if window_size is not None else self.window_size\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, window_size=ws)\n",
    "        x = self.output_norm(x)\n",
    "        x = self.output_proj(x)  # [B, 1, D, H, W]\n",
    "        return x\n",
    "\n",
    "\n",
    "def masked_bce_loss(preds, targets, partial_grid, criterion):\n",
    "    # Mask for unknown voxels (where partial is 0)\n",
    "    unknown_mask = (partial_grid == 0)\n",
    "    # Only compute BCE loss on unknown voxels\n",
    "    masked_loss = criterion(preds * unknown_mask, targets * unknown_mask)\n",
    "    # Avoid division by zero\n",
    "    denom = unknown_mask.float().sum() + 1e-6\n",
    "    return (masked_loss * unknown_mask.float()).sum() / denom\n",
    "\n",
    "def consistency_loss(preds, partial_grid):\n",
    "    # Penalize changes to known voxels (where partial is 1)\n",
    "    known_mask = (partial_grid == 1)\n",
    "    return F.mse_loss(preds * known_mask, partial_grid * known_mask)\n",
    "\n",
    "# levels = 16\n",
    "def compute_pos_weight(dataset, sample_size=100):\n",
    "    \"\"\"Estimate pos_weight for BCEWithLogitsLoss based on dataset occupancy.\"\"\"\n",
    "    total_occupied = 0\n",
    "    total_empty = 0\n",
    "    n = min(sample_size, len(dataset))\n",
    "    for i in range(n):\n",
    "        complete, _ = dataset[i]\n",
    "        total_occupied += (complete > 0.5).sum().item()\n",
    "        total_empty += (complete <= 0.5).sum().item()\n",
    "    if total_occupied == 0:\n",
    "        return torch.tensor([1.0])\n",
    "    return torch.tensor([total_empty / total_occupied])\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_set,\n",
    "    val_set,\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 1,\n",
    "    window_size: int = 3,\n",
    "    lambda_consistency: float = 1.0,\n",
    "    seed: int = 42\n",
    "):\n",
    "    \"\"\"Training loop for the voxel completion model with improved loss functions.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    print(f\"Train loader size: {len(train_loader)}, Val loader size: {len(val_loader)}\")\n",
    "    # Compute pos_weight for BCEWithLogitsLoss\n",
    "    pos_weight = compute_pos_weight(train_set)\n",
    "    print(f\"Using pos_weight for BCEWithLogitsLoss: {pos_weight.item():.2f}\")\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight.to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "    total_start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        num_samples_processed = 0\n",
    "        model.train()\n",
    "        epoch_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=True, unit='sample')\n",
    "        for batch_idx, (complete_grid, partial_grid) in enumerate(epoch_pbar):\n",
    "            complete_grid = complete_grid.to(device, non_blocking=True)\n",
    "            partial_grid = partial_grid.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            if partial_grid.dim() == 4:\n",
    "                partial_grid = partial_grid.unsqueeze(1)\n",
    "            if complete_grid.dim() == 4:\n",
    "                complete_grid = complete_grid.unsqueeze(1)\n",
    "            with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "                preds = model(partial_grid, window_size=window_size)\n",
    "                masked_loss = masked_bce_loss(preds, complete_grid, partial_grid, criterion)\n",
    "                cons_loss = consistency_loss(preds, partial_grid)\n",
    "                total_batch_loss = masked_loss + lambda_consistency * cons_loss\n",
    "            if scaler is not None:\n",
    "                scaler.scale(total_batch_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                total_batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            total_loss += total_batch_loss.item()\n",
    "            num_samples_processed += 1\n",
    "            del complete_grid, partial_grid, preds, masked_loss, cons_loss, total_batch_loss\n",
    "            torch.cuda.empty_cache()\n",
    "            epoch_pbar.set_postfix({\n",
    "                'train_loss': f'{total_loss/num_samples_processed:.4f}',\n",
    "                'samples': num_samples_processed,\n",
    "                'lr': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        avg_train_loss = total_loss / max(num_samples_processed, 1)\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for complete_grid, partial_grid in val_loader:\n",
    "                complete_grid = complete_grid.to(device, non_blocking=True)\n",
    "                partial_grid = partial_grid.to(device, non_blocking=True)\n",
    "                if partial_grid.dim() == 4:\n",
    "                    partial_grid = partial_grid.unsqueeze(1)\n",
    "                if complete_grid.dim() == 4:\n",
    "                    complete_grid = complete_grid.unsqueeze(1)\n",
    "                preds = model(partial_grid, window_size=window_size)\n",
    "                masked_loss = masked_bce_loss(preds, complete_grid, partial_grid, criterion)\n",
    "                cons_loss = consistency_loss(preds, partial_grid)\n",
    "                total_batch_loss = masked_loss + lambda_consistency * cons_loss\n",
    "                val_loss += total_batch_loss.item()\n",
    "                val_samples += 1\n",
    "                del complete_grid, partial_grid, preds, masked_loss, cons_loss, total_batch_loss\n",
    "                torch.cuda.empty_cache()\n",
    "        avg_val_loss = val_loss / max(val_samples, 1)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Time: {timedelta(seconds=int(epoch_time))}, Train loss: {avg_train_loss:.4f}, Val loss: {avg_val_loss:.4f}, Samples: {num_samples_processed}\")\n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"\\nTraining completed in {timedelta(seconds=int(total_time))}\")\n",
    "    print(f\"Average time per epoch: {timedelta(seconds=int(total_time/num_epochs))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e30d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-auth-oauthlib\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# DRIVE_PATH = \"/content/drive/MyDrive/AUB_masters/thesis/data/partial_data_16.zip\"  # Adjust this path to match your Drive structure\n",
    "# LOCAL_PATH = \"/content/partial_data\"\n",
    "# !mkdir -p {LOCAL_PATH}\n",
    "\n",
    "# print(\"Copying data from Drive to local storage...\")\n",
    "# !cp \"{DRIVE_PATH}\" \"{LOCAL_PATH}/data.zip\"\n",
    "# zip_path = f\"{LOCAL_PATH}/data.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b0eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporary directory: /tmp/tmppmkvwylj\n",
      "Extracted zip file to temporary directory\n",
      "Found 256571 total NPZ files\n",
      "Using 256571\n",
      "Total dataset size: 256571\n"
     ]
    }
   ],
   "source": [
    "zip_path = \"../../chunk_data_16_flood_fill_rm_20.zip\"\n",
    "dataset = VoxelDataset(zip_path)\n",
    "\n",
    "print(f\"Total dataset size: {len(dataset)}\")\n",
    "\n",
    "train_idx, val_idx, test_idx = split_dataset(dataset, seed=42)\n",
    "train_set = torch.utils.data.Subset(dataset, train_idx)\n",
    "val_set = torch.utils.data.Subset(dataset, val_idx)\n",
    "test_set = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = VoxelCompletionTransformer(\n",
    "    d_model=48,        \n",
    "    num_heads=6,       \n",
    "    num_layers=6,      \n",
    "    window_size=3,    \n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "torch.cuda.empty_cache()\n",
    "# best model before this was num_epochs=2\n",
    "train_model(model, train_set, val_set, num_epochs=5, batch_size=2, lambda_consistency=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"../../models/trained_model_rm_20_flood_fill.pth\"\n",
    "# os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "# }, MODEL_SAVE_PATH)\n",
    "# print(f\"Model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8b4fb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Updated test_model to use test_set ---\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "def test_model(model_path, test_set, sample_idx=0, threshold=0.5, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VoxelCompletionTransformer(\n",
    "        d_model=48,        \n",
    "        num_heads=6,       \n",
    "        num_layers=6,      \n",
    "        window_size=3,    \n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    complete, partial = test_set[sample_idx]\n",
    "\n",
    "    filled_complete = (complete > 0).sum()\n",
    "    # Of those, how many are also filled in partial?\n",
    "    filled_partial = ((complete > 0) & (partial > 0)).sum()\n",
    "    \n",
    "    missing_percent = (filled_complete - filled_partial) / filled_complete\n",
    "\n",
    "    # if missing_percent > 0.3:\n",
    "    print(\"Missing Percentage: \", missing_percent)\n",
    "\n",
    "    ptl = partial\n",
    "    partial = partial.unsqueeze(0).unsqueeze(0).to(device)  # [1, 1, D, H, W]\n",
    "    with torch.no_grad():\n",
    "        output = model(partial)\n",
    "\n",
    "        # with open(\"model_summary.txt\", \"a\") as f:\n",
    "        #     f.write(str(summary(model, input_size=(1, 1, 16, 16, 16))))\n",
    "\n",
    "        output = torch.sigmoid(output)\n",
    "        output[0, 0][ptl == 1] = 1.0\n",
    "        output = output.squeeze().cpu()\n",
    "    print(\"Inference complete.\")\n",
    "    print(\"Partial shape:\", partial.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            for k in range(output.shape[2]):\n",
    "                if output[i, j, k] > threshold:\n",
    "                    output[i, j, k] = 1.0\n",
    "                else:\n",
    "                    output[i, j, k] = 0.0\n",
    "    out_path = \"output_voxel.npy\"\n",
    "    complete_path = \"complete_voxel.npy\"\n",
    "    partial_path = \"partial_voxel.npy\"\n",
    "    np.save(out_path, output.numpy())\n",
    "    np.save(complete_path, complete)\n",
    "    np.save(partial_path, ptl)\n",
    "    print(\"Sample Index: \", sample_idx)\n",
    "    print(f\"Output saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5480df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    test_set=test_set,\n",
    "    sample_idx=\n",
    "    random.randint(0, len(test_set) - 1),\n",
    "    # 46000,\n",
    "    threshold=0.5,\n",
    "    device=device\n",
    ")\n",
    "# random.randint(0, len(test_set) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbcbf4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Updated test_model to use test_set ---\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "\n",
    "def test_model_acc(model_path, test_set, sample_idx=0, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VoxelCompletionTransformer(\n",
    "        d_model=48,        \n",
    "        num_heads=6,       \n",
    "        num_layers=6,      \n",
    "        window_size=5,    \n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    complete, partial = test_set[sample_idx]\n",
    "    ptl = partial\n",
    "    partial = partial.unsqueeze(0).unsqueeze(0).to(device)  # [1, 1, D, H, W]\n",
    "    with torch.no_grad():\n",
    "        output = model(partial)\n",
    "\n",
    "        # with open(\"model_summary.txt\", \"a\") as f:\n",
    "        #     f.write(str(summary(model, input_size=(1, 1, 16, 16, 16))))\n",
    "\n",
    "        output = torch.sigmoid(output)\n",
    "        output[0, 0][ptl == 1] = 1.0\n",
    "        output = output.squeeze().cpu()\n",
    "    print(\"Inference complete.\")\n",
    "    print(\"Partial shape:\", partial.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    # Binarize output\n",
    "    output_bin = (output > 0.5).float()\n",
    "\n",
    "    # Only evaluate on unknown voxels (partial == 0)\n",
    "    unknown_mask = (ptl == 0)\n",
    "    total_unknown = unknown_mask.sum().item()\n",
    "    if total_unknown == 0:\n",
    "        print(\"No unknown voxels to evaluate accuracy.\")\n",
    "        accuracy = None\n",
    "    else:\n",
    "        correct = ((output_bin == complete) & unknown_mask).sum().item()\n",
    "        accuracy = correct / total_unknown\n",
    "        print(f\"Accuracy on unknown voxels: {accuracy:.4f} ({correct}/{total_unknown})\")\n",
    "\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            for k in range(output.shape[2]):\n",
    "                if output[i, j, k] > 0.5:\n",
    "                    output[i, j, k] = 1.0\n",
    "                else:\n",
    "                    output[i, j, k] = 0.0\n",
    "    out_path = \"output_voxel.npy\"\n",
    "    complete_path = \"complete_voxel.npy\"\n",
    "    partial_path = \"partial_voxel.npy\"\n",
    "    np.save(out_path, output.numpy())\n",
    "    np.save(complete_path, complete)\n",
    "    np.save(partial_path, ptl)\n",
    "    print(\"Sample Index: \", sample_idx)\n",
    "    print(f\"Output saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58cd6b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete.\n",
      "Partial shape: torch.Size([1, 1, 16, 16, 16])\n",
      "Output shape: torch.Size([16, 16, 16])\n",
      "Accuracy on unknown voxels: 0.9875 (3879/3928)\n",
      "Sample Index:  18971\n",
      "Output saved to output_voxel.npy\n"
     ]
    }
   ],
   "source": [
    "test_model_acc(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    test_set=test_set,\n",
    "    sample_idx=random.randint(0, len(test_set) - 1),\n",
    "    device=device\n",
    ")\n",
    "# random.randint(0, len(test_set) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b797fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Iterative refinement inference ---\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "\n",
    "def test_model_iterative_ref(model_path, test_set, sample_idx=0, device=None, n_steps=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Iterative refinement: repeatedly feed the model's output as input, allowing it to refine its prediction.\n",
    "    At each step, the model tries to improve upon its previous output, correcting errors and adding finer structure.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VoxelCompletionTransformer(\n",
    "        d_model=48,        \n",
    "        num_heads=6,       \n",
    "        num_layers=6,      \n",
    "        window_size=5,    \n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    complete, partial = test_set[sample_idx]\n",
    "    ptl = partial.clone()\n",
    "    # Initial input is the partial observation\n",
    "    current = partial.clone()\n",
    "    current_input = current.unsqueeze(0).unsqueeze(0).to(device)  # [1, 1, D, H, W]\n",
    "    with torch.no_grad():\n",
    "        for step in range(n_steps):\n",
    "            output = model(current_input)\n",
    "            output = torch.sigmoid(output)\n",
    "            # Always keep known voxels fixed (partial==1)\n",
    "            output[0, 0][ptl == 1] = 1.0\n",
    "            # Prepare for next step: use output as new input\n",
    "            current = output.squeeze().cpu()\n",
    "            if verbose:\n",
    "                print(f\"Refinement step {step+1}/{n_steps} complete.\")\n",
    "            # Next input: binarize or keep as probability?\n",
    "            # Option 1: Use probabilities (soft refinement)\n",
    "            current_input = current.unsqueeze(0).unsqueeze(0).to(device)\n",
    "        # Final output: binarize for saving and evaluation\n",
    "        output_bin = (current > 0.5).float()\n",
    "        # Accuracy on unknown voxels\n",
    "        unknown_mask = (ptl == 0)\n",
    "        total_unknown = unknown_mask.sum().item()\n",
    "        if total_unknown == 0:\n",
    "            print(\"No unknown voxels to evaluate accuracy.\")\n",
    "            accuracy = None\n",
    "        else:\n",
    "            correct = ((output_bin == complete) & unknown_mask).sum().item()\n",
    "            accuracy = correct / total_unknown\n",
    "            print(f\"Accuracy on unknown voxels after refinement: {accuracy:.4f} ({correct}/{total_unknown})\")\n",
    "    # Save results\n",
    "    out_path = \"output_voxel.npy\"\n",
    "    complete_path = \"complete_voxel.npy\"\n",
    "    partial_path = \"partial_voxel.npy\"\n",
    "    np.save(out_path, output_bin.numpy())\n",
    "    np.save(complete_path, complete)\n",
    "    np.save(partial_path, ptl)\n",
    "    print(f\"Refined output saved to {out_path}\")\n",
    "    print(\"Current Sample Index: \", sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "09510464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refinement step 1/2 complete.\n",
      "Refinement step 2/2 complete.\n",
      "Accuracy on unknown voxels after refinement: 0.9333 (3626/3885)\n",
      "Refined output saved to output_voxel.npy\n",
      "Current Sample Index:  5729\n"
     ]
    }
   ],
   "source": [
    "test_model_iterative_ref(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    test_set=test_set,\n",
    "    sample_idx=random.randint(0, len(test_set) - 1),\n",
    "    device=device,\n",
    "    n_steps=2\n",
    ")\n",
    "# random.randint(0, len(test_set) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a34c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
