{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bea1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extension to agent copy 4.ipynb F1-score, pos weight\n",
    "from typing import Tuple, List\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "import zipfile\n",
    "import shutil\n",
    "import tempfile\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Subset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89f6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelDataLoader:\n",
    "    \"\"\"Loads and processes NPZ voxel data from a zip file\"\"\"\n",
    "\n",
    "    def __init__(self, zip_path: str):\n",
    "        # Create a temporary directory\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        print(f\"Created temporary directory: {self.temp_dir}\")\n",
    "\n",
    "        # Extract zip file\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.temp_dir)\n",
    "        print(f\"Extracted zip file to temporary directory\")\n",
    "\n",
    "        # Find all NPZ files\n",
    "        all_files = glob.glob(os.path.join(self.temp_dir, \"**/*.npz\"), recursive=True)\n",
    "        print(f\"Found {len(all_files)} total NPZ files\")\n",
    "\n",
    "        if len(all_files) == 0:\n",
    "            raise ValueError(f\"No NPZ files found in zip file\")\n",
    "\n",
    "        random.shuffle(all_files)  # Shuffle before splitting\n",
    "        cutoff = int(len(all_files))\n",
    "        self.npz_files = all_files[:cutoff]\n",
    "        print(f\"Using {len(self.npz_files)}\")\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup temporary directory when object is destroyed\"\"\"\n",
    "        try:\n",
    "            shutil.rmtree(self.temp_dir)\n",
    "            print(f\"Cleaned up temporary directory: {self.temp_dir}\")\n",
    "        except:\n",
    "            print(f\"Failed to clean up temporary directory: {self.temp_dir}\")\n",
    "\n",
    "    def load_single_file(self, file_path: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        data = np.load(file_path)\n",
    "\n",
    "        # More robust key checking\n",
    "        if 'complete' not in data or 'partial' not in data:\n",
    "            raise ValueError(f\"NPZ file {file_path} must contain both 'complete' and 'partial' arrays\")\n",
    "\n",
    "        complete = torch.from_numpy(data['complete']).float()\n",
    "        partial = torch.from_numpy(data['partial']).float()\n",
    "\n",
    "        # Verify shapes match\n",
    "        if complete.shape != partial.shape:\n",
    "            raise ValueError(f\"Shape mismatch in {file_path}: complete {complete.shape} vs partial {partial.shape}\")\n",
    "\n",
    "        return complete, partial\n",
    "\n",
    "    def get_all_data(self) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"Load all voxel pairs from all NPZ files\"\"\"\n",
    "        all_data = []\n",
    "        for file_path in self.npz_files:\n",
    "            complete, partial = self.load_single_file(file_path)\n",
    "            all_data.append((complete, partial))\n",
    "        return all_data\n",
    "\n",
    "    def get_voxel_grids(self, index: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns complete and partial voxel grids from a specific file\"\"\"\n",
    "        if index >= len(self.npz_files):\n",
    "            raise IndexError(f\"Index {index} out of range. Only {len(self.npz_files)} files available.\")\n",
    "        return self.load_single_file(self.npz_files[index])\n",
    "\n",
    "\n",
    "class VoxelDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"PyTorch Dataset for voxel completion\"\"\"\n",
    "\n",
    "    def __init__(self, zip_path: str, transform=None):\n",
    "        self.data_loader = VoxelDataLoader(zip_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader.npz_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        complete, partial = self.data_loader.get_voxel_grids(idx)\n",
    "        # Normalize to [0,1] if not already\n",
    "        complete = (complete > 0).float()\n",
    "        partial = (partial > 0).float()\n",
    "        if self.transform:\n",
    "            complete, partial = self.transform(complete, partial)\n",
    "        return complete, partial\n",
    "\n",
    "\n",
    "# Update data loader creation function\n",
    "def create_data_loader(zip_path: str, batch_size: int = 1, shuffle: bool = True, num_workers: int = 0):\n",
    "    \"\"\"Create a PyTorch DataLoader for training\"\"\"\n",
    "    dataset = VoxelDataset(zip_path)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        # pin_memory=True\n",
    "    )\n",
    "\n",
    "\n",
    "def split_dataset(dataset, train_ratio=0.8, val_ratio=0.2, seed=42):\n",
    "    n = len(dataset)\n",
    "    indices = list(range(n))\n",
    "    random.Random(seed).shuffle(indices)\n",
    "\n",
    "    # from dataset: 80% train 20% test \n",
    "    n_trainval = int(n * 0.8)\n",
    "    n_test = n - n_trainval\n",
    "    trainval_indices = indices[:n_trainval]\n",
    "    test_indices = indices[n_trainval:]\n",
    "    # from training data: 80% train 20% validation\n",
    "    n_train = int(len(trainval_indices) * 0.8)\n",
    "    train_indices = trainval_indices[:n_train]\n",
    "    val_indices = trainval_indices[n_train:]\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "def create_data_loaders(zip_path, batch_size=1, shuffle=True, num_workers=0, seed=42):\n",
    "    dataset = VoxelDataset(zip_path)\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    train_idx, val_idx, test_idx = split_dataset(dataset, seed=seed)\n",
    "    train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    val_loader = DataLoader(Subset(dataset, val_idx), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(Subset(dataset, test_idx), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "class SpatialAttention3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient 3D spatial attention with proper windowing.\n",
    "    Maintains 3D structure throughout (never flattens below 3D).\n",
    "    At each level, attention looks in all 6 directions (behind, in front, left, right, above, under) via 3D windowing.\n",
    "    Now supports dynamic window size per forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int = 6, window_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.head_dim = d_model // num_heads\n",
    "        assert d_model % num_heads == 0\n",
    "        self.qkv = nn.Conv3d(d_model, d_model * 3, kernel_size=1)\n",
    "        self.proj = nn.Conv3d(d_model, d_model, kernel_size=1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x, window_size=None):\n",
    "        B, C, D, H, W = x.shape\n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x)  # [B, 3*C, D, H, W]\n",
    "        q, k, v = qkv.chunk(3, dim=1)  # Each: [B, C, D, H, W]\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, self.num_heads, self.head_dim, D, H, W)\n",
    "        k = k.view(B, self.num_heads, self.head_dim, D, H, W)\n",
    "        v = v.view(B, self.num_heads, self.head_dim, D, H, W)\n",
    "        # Extract windows efficiently using unfold\n",
    "        ws = window_size if window_size is not None else self.window_size\n",
    "        pad = ws // 2\n",
    "        # Pad the tensors\n",
    "        q_pad = F.pad(q, [pad]*6, mode='constant', value=0)\n",
    "        k_pad = F.pad(k, [pad]*6, mode='constant', value=0)\n",
    "        v_pad = F.pad(v, [pad]*6, mode='constant', value=0)\n",
    "        # Extract windows - much more efficient than conv3d approach\n",
    "        def extract_windows(tensor):\n",
    "            # tensor: [B, heads, head_dim, D_pad, H_pad, W_pad]\n",
    "            windows = tensor.unfold(3, ws, 1).unfold(4, ws, 1).unfold(5, ws, 1)\n",
    "            # Result: [B, heads, head_dim, D, H, W, ws, ws, ws]\n",
    "            return windows.contiguous()\n",
    "        q_win = extract_windows(q_pad)  # [B, heads, head_dim, D, H, W, ws, ws, ws]\n",
    "        k_win = extract_windows(k_pad)\n",
    "        v_win = extract_windows(v_pad)\n",
    "        # Get center query for each position\n",
    "        center = ws // 2\n",
    "        q_center = q_win[:, :, :, :, :, :, center, center, center]  # [B, heads, head_dim, D, H, W]\n",
    "        # Flatten spatial dimensions of windows (dynamic shape)\n",
    "        k_flat = k_win.view(B, self.num_heads, self.head_dim, D, H, W, ws*ws*ws)\n",
    "        v_flat = v_win.view(B, self.num_heads, self.head_dim, D, H, W, ws*ws*ws)\n",
    "        # Compute attention scores\n",
    "        q_center = q_center.permute(0, 1, 3, 4, 5, 2).unsqueeze(-1)\n",
    "        k_flat = k_flat.permute(0, 1, 3, 4, 5, 2, 6)\n",
    "        v_flat = v_flat.permute(0, 1, 3, 4, 5, 2, 6)\n",
    "        # Attention computation\n",
    "        attn_scores = (q_center * k_flat).sum(dim=-2) * self.scale  # [B, heads, D, H, W, ws³]\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        # Apply attention to values\n",
    "        attn_out = (attn_weights.unsqueeze(-2) * v_flat).sum(dim=-1)  # [B, heads, D, H, W, head_dim]\n",
    "        # Reshape back to original format\n",
    "        attn_out = attn_out.permute(0, 1, 5, 2, 3, 4).contiguous()  # [B, heads, head_dim, D, H, W]\n",
    "        attn_out = attn_out.view(B, C, D, H, W)\n",
    "        # Final projection\n",
    "        out = self.proj(attn_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class VoxelTransformerLayer3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer layer with proper normalization and residuals.\n",
    "    Now supports dynamic window size for attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int = 8, window_size: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Layer normalization (adapted for 3D)\n",
    "        self.norm1 = nn.GroupNorm(1, d_model)  # GroupNorm works better for 3D than LayerNorm\n",
    "        self.norm2 = nn.GroupNorm(1, d_model)\n",
    "        \n",
    "        # Attention\n",
    "        self.attention = SpatialAttention3D(d_model, num_heads, window_size)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Conv3d(d_model, d_model * 4, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout3d(dropout),\n",
    "            nn.Conv3d(d_model * 4, d_model, kernel_size=1),\n",
    "            nn.Dropout3d(dropout)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout3d(dropout)\n",
    "        \n",
    "    def forward(self, x, window_size=None):\n",
    "        # Attention block with residual connection\n",
    "        norm_x = self.norm1(x)\n",
    "        attn_out = self.attention(norm_x, window_size=window_size)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        \n",
    "        # FFN block with residual connection\n",
    "        norm_x = self.norm2(x)\n",
    "        ffn_out = self.ffn(norm_x)\n",
    "        x = x + ffn_out\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned 3D positional encoding for voxel grids.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_grid_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_grid_size = max_grid_size\n",
    "        # Learnable positional embedding for each voxel position\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, d_model, max_grid_size, max_grid_size, max_grid_size)\n",
    "        )\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, d_model, D, H, W]\n",
    "        _, _, D, H, W = x.shape\n",
    "        return self.pos_embed[:, :, :D, :H, :W]\n",
    "\n",
    "\n",
    "class VoxelCompletionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved 3D transformer for voxel completion.\n",
    "    Predicts in a single level at the given window size.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int = 64, num_heads: int = 6, num_layers: int = 6,\n",
    "                 max_grid_size: int = 32, window_size: int = 5, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_grid_size = max_grid_size\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        # Input projection\n",
    "        self.input_proj = nn.Conv3d(1, d_model, kernel_size=1)\n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding3D(d_model, max_grid_size)\n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            VoxelTransformerLayer3D(d_model, num_heads, window_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        # Output projection\n",
    "        self.output_norm = nn.GroupNorm(1, d_model)\n",
    "        self.output_proj = nn.Conv3d(d_model, 1, kernel_size=1)\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def forward(self, x, window_size=None):\n",
    "        # x: [B, 1, D, H, W]\n",
    "        x = self.input_proj(x)  # [B, d_model, D, H, W]\n",
    "        x = x + self.pos_encoding(x)\n",
    "        ws = window_size if window_size is not None else self.window_size\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, window_size=ws)\n",
    "        x = self.output_norm(x)\n",
    "        x = self.output_proj(x)  # [B, 1, D, H, W]\n",
    "        return x\n",
    "\n",
    "\n",
    "def masked_bce_loss(preds, targets, partial_grid, criterion):\n",
    "    # Mask for unknown voxels (where partial is 0)\n",
    "    unknown_mask = (partial_grid == 0)\n",
    "    # Only compute BCE loss on unknown voxels\n",
    "    masked_loss = criterion(preds * unknown_mask, targets * unknown_mask)\n",
    "    # Avoid division by zero\n",
    "    denom = unknown_mask.float().sum() + 1e-6\n",
    "    return (masked_loss * unknown_mask.float()).sum() / denom\n",
    "\n",
    "def consistency_loss(preds, partial_grid):\n",
    "    # Penalize changes to known voxels (where partial is 1)\n",
    "    known_mask = (partial_grid == 1)\n",
    "    return F.mse_loss(preds * known_mask, partial_grid * known_mask)\n",
    "\n",
    "# levels = 16\n",
    "def compute_pos_weight(dataset, sample_size=100):\n",
    "    \"\"\"Estimate pos_weight for BCEWithLogitsLoss based on dataset occupancy.\"\"\"\n",
    "    total_occupied = 0\n",
    "    total_empty = 0\n",
    "    n = min(sample_size, len(dataset))\n",
    "    for i in range(n):\n",
    "        complete, _ = dataset[i]\n",
    "        total_occupied += (complete > 0.5).sum().item()\n",
    "        total_empty += (complete <= 0.5).sum().item()\n",
    "    if total_occupied == 0:\n",
    "        return torch.tensor([1.0])\n",
    "    return torch.tensor([total_empty / total_occupied])\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_set,\n",
    "    val_set,\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 1,\n",
    "    window_size: int = 3,\n",
    "    lambda_consistency: float = 1.0,\n",
    "    seed: int = 42\n",
    "):\n",
    "    \"\"\"Training loop for the voxel completion model with improved loss functions.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    print(f\"Train loader size: {len(train_loader)}, Val loader size: {len(val_loader)}\")\n",
    "    # Compute pos_weight for BCEWithLogitsLoss\n",
    "    pos_weight = compute_pos_weight(train_set)\n",
    "    print(f\"Using pos_weight for BCEWithLogitsLoss: {pos_weight.item():.2f}\")\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight.to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "    total_start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        num_samples_processed = 0\n",
    "        model.train()\n",
    "        epoch_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=True, unit='sample')\n",
    "        for batch_idx, (complete_grid, partial_grid) in enumerate(epoch_pbar):\n",
    "            complete_grid = complete_grid.to(device, non_blocking=True)\n",
    "            partial_grid = partial_grid.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            if partial_grid.dim() == 4:\n",
    "                partial_grid = partial_grid.unsqueeze(1)\n",
    "            if complete_grid.dim() == 4:\n",
    "                complete_grid = complete_grid.unsqueeze(1)\n",
    "            with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "                preds = model(partial_grid, window_size=window_size)\n",
    "                masked_loss = masked_bce_loss(preds, complete_grid, partial_grid, criterion)\n",
    "                cons_loss = consistency_loss(preds, partial_grid)\n",
    "                total_batch_loss = masked_loss + lambda_consistency * cons_loss\n",
    "            if scaler is not None:\n",
    "                scaler.scale(total_batch_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                total_batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            total_loss += total_batch_loss.item()\n",
    "            num_samples_processed += 1\n",
    "            del complete_grid, partial_grid, preds, masked_loss, cons_loss, total_batch_loss\n",
    "            torch.cuda.empty_cache()\n",
    "            epoch_pbar.set_postfix({\n",
    "                'train_loss': f'{total_loss/num_samples_processed:.4f}',\n",
    "                'samples': num_samples_processed,\n",
    "                'lr': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        avg_train_loss = total_loss / max(num_samples_processed, 1)\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for complete_grid, partial_grid in val_loader:\n",
    "                complete_grid = complete_grid.to(device, non_blocking=True)\n",
    "                partial_grid = partial_grid.to(device, non_blocking=True)\n",
    "                if partial_grid.dim() == 4:\n",
    "                    partial_grid = partial_grid.unsqueeze(1)\n",
    "                if complete_grid.dim() == 4:\n",
    "                    complete_grid = complete_grid.unsqueeze(1)\n",
    "                preds = model(partial_grid, window_size=window_size)\n",
    "                masked_loss = masked_bce_loss(preds, complete_grid, partial_grid, criterion)\n",
    "                cons_loss = consistency_loss(preds, partial_grid)\n",
    "                total_batch_loss = masked_loss + lambda_consistency * cons_loss\n",
    "                val_loss += total_batch_loss.item()\n",
    "                val_samples += 1\n",
    "                del complete_grid, partial_grid, preds, masked_loss, cons_loss, total_batch_loss\n",
    "                torch.cuda.empty_cache()\n",
    "        avg_val_loss = val_loss / max(val_samples, 1)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Time: {timedelta(seconds=int(epoch_time))}, Train loss: {avg_train_loss:.4f}, Val loss: {avg_val_loss:.4f}, Samples: {num_samples_processed}\")\n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"\\nTraining completed in {timedelta(seconds=int(total_time))}\")\n",
    "    print(f\"Average time per epoch: {timedelta(seconds=int(total_time/num_epochs))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e30d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-auth-oauthlib\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# DRIVE_PATH = \"/content/drive/MyDrive/AUB_masters/thesis/data/partial_data_16.zip\"  # Adjust this path to match your Drive structure\n",
    "# LOCAL_PATH = \"/content/partial_data\"\n",
    "# !mkdir -p {LOCAL_PATH}\n",
    "\n",
    "# print(\"Copying data from Drive to local storage...\")\n",
    "# !cp \"{DRIVE_PATH}\" \"{LOCAL_PATH}/data.zip\"\n",
    "# zip_path = f\"{LOCAL_PATH}/data.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "976b0eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporary directory: /tmp/tmps9my9zcg\n",
      "Extracted zip file to temporary directory\n",
      "Found 264088 total NPZ files\n",
      "Using 264088\n",
      "Total dataset size: 264088\n",
      "Using device: cuda\n",
      "Starting training...\n",
      "Using device: cuda\n",
      "Train loader size: 84508, Val loader size: 21127\n",
      "Using pos_weight for BCEWithLogitsLoss: 16.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8852/3944767203.py:350: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
      "Epoch 1/2:   0%|          | 0/84508 [00:00<?, ?sample/s]/tmp/ipykernel_8852/3944767203.py:366: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
      "Epoch 1/2: 100%|██████████| 84508/84508 [8:18:42<00:00,  2.82sample/s, train_loss=0.1403, samples=84508, lr=0.0001]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Time: 9:01:11, Train loss: 0.1403, Val loss: 0.0695, Samples: 84508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 84508/84508 [8:19:31<00:00,  2.82sample/s, train_loss=0.0696, samples=84508, lr=0.0001]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Time: 9:02:10, Train loss: 0.0696, Val loss: 0.0640, Samples: 84508\n",
      "\n",
      "Training completed in 18:03:22\n",
      "Average time per epoch: 9:01:41\n"
     ]
    }
   ],
   "source": [
    "zip_path = \"../../partial_data_32_plane_5.zip\"\n",
    "dataset = VoxelDataset(zip_path)\n",
    "\n",
    "print(f\"Total dataset size: {len(dataset)}\")\n",
    "\n",
    "train_idx, val_idx, test_idx = split_dataset(dataset, seed=42)\n",
    "train_set = torch.utils.data.Subset(dataset, train_idx)\n",
    "val_set = torch.utils.data.Subset(dataset, val_idx)\n",
    "test_set = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# model = VoxelCompletionTransformer(\n",
    "#     d_model=96,        \n",
    "#     num_heads=6,       \n",
    "#     num_layers=2,      \n",
    "#     window_size=3,    \n",
    "#     dropout=0.1\n",
    "# ).to(device)\n",
    "\n",
    "model = VoxelCompletionTransformer(\n",
    "    d_model=48,        \n",
    "    num_heads=6,       \n",
    "    num_layers=6,      \n",
    "    window_size=3,    \n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "torch.cuda.empty_cache()\n",
    "# best model before this was num_epochs=2\n",
    "train_model(model, train_set, val_set, window_size=3, num_epochs=2, batch_size=2, lambda_consistency=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"../../models/model_32/trained_model.pth\"\n",
    "os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}, MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4fb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Updated test_model to use test_set ---\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "def test_model(model_path, test_set, sample_idx=0, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VoxelCompletionTransformer(\n",
    "        d_model=48,        \n",
    "        num_heads=6,       \n",
    "        num_layers=6,      \n",
    "        window_size=3,    \n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    complete, partial = test_set[sample_idx]\n",
    "    ptl = partial\n",
    "    partial = partial.unsqueeze(0).unsqueeze(0).to(device)  # [1, 1, D, H, W]\n",
    "    with torch.no_grad():\n",
    "        output = model(partial)\n",
    "\n",
    "        with open(\"model_summary.txt\", \"a\") as f:\n",
    "            f.write(str(summary(model, input_size=(1, 1, 32, 32, 32))))\n",
    "\n",
    "        output = torch.sigmoid(output)\n",
    "        output[0, 0][ptl == 1] = 1.0\n",
    "        output = output.squeeze().cpu()\n",
    "    print(\"Inference complete.\")\n",
    "    print(\"Partial shape:\", partial.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            for k in range(output.shape[2]):\n",
    "                if output[i, j, k] > 0.5:\n",
    "                    output[i, j, k] = 1.0\n",
    "                else:\n",
    "                    output[i, j, k] = 0.0\n",
    "    out_path = \"output_voxel.npy\"\n",
    "    complete_path = \"complete_voxel.npy\"\n",
    "    partial_path = \"partial_voxel.npy\"\n",
    "    np.save(out_path, output.numpy())\n",
    "    np.save(complete_path, complete)\n",
    "    np.save(partial_path, ptl)\n",
    "    print(f\"Output saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c0c5f",
   "metadata": {},
   "source": [
    "# Save Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23a77826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 52818 test samples to ../../saved_test_set_32_npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "save_dir = \"../../saved_test_set_32_npz\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for i, (complete, partial) in enumerate(test_set):\n",
    "    np.savez_compressed(\n",
    "        os.path.join(save_dir, f\"sample_{i}.npz\"),\n",
    "        complete=complete.numpy(),\n",
    "        partial=partial.numpy()\n",
    "    )\n",
    "print(f\"Saved {len(test_set)} test samples to {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9cbb56",
   "metadata": {},
   "source": [
    "# Load Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3240a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 52818 test samples from ../../saved_test_set_32_npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "load_dir = \"../../saved_test_set_32_npz\"\n",
    "test_samples = []\n",
    "\n",
    "for file in sorted(glob.glob(os.path.join(load_dir, \"sample_*.npz\"))):\n",
    "    data = np.load(file)\n",
    "    complete = torch.from_numpy(data['complete']).float()\n",
    "    partial = torch.from_numpy(data['partial']).float()\n",
    "    test_samples.append((complete, partial))\n",
    "\n",
    "print(f\"Loaded {len(test_samples)} test samples from {load_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b5d34",
   "metadata": {},
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce738ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized test accuracy computation for voxel completion using on-the-fly loading from disk, with CPU accuracy calculation to save GPU memory\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TestNPZDataset(Dataset):\n",
    "    def __init__(self, npz_dir, fraction=1.0, seed=42):\n",
    "        files = sorted(glob.glob(os.path.join(npz_dir, \"sample_*.npz\")))\n",
    "        if fraction < 1.0:\n",
    "            import random\n",
    "            random.Random(seed).shuffle(files)\n",
    "            n = max(1, int(len(files) * fraction))\n",
    "            files = files[:n]\n",
    "        self.files = files\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.files[idx])\n",
    "        complete = torch.from_numpy(data['complete']).float()\n",
    "        partial = torch.from_numpy(data['partial']).float()\n",
    "        return complete, partial\n",
    "\n",
    "def compute_test_accuracy(model_path, npz_dir, device=None, threshold=0.5, batch_size=1, num_workers=0, fraction=1.0, seed=42):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the model on the test set using batches, loading each sample from disk on-the-fly.\n",
    "    Only unknown voxels (partial == 0) are considered for accuracy.\n",
    "    Args:\n",
    "        model_path: Path to the saved model.\n",
    "        npz_dir: Directory containing test .npz files.\n",
    "        device: torch.device\n",
    "        threshold: Threshold for binarizing predictions.\n",
    "        batch_size: Number of samples per batch.\n",
    "        num_workers: DataLoader workers.\n",
    "        fraction: Fraction of data to use (0 < fraction <= 1.0).\n",
    "        seed: Random seed for shuffling.\n",
    "    Returns:\n",
    "        accuracy: float\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VoxelCompletionTransformer(\n",
    "        d_model=48,\n",
    "        num_heads=6,\n",
    "        num_layers=6,\n",
    "        window_size=3,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_dataset = TestNPZDataset(npz_dir, fraction=fraction, seed=seed)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, desc=\"Evaluating\", leave=True) as pbar:\n",
    "            for batch in pbar:\n",
    "                complete, partial = batch  # both on CPU\n",
    "                inp = partial.unsqueeze(1).to(device)  # move only input to device\n",
    "                output = model(inp)\n",
    "                output = torch.sigmoid(output).squeeze(1).cpu()  # move output back to CPU\n",
    "                # Only evaluate on unknown voxels (partial == 0)\n",
    "                unknown_mask = (partial == 0)\n",
    "                if unknown_mask.sum() == 0:\n",
    "                    continue\n",
    "                pred_bin = (output > threshold).float()\n",
    "                # All accuracy computation on CPU\n",
    "                correct += ((pred_bin == complete) * unknown_mask).sum().item()\n",
    "                total += unknown_mask.sum().item()\n",
    "                running_accuracy = correct / total if total > 0 else 0.0\n",
    "                pbar.set_postfix({\"accuracy\": f\"{running_accuracy * 100:.2f}%\"})\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    print(f\"Test accuracy (unknown voxels): {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c3873de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5281/5281 [05:52<00:00, 14.99it/s, accuracy=98.65%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (unknown voxels): 98.65%\n",
      "Test Accuracy:  0.9865352607778242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "load_dir = \"../../saved_test_set_32_npz\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_acc = compute_test_accuracy(model_path=MODEL_SAVE_PATH, npz_dir=load_dir, device=device, threshold=0.5, batch_size=1, fraction=0.1)\n",
    "print(\"Test Accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe5480df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete.\n",
      "Partial shape: torch.Size([1, 1, 32, 32, 32])\n",
      "Output shape: torch.Size([32, 32, 32])\n",
      "Output saved to output_voxel.npy\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_model(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    test_set=test_samples,\n",
    "    sample_idx=random.randint(0, len(test_samples) - 1),\n",
    "    device=device\n",
    ")\n",
    "# 5, 6, 7, 9, 11, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 39, 41\n",
    "# random.randint(0, len(test_set) - 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
