{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bea1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extension to agent copy 4.ipynb F1-score, pos weight\n",
    "from typing import Tuple, List\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "import zipfile\n",
    "import shutil\n",
    "import tempfile\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Subset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a89f6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelDataLoader:\n",
    "    \"\"\"Loads and processes NPZ voxel data from a zip file\"\"\"\n",
    "\n",
    "    def __init__(self, zip_path: str):\n",
    "        # Create a temporary directory\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        print(f\"Created temporary directory: {self.temp_dir}\")\n",
    "\n",
    "        # Extract zip file\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.temp_dir)\n",
    "        print(f\"Extracted zip file to temporary directory\")\n",
    "\n",
    "        # Find all NPZ files\n",
    "        all_files = glob.glob(os.path.join(self.temp_dir, \"**/*.npz\"), recursive=True)\n",
    "        print(f\"Found {len(all_files)} total NPZ files\")\n",
    "\n",
    "        if len(all_files) == 0:\n",
    "            raise ValueError(f\"No NPZ files found in zip file\")\n",
    "\n",
    "        random.shuffle(all_files)  # Shuffle before splitting\n",
    "        cutoff = int(len(all_files))\n",
    "        self.npz_files = all_files[:cutoff]\n",
    "        print(f\"Using {len(self.npz_files)}\")\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup temporary directory when object is destroyed\"\"\"\n",
    "        try:\n",
    "            shutil.rmtree(self.temp_dir)\n",
    "            print(f\"Cleaned up temporary directory: {self.temp_dir}\")\n",
    "        except:\n",
    "            print(f\"Failed to clean up temporary directory: {self.temp_dir}\")\n",
    "\n",
    "    def load_single_file(self, file_path: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        data = np.load(file_path)\n",
    "\n",
    "        # More robust key checking\n",
    "        if 'complete' not in data or 'partial' not in data:\n",
    "            raise ValueError(f\"NPZ file {file_path} must contain both 'complete' and 'partial' arrays\")\n",
    "\n",
    "        complete = torch.from_numpy(data['complete']).float()\n",
    "        partial = torch.from_numpy(data['partial']).float()\n",
    "\n",
    "        # Verify shapes match\n",
    "        if complete.shape != partial.shape:\n",
    "            raise ValueError(f\"Shape mismatch in {file_path}: complete {complete.shape} vs partial {partial.shape}\")\n",
    "\n",
    "        return complete, partial\n",
    "\n",
    "    def get_all_data(self) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"Load all voxel pairs from all NPZ files\"\"\"\n",
    "        all_data = []\n",
    "        for file_path in self.npz_files:\n",
    "            complete, partial = self.load_single_file(file_path)\n",
    "            all_data.append((complete, partial))\n",
    "        return all_data\n",
    "\n",
    "    def get_voxel_grids(self, index: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns complete and partial voxel grids from a specific file\"\"\"\n",
    "        if index >= len(self.npz_files):\n",
    "            raise IndexError(f\"Index {index} out of range. Only {len(self.npz_files)} files available.\")\n",
    "        return self.load_single_file(self.npz_files[index])\n",
    "\n",
    "\n",
    "class VoxelDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"PyTorch Dataset for voxel completion\"\"\"\n",
    "\n",
    "    def __init__(self, zip_path: str, transform=None):\n",
    "        self.data_loader = VoxelDataLoader(zip_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader.npz_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        complete, partial = self.data_loader.get_voxel_grids(idx)\n",
    "        # Normalize to [0,1] if not already\n",
    "        complete = (complete > 0).float()\n",
    "        partial = (partial > 0).float()\n",
    "        if self.transform:\n",
    "            complete, partial = self.transform(complete, partial)\n",
    "        return complete, partial\n",
    "\n",
    "\n",
    "# Update data loader creation function\n",
    "def create_data_loader(zip_path: str, batch_size: int = 1, shuffle: bool = True, num_workers: int = 0):\n",
    "    \"\"\"Create a PyTorch DataLoader for training\"\"\"\n",
    "    dataset = VoxelDataset(zip_path)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        # pin_memory=True\n",
    "    )\n",
    "\n",
    "\n",
    "def split_dataset(dataset, train_ratio=0.8, val_ratio=0.2, seed=42):\n",
    "    n = len(dataset)\n",
    "    indices = list(range(n))\n",
    "    random.Random(seed).shuffle(indices)\n",
    "\n",
    "    # from dataset: 80% train 20% test \n",
    "    n_trainval = int(n * 0.8)\n",
    "    n_test = n - n_trainval\n",
    "    trainval_indices = indices[:n_trainval]\n",
    "    test_indices = indices[n_trainval:]\n",
    "    # from training data: 80% train 20% validation\n",
    "    n_train = int(len(trainval_indices) * 0.8)\n",
    "    train_indices = trainval_indices[:n_train]\n",
    "    val_indices = trainval_indices[n_train:]\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "def create_data_loaders(zip_path, batch_size=1, shuffle=True, num_workers=0, seed=42):\n",
    "    dataset = VoxelDataset(zip_path)\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    train_idx, val_idx, test_idx = split_dataset(dataset, seed=seed)\n",
    "    train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    val_loader = DataLoader(Subset(dataset, val_idx), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(Subset(dataset, test_idx), batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "class LocalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    3D local attention: attends to a 3D window [ws, ws, ws] around the target voxel, preserving spatial structure.\n",
    "    Input: target_embedding [B, d_model], neighbor_embeddings [B, ws, ws, ws, d_model]\n",
    "    Output: attended_embedding [B, d_model]\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int = 4, window_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.window_size = window_size\n",
    "        assert d_model % num_heads == 0\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, target_embedding, neighbor_embeddings):\n",
    "        # target_embedding: [B, d_model]\n",
    "        # neighbor_embeddings: [B, ws, ws, ws, d_model]\n",
    "        B, ws1, ws2, ws3, d_model = neighbor_embeddings.shape\n",
    "        N = ws1 * ws2 * ws3\n",
    "        Q = self.q_proj(target_embedding).view(B, self.num_heads, self.head_dim)  # [B, num_heads, head_dim]\n",
    "        K = self.k_proj(neighbor_embeddings).view(B, ws1, ws2, ws3, self.num_heads, self.head_dim)  # [B, ws, ws, ws, num_heads, head_dim]\n",
    "        V = self.v_proj(neighbor_embeddings).view(B, ws1, ws2, ws3, self.num_heads, self.head_dim)  # [B, ws, ws, ws, num_heads, head_dim]\n",
    "        # Center voxel is at (ws//2, ws//2, ws//2)\n",
    "        Q = Q.unsqueeze(1).unsqueeze(1).unsqueeze(1)  # [B,1,1,1,num_heads,head_dim]\n",
    "        # Compute attention scores: [B, ws, ws, ws, num_heads]\n",
    "        attn_scores = (Q * K).sum(-1) * self.scale  # [B, ws, ws, ws, num_heads]\n",
    "        attn_scores = attn_scores.permute(0, 4, 1, 2, 3)  # [B, num_heads, ws, ws, ws]\n",
    "        attn_weights = attn_scores.reshape(B, self.num_heads, -1).softmax(-1).reshape(B, self.num_heads, ws1, ws2, ws3)\n",
    "        # Weighted sum over 3D window\n",
    "        V = V.permute(0, 4, 1, 2, 3, 5)  # [B, num_heads, ws, ws, ws, head_dim]\n",
    "        attn_weights = attn_weights.unsqueeze(-1)  # [B, num_heads, ws, ws, ws, 1]\n",
    "        attn_out = (attn_weights * V).sum(dim=(2,3,4))  # [B, num_heads, head_dim]\n",
    "        attn_out = attn_out.reshape(B, self.d_model)\n",
    "        return self.out_proj(attn_out)\n",
    "\n",
    "class VoxelTransformerLayerVoxelwise(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int = 4, window_size: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = LocalAttention(d_model, num_heads, window_size)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, target_embedding, neighbor_embeddings):\n",
    "        x = self.norm1(target_embedding)\n",
    "        attn_out = self.attn(x, neighbor_embeddings)\n",
    "        x = x + attn_out\n",
    "        x2 = self.norm2(x)\n",
    "        ffn_out = self.ffn(x2)\n",
    "        return x + ffn_out\n",
    "\n",
    "class PositionalEncoding3D(nn.Module):\n",
    "    def __init__(self, d_model: int, max_grid_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_grid_size = max_grid_size\n",
    "        \n",
    "        # Create learnable positional embeddings\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(max_grid_size, max_grid_size, max_grid_size, d_model)\n",
    "        )\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: input tensor with shape [B, D, H, W] or similar\n",
    "        # Extract spatial dimensions\n",
    "        *_, D, H, W = x.shape\n",
    "        return self.pos_embed[:D, :H, :W, :]  # [D, H, W, d_model]\n",
    "    \n",
    "    def get_encoding(self, D, H, W):\n",
    "        \"\"\"Get positional encoding for specific dimensions\"\"\"\n",
    "        return self.pos_embed[:D, :H, :W, :]  # [D, H, W, d_model]\n",
    "\n",
    "\n",
    "class VoxelCompletionTransformerVoxelwise(nn.Module):\n",
    "    def __init__(self, d_model=64, num_heads=4, num_layers=4, window_size=3, dropout=0.1, max_grid_size=32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.window_size = window_size\n",
    "        self.input_proj = nn.Linear(1, d_model)\n",
    "        self.pos_encoding = PositionalEncoding3D(d_model, max_grid_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            VoxelTransformerLayerVoxelwise(d_model, num_heads, window_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_proj = nn.Linear(d_model, 1)\n",
    "    \n",
    "    def extract_neighborhood(self, grid, idx):\n",
    "        \"\"\"Extract neighborhood windows around target positions\"\"\"\n",
    "        # grid: [B, D, H, W], idx: [B, 3] (x, y, z)\n",
    "        B = grid.shape[0]\n",
    "        ws = self.window_size\n",
    "        pad = ws // 2\n",
    "        \n",
    "        # Pad the grid\n",
    "        padded = F.pad(grid, (pad, pad, pad, pad, pad, pad), mode='constant', value=0)  # [B, D+2p, H+2p, W+2p]\n",
    "        \n",
    "        # Extract neighborhoods for all batch items\n",
    "        neighborhoods = []\n",
    "        for b in range(B):\n",
    "            x, y, z = idx[b]\n",
    "            x, y, z = x + pad, y + pad, z + pad  # Account for padding\n",
    "            \n",
    "            # Extract the window\n",
    "            neighborhood = padded[b, x-pad:x+pad+1, y-pad:y+pad+1, z-pad:z+pad+1]  # [ws, ws, ws]\n",
    "            neighborhoods.append(neighborhood.unsqueeze(-1))  # [ws, ws, ws, 1]\n",
    "        \n",
    "        neighborhoods = torch.stack(neighborhoods, dim=0)  # [B, ws, ws, ws, 1]\n",
    "        return neighborhoods\n",
    "    \n",
    "    def extract_neighborhood_positions(self, pos_enc_grid, target_idx):\n",
    "        \"\"\"Extract positional encodings for neighborhood windows around target positions\"\"\"\n",
    "        B = target_idx.shape[0]\n",
    "        ws = self.window_size\n",
    "        pad = ws // 2\n",
    "        \n",
    "        # Pad the positional encoding grid\n",
    "        pos_enc_padded = F.pad(pos_enc_grid, (0, 0, pad, pad, pad, pad, pad, pad), mode='constant', value=0)\n",
    "        \n",
    "        # Extract neighborhoods for all batch items\n",
    "        neighbor_positions = []\n",
    "        for b in range(B):\n",
    "            x, y, z = target_idx[b]\n",
    "            x, y, z = x + pad, y + pad, z + pad  # Account for padding\n",
    "            \n",
    "            # Extract the positional encoding window\n",
    "            pos_window = pos_enc_padded[x-pad:x+pad+1, y-pad:y+pad+1, z-pad:z+pad+1, :]  # [ws, ws, ws, d_model]\n",
    "            neighbor_positions.append(pos_window)\n",
    "        \n",
    "        neighbor_positions = torch.stack(neighbor_positions, dim=0)  # [B, ws, ws, ws, d_model]\n",
    "        return neighbor_positions\n",
    "\n",
    "    def forward(self, partial_grid, target_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            partial_grid: [B, D, H, W] - partially filled voxel grid\n",
    "            target_idx: [B, 3] - target positions (x, y, z) to predict\n",
    "        \n",
    "        Returns:\n",
    "            logit: [B] - prediction logits for target voxels\n",
    "        \"\"\"\n",
    "        B, D, H, W = partial_grid.shape\n",
    "        ws = self.window_size\n",
    "        center = ws // 2\n",
    "        \n",
    "        # Get positional encodings for the entire spatial grid\n",
    "        pos_enc_grid = self.pos_encoding.get_encoding(D, H, W)  # [D, H, W, d_model]\n",
    "        \n",
    "        # Extract neighborhoods from the voxel grid\n",
    "        neighborhoods = self.extract_neighborhood(partial_grid, target_idx)  # [B, ws, ws, ws, 1]\n",
    "        \n",
    "        # Get target voxel values (center of neighborhood)\n",
    "        target_voxel = neighborhoods[:, center, center, center, :]  # [B, 1]\n",
    "        \n",
    "        # Get positional encodings for target positions\n",
    "        # Use advanced indexing to efficiently get positions for all batch items\n",
    "        target_pos = pos_enc_grid[target_idx[:, 0], target_idx[:, 1], target_idx[:, 2], :]  # [B, d_model]\n",
    "        \n",
    "        # Project target voxel and add positional encoding\n",
    "        x = self.input_proj(target_voxel) + target_pos  # [B, d_model]\n",
    "        \n",
    "        # Get positional encodings for neighborhood windows\n",
    "        neighbor_pos = self.extract_neighborhood_positions(pos_enc_grid, target_idx)  # [B, ws, ws, ws, d_model]\n",
    "        \n",
    "        # Project neighbor voxels and add their positional encodings\n",
    "        neighbor_emb = self.input_proj(neighborhoods) + neighbor_pos  # [B, ws, ws, ws, d_model]\n",
    "        \n",
    "        # Apply transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, neighbor_emb)\n",
    "        \n",
    "        # Generate final prediction logit\n",
    "        logit = self.output_proj(x).squeeze(-1)  # [B]\n",
    "        return logit\n",
    "\n",
    "def train_model_voxelwise(\n",
    "    model: nn.Module,\n",
    "    train_set,\n",
    "    val_set,\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 1,\n",
    "    window_size: int = 3,\n",
    "    num_voxels_per_obj: int = 64,\n",
    "    seed: int = 42\n",
    "):\n",
    "    print(f\"Batch Size: {batch_size}, Window Size: {window_size}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    print(f\"Train loader size: {len(train_loader)}, Val loader size: {len(val_loader)}\")\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    total_start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        num_samples_processed = 0\n",
    "        model.train()\n",
    "        epoch_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=True, unit='sample')\n",
    "        for batch_idx, (complete_grid, partial_grid) in enumerate(epoch_pbar):\n",
    "            complete_grid = complete_grid.to(device)\n",
    "            partial_grid = partial_grid.to(device)\n",
    "            B, D, H, W = partial_grid.shape\n",
    "            # 1. Sample indices: half truly missing, half always empty\n",
    "            target_indices = sample_balanced_voxels(partial_grid, complete_grid, num_voxels_per_obj, window_size=window_size)  # [B, num_voxels, 3]\n",
    "            batch_losses = []\n",
    "            # Iterative prediction: update partial_grid after each prediction\n",
    "            for v in range(num_voxels_per_obj):\n",
    "                idx = target_indices[:, v, :]  # [B, 3]\n",
    "                logits = model(partial_grid, idx)  # [B]\n",
    "                labels = complete_grid[torch.arange(B), idx[:,0], idx[:,1], idx[:,2]].float()  # [B]\n",
    "                loss = criterion(logits, labels)\n",
    "                batch_losses.append(loss)\n",
    "                # Update partial_grid for next iteration (if predicted filled)\n",
    "                with torch.no_grad():\n",
    "                    pred_filled = (torch.sigmoid(logits) > 0.5).float()\n",
    "                    for b in range(B):\n",
    "                        x, y, z = idx[b]\n",
    "                        if pred_filled[b] == 1.0:\n",
    "                            partial_grid[b, x, y, z] = 1.0\n",
    "            total_batch_loss = torch.stack(batch_losses).mean()\n",
    "            optimizer.zero_grad()\n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += total_batch_loss.item()\n",
    "            num_samples_processed += 1\n",
    "            epoch_pbar.set_postfix({\n",
    "                'train_loss': f'{total_loss/num_samples_processed:.4f}',\n",
    "                'samples': num_samples_processed,\n",
    "                'lr': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        avg_train_loss = total_loss / max(num_samples_processed, 1)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {avg_train_loss:.4f}, Samples: {num_samples_processed}\")\n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"\\nTraining completed in {timedelta(seconds=int(total_time))}\")\n",
    "    print(f\"Average time per epoch: {timedelta(seconds=int(total_time/num_epochs))}\")\n",
    "\n",
    "def sample_missing_voxels(partial_grid, complete_grid, num_samples, window_size=3):\n",
    "    # Only sample from truly missing voxels: (partial == 0) & (complete == 1)\n",
    "    # Prioritize those with most filled neighbors, break ties by distance to origin\n",
    "    B, D, H, W = partial_grid.shape\n",
    "    pad = window_size // 2\n",
    "    indices = []\n",
    "    for b in range(B):\n",
    "        missing = ((partial_grid[b] == 0) & (complete_grid[b] == 1)).nonzero(as_tuple=False)  # [N, 3]\n",
    "        if len(missing) == 0:\n",
    "            # fallback: pick random\n",
    "            x = torch.randint(0, D, (num_samples,))\n",
    "            y = torch.randint(0, H, (num_samples,))\n",
    "            z = torch.randint(0, W, (num_samples,))\n",
    "            idx = torch.stack([x, y, z], dim=1)\n",
    "            indices.append(idx)\n",
    "            continue\n",
    "        # Pad partial grid for easy neighbor counting\n",
    "        padded = F.pad(partial_grid[b].unsqueeze(0), (pad, pad, pad, pad, pad, pad), mode='constant', value=0)[0]\n",
    "        neighbor_counts = []\n",
    "        dists = []\n",
    "        for coord in missing:\n",
    "            x, y, z = coord.tolist()\n",
    "            x_p, y_p, z_p = x+pad, y+pad, z+pad\n",
    "            window = padded[x_p-pad:x_p+pad+1, y_p-pad:y_p+pad+1, z_p-pad:z_p+pad+1]\n",
    "            count = window.sum().item()\n",
    "            neighbor_counts.append(count)\n",
    "            dists.append(x**2 + y**2 + z**2)\n",
    "        neighbor_counts = np.array(neighbor_counts)\n",
    "        dists = np.array(dists)\n",
    "        # Sort: most filled neighbors first, then closest to origin\n",
    "        sort_idx = np.lexsort((dists, -neighbor_counts))\n",
    "        chosen = missing[sort_idx][:num_samples]\n",
    "        # Pad if not enough missing voxels\n",
    "        if chosen.shape[0] < num_samples:\n",
    "            pad_count = num_samples - chosen.shape[0]\n",
    "            pad_idx = chosen[-1].unsqueeze(0).repeat(pad_count, 1) if chosen.shape[0] > 0 else torch.zeros((pad_count, 3), dtype=torch.long)\n",
    "            chosen = torch.cat([chosen, pad_idx], dim=0)\n",
    "        indices.append(chosen)\n",
    "    indices = torch.stack(indices, dim=0)  # [B, num_samples, 3]\n",
    "    return indices\n",
    "\n",
    "def sample_balanced_voxels(partial_grid, complete_grid, num_samples, window_size=3):\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    # Half from truly missing, half from always empty\n",
    "    B, D, H, W = partial_grid.shape\n",
    "    pad = window_size // 2\n",
    "    indices = []\n",
    "    n1 = num_samples // 2\n",
    "    n2 = num_samples - n1\n",
    "\n",
    "    for b in range(B):\n",
    "        # Truly missing: present in complete, missing in partial\n",
    "        missing = ((partial_grid[b] == 0) & (complete_grid[b] == 1)).nonzero(as_tuple=False)  # [N1, 3]\n",
    "        # Always empty: missing in both\n",
    "        empty = ((partial_grid[b] == 0) & (complete_grid[b] == 0)).nonzero(as_tuple=False)  # [N2, 3]\n",
    "\n",
    "        # Prioritize truly missing first\n",
    "        if len(missing) > 0:\n",
    "            padded = F.pad(\n",
    "                partial_grid[b].unsqueeze(0),\n",
    "                (pad, pad, pad, pad, pad, pad),\n",
    "                mode=\"constant\",\n",
    "                value=0\n",
    "            )[0]\n",
    "            neighbor_counts = []\n",
    "            dists = []\n",
    "\n",
    "            for coord in missing:\n",
    "                x, y, z = coord.tolist()\n",
    "                x_p, y_p, z_p = x + pad, y + pad, z + pad\n",
    "                window = padded[\n",
    "                    x_p - pad : x_p + pad + 1,\n",
    "                    y_p - pad : y_p + pad + 1,\n",
    "                    z_p - pad : z_p + pad + 1,\n",
    "                ]\n",
    "                count = window.sum().item()\n",
    "                neighbor_counts.append(count)\n",
    "                dists.append(x**2 + y**2 + z**2)\n",
    "\n",
    "            # Explicit sorting: most neighbors first, then closest to origin\n",
    "            sort_idx = sorted(\n",
    "                range(len(missing)),\n",
    "                key=lambda i: (-neighbor_counts[i], dists[i])\n",
    "            )\n",
    "            chosen_missing = missing[sort_idx][:n1]\n",
    "\n",
    "            # Pad if not enough\n",
    "            if chosen_missing.shape[0] < n1:\n",
    "                pad_count = n1 - chosen_missing.shape[0]\n",
    "                pad_idx = (\n",
    "                    chosen_missing[-1].unsqueeze(0).repeat(pad_count, 1)\n",
    "                    if chosen_missing.shape[0] > 0\n",
    "                    else torch.zeros((pad_count, 3), dtype=torch.long)\n",
    "                )\n",
    "                chosen_missing = torch.cat([chosen_missing, pad_idx], dim=0)\n",
    "        else:\n",
    "            # Random fallback if no missing voxels\n",
    "            x = torch.randint(0, D, (n1,))\n",
    "            y = torch.randint(0, H, (n1,))\n",
    "            z = torch.randint(0, W, (n1,))\n",
    "            chosen_missing = torch.stack([x, y, z], dim=1)\n",
    "\n",
    "        # Randomly sample from always empty\n",
    "        if len(empty) > 0:\n",
    "            perm = torch.randperm(len(empty))[:n2]\n",
    "            chosen_empty = empty[perm]\n",
    "            if chosen_empty.shape[0] < n2:\n",
    "                pad_count = n2 - chosen_empty.shape[0]\n",
    "                pad_idx = (\n",
    "                    chosen_empty[-1].unsqueeze(0).repeat(pad_count, 1)\n",
    "                    if chosen_empty.shape[0] > 0\n",
    "                    else torch.zeros((pad_count, 3), dtype=torch.long)\n",
    "                )\n",
    "                chosen_empty = torch.cat([chosen_empty, pad_idx], dim=0)\n",
    "        else:\n",
    "            # Random fallback if no empty voxels\n",
    "            x = torch.randint(0, D, (n2,))\n",
    "            y = torch.randint(0, H, (n2,))\n",
    "            z = torch.randint(0, W, (n2,))\n",
    "            chosen_empty = torch.stack([x, y, z], dim=1)\n",
    "\n",
    "        # Combine missing + empty\n",
    "        indices.append(torch.cat([chosen_missing, chosen_empty], dim=0))\n",
    "\n",
    "    indices = torch.stack(indices, dim=0)  # [B, num_samples, 3]\n",
    "    return indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e30d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-auth-oauthlib\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# DRIVE_PATH = \"/content/drive/MyDrive/AUB_masters/thesis/data/partial_data_16.zip\"  # Adjust this path to match your Drive structure\n",
    "# LOCAL_PATH = \"/content/partial_data\"\n",
    "# !mkdir -p {LOCAL_PATH}\n",
    "\n",
    "# print(\"Copying data from Drive to local storage...\")\n",
    "# !cp \"{DRIVE_PATH}\" \"{LOCAL_PATH}/data.zip\"\n",
    "# zip_path = f\"{LOCAL_PATH}/data.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = \"../../chunk_data_16_flood_fill_rm_40.zip\"\n",
    "dataset = VoxelDataset(zip_path)\n",
    "\n",
    "print(f\"Total dataset size: {len(dataset)}\")\n",
    "\n",
    "train_idx, val_idx, test_idx = split_dataset(dataset, seed=42)\n",
    "train_set = torch.utils.data.Subset(dataset, train_idx)\n",
    "val_set = torch.utils.data.Subset(dataset, val_idx)\n",
    "test_set = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = VoxelCompletionTransformer(\n",
    "    d_model=48,        \n",
    "    num_heads=6,       \n",
    "    num_layers=6,      \n",
    "    window_size=3,    \n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "torch.cuda.empty_cache()\n",
    "# best model before this was num_epochs=2\n",
    "train_model(model, train_set, val_set, num_epochs=5, batch_size=2, lambda_consistency=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec58e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = \"../../models/iterative_model_rm_20/iterative_model_rm_20_dmodel_RES_32.pth\"\n",
    "# os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "# }, MODEL_SAVE_PATH)\n",
    "# print(f\"Model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23d3323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporary directory: /tmp/tmplcoiamyx\n",
      "Extracted zip file to temporary directory\n",
      "Found 264091 total NPZ files\n",
      "Using 264091\n",
      "Cleaned up temporary directory: /tmp/tmpbnh9we5n\n",
      "Loaded 1000 test samples from ../../test_data/test_data_rm_20_iterative/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import json \n",
    "\n",
    "# load_dir = \"../../test_data/test_data_rm_40/\"\n",
    "# test_samples = []\n",
    "\n",
    "# for file in sorted(glob.glob(os.path.join(load_dir, \"test_*.npz\"))):\n",
    "#     data = np.load(file)\n",
    "#     complete = torch.from_numpy(data['complete']).float()\n",
    "#     partial = torch.from_numpy(data['partial']).float()\n",
    "#     test_samples.append((complete, partial))\n",
    "    \n",
    "zip_path = \"../../model_data/chunk_data_32_flood_fill_rm_20.zip\"\n",
    "dataset = VoxelDataset(zip_path)\n",
    "\n",
    "test_dir = \"../../test_data/test_data_rm_20_iterative/\"\n",
    "test_indices_file = os.path.join(test_dir, \"test_indices.json\")\n",
    "\n",
    "with open(test_indices_file, \"r\") as f:\n",
    "    test_idx = json.load(f)\n",
    "\n",
    "test_samples = []\n",
    "for idx in range(1000):\n",
    "    complete, partial = dataset[idx]\n",
    "    test_samples.append((complete, partial))\n",
    "\n",
    "print(f\"Loaded {len(test_samples)} test samples from {test_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4fb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Updated test_model to use test_set ---\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "def test_model(model_path, test_set, sample_idx=0, threshold=0.5, device=None):\n",
    "    \"\"\"\n",
    "    For every empty voxel in the partial grid, predict if it should be filled (probability > threshold).\n",
    "    Voxels are sorted by number of filled neighbors in a 3x3x3 window (descending), breaking ties by distance to origin (ascending).\n",
    "    After each prediction, update the output grid for the next prediction.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VoxelCompletionTransformerVoxelwise(\n",
    "        d_model=96,        \n",
    "        num_heads=6,       \n",
    "        num_layers=6,\n",
    "        max_grid_size=32,      \n",
    "        window_size=3,    \n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace('module.', '') if k.startswith('module.') else k\n",
    "        new_state_dict[new_key] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    complete, partial = test_set[sample_idx]\n",
    "    complete = (complete > 0).float()\n",
    "    partial = (partial > 0).float()\n",
    "    B, D, H, W = 1, *partial.shape\n",
    "    partial_grid = partial.unsqueeze(0).to(device)\n",
    "    complete_grid = complete.unsqueeze(0).to(device)\n",
    "    output_grid = partial_grid.clone()\n",
    "\n",
    "    # Find all empty voxels in the partial grid\n",
    "    empty_voxels = (partial_grid[0] == 0).nonzero(as_tuple=False)  # [N, 3]\n",
    "\n",
    "    # Sort empty voxels by number of filled neighbors in a 3x3x3 window, break ties by distance to origin\n",
    "    def count_filled_neighbors_3x3x3(voxel, grid):\n",
    "        x, y, z = voxel\n",
    "        D, H, W = grid.shape[1:]\n",
    "        count = 0\n",
    "        for dx in [-1, 0, 1]:\n",
    "            for dy in [-1, 0, 1]:\n",
    "                for dz in [-1, 0, 1]:\n",
    "                    nx, ny, nz = x + dx, y + dy, z + dz\n",
    "                    if 0 <= nx < D and 0 <= ny < H and 0 <= nz < W:\n",
    "                        if dx == 0 and dy == 0 and dz == 0:\n",
    "                            continue  # skip the voxel itself\n",
    "                        if grid[0, nx, ny, nz] > 0:\n",
    "                            count += 1\n",
    "        return count\n",
    "\n",
    "    voxel_scores = []\n",
    "    for v in empty_voxels:\n",
    "        filled_neighbors = count_filled_neighbors_3x3x3(v.tolist(), output_grid)\n",
    "        distance = sum([coord**2 for coord in v.tolist()])  # squared Euclidean distance to origin\n",
    "        voxel_scores.append((v, filled_neighbors, distance))\n",
    "    # Sort: more neighbors first, then closer to origin\n",
    "    voxel_scores.sort(key=lambda x: (-x[1], x[2]))\n",
    "    sorted_empty_voxels = [v[0] for v in voxel_scores]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in sorted_empty_voxels:\n",
    "            idx = idx.unsqueeze(0)  # [1, 3]\n",
    "            logits = model(output_grid, idx)  # [1]\n",
    "            pred_filled = (torch.sigmoid(logits) > threshold).float()\n",
    "            # Update output grid for next prediction\n",
    "            if pred_filled[0] == 1.0:\n",
    "                x, y, z = idx[0]\n",
    "                output_grid[0, x, y, z] = 1.0\n",
    "    output = output_grid.squeeze(0).cpu().numpy()\n",
    "    np.save(\"output_voxel.npy\", output)\n",
    "    np.save(\"complete_voxel.npy\", complete.cpu().numpy())\n",
    "    np.save(\"partial_voxel.npy\", partial.cpu().numpy())\n",
    "    print(\"Inference complete. Output saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe5480df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Output saved.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_model(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    test_set=test_samples,\n",
    "    sample_idx=\n",
    "    # 74,\n",
    "    random.randint(0, 1000),\n",
    "    threshold=0.9,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c26db6",
   "metadata": {},
   "source": [
    "# Test Function w/ Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7f086b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Updated test_model_metrics to compute CD, IoU, F1, HD ---\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist, directed_hausdorff\n",
    "\n",
    "def test_model_metrics(model_path, test_set, sample_idx=0, threshold=0.5, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VoxelCompletionTransformer(\n",
    "        d_model=96,        \n",
    "        num_heads=6,       \n",
    "        num_layers=6,\n",
    "        max_grid_size=32,      \n",
    "        window_size=3,    \n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    # checkpoint = torch.load(model_path, map_location=device)\n",
    "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace('module.', '') if k.startswith('module.') else k\n",
    "        new_state_dict[new_key] = v\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "    model.eval()\n",
    "    complete, partial = test_set[sample_idx]\n",
    "\n",
    "    filled_complete = (complete > 0).sum()\n",
    "    filled_partial = ((complete > 0) & (partial > 0)).sum()\n",
    "    missing_percent = (filled_complete - filled_partial) / filled_complete\n",
    "    print(\"Missing Percentage: \", missing_percent)\n",
    "\n",
    "    ptl = partial\n",
    "    partial = partial.unsqueeze(0).unsqueeze(0).to(device)  # [1, 1, D, H, W]\n",
    "    with torch.no_grad():\n",
    "        output = model(partial)\n",
    "        output = torch.sigmoid(output)\n",
    "        # output[0, 0][ptl == 1] = 1.0\n",
    "        output = output.squeeze().cpu()\n",
    "\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            for k in range(output.shape[2]):\n",
    "                if output[i, j, k] > threshold:\n",
    "                    output[i, j, k] = 1.0\n",
    "                else:\n",
    "                    output[i, j, k] = 0.0\n",
    "    print(\"Inference complete.\")\n",
    "    print(\"Partial shape:\", partial.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "    # Binarize output\n",
    "    pred = (output > threshold).float()\n",
    "    gt = (complete > 0).float()\n",
    "\n",
    "    # IoU\n",
    "    intersection = ((pred == 1) & (gt == 1)).sum().item()\n",
    "    union = ((pred == 1) | (gt == 1)).sum().item()\n",
    "    iou = intersection / (union + 1e-8)\n",
    "    print(f\"IoU: {iou:.4f}\")\n",
    "\n",
    "    # F1 Score\n",
    "    tp = ((pred == 1) & (gt == 1)).sum().item()\n",
    "    fp = ((pred == 1) & (gt == 0)).sum().item()\n",
    "    fn = ((pred == 0) & (gt == 1)).sum().item()\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Chamfer Distance (CD)\n",
    "    def get_points(voxel):\n",
    "        return np.argwhere(voxel.numpy() > 0.5)\n",
    "    pred_points = get_points(pred)\n",
    "    gt_points = get_points(gt)\n",
    "    if len(pred_points) > 0 and len(gt_points) > 0:\n",
    "        dist_pred_to_gt = cdist(pred_points, gt_points)\n",
    "        dist_gt_to_pred = cdist(gt_points, pred_points)\n",
    "        chamfer = np.mean(np.min(dist_pred_to_gt, axis=1)) + np.mean(np.min(dist_gt_to_pred, axis=1))\n",
    "        print(f\"Chamfer Distance: {chamfer:.4f}\")\n",
    "    else:\n",
    "        print(\"Chamfer Distance: N/A (empty prediction or ground truth)\")\n",
    "\n",
    "    # Hausdorff Distance (HD, UHD)\n",
    "    if len(pred_points) > 0 and len(gt_points) > 0:\n",
    "        hd_pred_gt = directed_hausdorff(pred_points, gt_points)[0]\n",
    "        hd_gt_pred = directed_hausdorff(gt_points, pred_points)[0]\n",
    "        hausdorff = max(hd_pred_gt, hd_gt_pred)\n",
    "        print(f\"Hausdorff Distance: {hausdorff:.4f}\")\n",
    "        print(f\"Unidirectional HD (pred→gt): {hd_pred_gt:.4f}, (gt→pred): {hd_gt_pred:.4f}\")\n",
    "    else:\n",
    "        print(\"Hausdorff Distance: N/A (empty prediction or ground truth)\")\n",
    "\n",
    "    out_path = \"output_voxel.npy\"\n",
    "    complete_path = \"complete_voxel.npy\"\n",
    "    partial_path = \"partial_voxel.npy\"\n",
    "    np.save(out_path, output.numpy())\n",
    "    np.save(complete_path, complete)\n",
    "    np.save(partial_path, ptl)\n",
    "    print(\"Sample Index: \", sample_idx)\n",
    "    print(f\"Output saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2f356fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Percentage:  tensor(0.1999)\n",
      "Inference complete.\n",
      "Partial shape: torch.Size([1, 1, 32, 32, 32])\n",
      "Output shape: torch.Size([32, 32, 32])\n",
      "IoU: 0.8934\n",
      "F1 Score: 0.9437\n",
      "Chamfer Distance: 0.1213\n",
      "Hausdorff Distance: 2.4495\n",
      "Unidirectional HD (pred→gt): 2.4495, (gt→pred): 1.0000\n",
      "Sample Index:  765\n",
      "Output saved to output_voxel.npy\n"
     ]
    }
   ],
   "source": [
    "test_model_metrics(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    test_set=test_samples,\n",
    "    sample_idx=random.randint(0, len(test_samples) - 1),\n",
    "    # sample_idx=293,\n",
    "    threshold=0.7\n",
    ")\n",
    "# 25451\n",
    "# 31878\n",
    "# 41617"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde604aa",
   "metadata": {},
   "source": [
    "# Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7b79d98b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[139]\u001b[39m\u001b[32m, line 407\u001b[39m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAverage entropy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.mean(entropy_map)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    402\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEntropy range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentropy_map.min()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentropy_map.max()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    405\u001b[39m output, attention_weights, partial, complete, ptl = test_model_with_attention(\n\u001b[32m    406\u001b[39m     model_path=MODEL_SAVE_PATH,\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     test_set=\u001b[43mtest_set\u001b[49m,\n\u001b[32m    408\u001b[39m     sample_idx=\u001b[32m0\u001b[39m\n\u001b[32m    409\u001b[39m )\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m# Visualize attention weights\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_weights:\n",
      "\u001b[31mNameError\u001b[39m: name 'test_set' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def test_model_with_attention(model_path, test_set, sample_idx=0, threshold=0.5, device=None):\n",
    "    \"\"\"Modified inference function that captures attention weights\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = VoxelCompletionTransformer(\n",
    "        d_model=48,\n",
    "        num_heads=6,\n",
    "        num_layers=6,\n",
    "        window_size=3,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Hook to capture attention weights\n",
    "    attention_weights = []\n",
    "    \n",
    "    def attention_hook(module, input, output):\n",
    "        # Capture attention weights from each attention layer\n",
    "        if hasattr(module, 'attn_weights'):\n",
    "            attention_weights.append(module.attn_weights.detach().cpu())\n",
    "    \n",
    "    # Register hooks on attention layers\n",
    "    for name, module in model.named_modules():\n",
    "        if 'attention' in name.lower() or 'attn' in name.lower():\n",
    "            module.register_forward_hook(attention_hook)\n",
    "    \n",
    "    complete, partial = test_set[sample_idx]\n",
    "    filled_complete = (complete > 0).sum()\n",
    "    filled_partial = ((complete > 0) & (partial > 0)).sum()\n",
    "    missing_percent = (filled_complete - filled_partial) / filled_complete\n",
    "    \n",
    "    print(\"Missing Percentage: \", missing_percent)\n",
    "    ptl = partial\n",
    "    partial = partial.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(partial)\n",
    "    \n",
    "    output = torch.sigmoid(output)\n",
    "    output[0, 0][ptl == 1] = 1.0\n",
    "    output = output.squeeze().cpu()\n",
    "    \n",
    "    # Process output\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            for k in range(output.shape[2]):\n",
    "                if output[i, j, k] > threshold:\n",
    "                    output[i, j, k] = 1.0\n",
    "                else:\n",
    "                    output[i, j, k] = 0.0\n",
    "    \n",
    "    # Save outputs\n",
    "    out_path = \"output_voxel.npy\"\n",
    "    complete_path = \"complete_voxel.npy\"\n",
    "    partial_path = \"partial_voxel.npy\"\n",
    "    \n",
    "    np.save(out_path, output.numpy())\n",
    "    np.save(complete_path, complete)\n",
    "    np.save(partial_path, ptl)\n",
    "    \n",
    "    print(\"Sample Index: \", sample_idx)\n",
    "    print(f\"Output saved to {out_path}\")\n",
    "    \n",
    "    return output, attention_weights, partial.squeeze(), complete, ptl\n",
    "\n",
    "def visualize_attention_weights(attention_weights, layer_idx=0, head_idx=0, save_path=\"attention_viz.png\"):\n",
    "    \"\"\"Visualize attention weights for a specific layer and head\"\"\"\n",
    "    if len(attention_weights) == 0:\n",
    "        print(\"No attention weights captured. Make sure your model has attention layers with 'attention_weights' attribute.\")\n",
    "        return\n",
    "    \n",
    "    if layer_idx >= len(attention_weights):\n",
    "        print(f\"Layer {layer_idx} not found. Available layers: 0-{len(attention_weights)-1}\")\n",
    "        return\n",
    "    \n",
    "    attn = attention_weights[layer_idx]\n",
    "    print(f\"Attention weights shape: {attn.shape}\")\n",
    "    \n",
    "    # Handle different attention weight shapes\n",
    "    if len(attn.shape) == 6:  # [batch, heads, D, H, W, window_size]\n",
    "        attn = attn[0, head_idx]  # Select first batch, specific head -> [D, H, W, window_size]\n",
    "        # Reshape to 2D for visualization: [D*H*W, window_size]\n",
    "        attn = attn.reshape(-1, attn.shape[-1])\n",
    "    elif len(attn.shape) == 5:  # [heads, D, H, W, window_size]\n",
    "        attn = attn[head_idx]  # Select specific head -> [D, H, W, window_size]\n",
    "        attn = attn.reshape(-1, attn.shape[-1])\n",
    "    elif len(attn.shape) == 4:  # [batch, heads, seq_len, seq_len]\n",
    "        attn = attn[0, head_idx]  # Select first batch, specific head\n",
    "    elif len(attn.shape) == 3:  # [heads, seq_len, seq_len]\n",
    "        attn = attn[head_idx]  # Select specific head\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.heatmap(attn.numpy(), cmap='Blues', cbar=True, square=True)\n",
    "    plt.title(f'Attention Weights - Layer {layer_idx}, Head {head_idx}')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    \n",
    "    # Attention distribution histogram\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(attn.numpy().flatten(), bins=50, alpha=0.7, color='skyblue')\n",
    "    plt.title('Attention Weight Distribution')\n",
    "    plt.xlabel('Attention Weight')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Max attention per query\n",
    "    plt.subplot(2, 2, 3)\n",
    "    max_attn = torch.max(attn, dim=1)[0]\n",
    "    plt.plot(max_attn.numpy(), marker='o', linewidth=2, markersize=4)\n",
    "    plt.title('Maximum Attention per Query Position')\n",
    "    plt.xlabel('Query Position')\n",
    "    plt.ylabel('Max Attention Weight')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Attention entropy (attention spread)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    entropy = -torch.sum(attn * torch.log(attn + 1e-9), dim=1)\n",
    "    plt.plot(entropy.numpy(), marker='s', linewidth=2, markersize=4, color='red')\n",
    "    plt.title('Attention Entropy per Query Position')\n",
    "    plt.xlabel('Query Position')\n",
    "    plt.ylabel('Entropy')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_3d_attention(attention_weights, voxel_shape=(16, 16, 16), layer_idx=0, head_idx=0, \n",
    "                          query_pos=None, save_path=\"attention_3d.png\"):\n",
    "    \"\"\"Visualize 3D attention patterns for voxel data\"\"\"\n",
    "    if len(attention_weights) == 0:\n",
    "        print(\"No attention weights captured.\")\n",
    "        return\n",
    "    \n",
    "    attn = attention_weights[layer_idx]\n",
    "    print(f\"Original attention shape: {attn.shape}\")\n",
    "    \n",
    "    # Handle 6D attention weights [batch, heads, D, H, W, window_size]\n",
    "    if len(attn.shape) == 6:\n",
    "        attn = attn[0, head_idx]  # Select first batch and head -> [D, H, W, window_size]\n",
    "    elif len(attn.shape) == 5:\n",
    "        attn = attn[head_idx]  # Select head -> [D, H, W, window_size]\n",
    "    elif len(attn.shape) == 4:\n",
    "        attn = attn[0, head_idx]\n",
    "    elif len(attn.shape) == 3:\n",
    "        attn = attn[head_idx]\n",
    "    \n",
    "    print(f\"After selection: {attn.shape}\")\n",
    "    \n",
    "    # For windowed attention, we need to handle this differently\n",
    "    if len(attn.shape) == 4:  # [D, H, W, window_size]\n",
    "        D, H, W, window_size = attn.shape\n",
    "        \n",
    "        # Select a specific voxel position to visualize its attention\n",
    "        if query_pos is None:\n",
    "            query_d, query_h, query_w = D//2, H//2, W//2  # Center voxel\n",
    "        else:\n",
    "            query_d, query_h, query_w = query_pos\n",
    "        \n",
    "        # Get attention weights for this query position\n",
    "        attention_values = attn[query_d, query_h, query_w].numpy()  # [window_size]\n",
    "        \n",
    "        # Create 3D visualization showing attention within the local window\n",
    "        fig = plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Calculate window positions relative to query\n",
    "        window_radius = int(np.cbrt(window_size)) // 2  # Assuming cubic window\n",
    "        positions = []\n",
    "        \n",
    "        idx = 0\n",
    "        for dz in range(-window_radius, window_radius + 1):\n",
    "            for dy in range(-window_radius, window_radius + 1):\n",
    "                for dx in range(-window_radius, window_radius + 1):\n",
    "                    if idx < window_size:\n",
    "                        # Absolute positions\n",
    "                        abs_d = max(0, min(D-1, query_d + dz))\n",
    "                        abs_h = max(0, min(H-1, query_h + dy))\n",
    "                        abs_w = max(0, min(W-1, query_w + dx))\n",
    "                        positions.append([abs_w, abs_h, abs_d])  # x, y, z\n",
    "                        idx += 1\n",
    "        \n",
    "        positions = np.array(positions)\n",
    "        \n",
    "        # 3D scatter plot\n",
    "        ax1 = fig.add_subplot(131, projection='3d')\n",
    "        scatter = ax1.scatter(positions[:, 0], positions[:, 1], positions[:, 2], \n",
    "                             c=attention_values, cmap='viridis', s=50, alpha=0.8)\n",
    "        \n",
    "        # Highlight query position\n",
    "        query_3d = [query_w, query_h, query_d]\n",
    "        ax1.scatter(query_3d[0], query_3d[1], query_3d[2], \n",
    "                   c='red', s=200, marker='*', edgecolors='black', linewidth=2)\n",
    "        \n",
    "        ax1.set_xlabel('X (W)')\n",
    "        ax1.set_ylabel('Y (H)')\n",
    "        ax1.set_zlabel('Z (D)')\n",
    "        ax1.set_title(f'3D Attention Pattern\\nQuery at ({query_w}, {query_h}, {query_d})')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=ax1, shrink=0.5)\n",
    "        cbar.set_label('Attention Weight')\n",
    "        \n",
    "        # 2D projections\n",
    "        ax2 = fig.add_subplot(132)\n",
    "        scatter2 = ax2.scatter(positions[:, 0], positions[:, 1], c=attention_values, cmap='viridis', s=30)\n",
    "        ax2.scatter(query_3d[0], query_3d[1], c='red', s=100, marker='*', edgecolors='black')\n",
    "        ax2.set_xlabel('X (W)')\n",
    "        ax2.set_ylabel('Y (H)')\n",
    "        ax2.set_title('XY Projection')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax3 = fig.add_subplot(133)\n",
    "        scatter3 = ax3.scatter(positions[:, 0], positions[:, 2], c=attention_values, cmap='viridis', s=30)\n",
    "        ax3.scatter(query_3d[0], query_3d[2], c='red', s=100, marker='*', edgecolors='black')\n",
    "        ax3.set_xlabel('X (W)')\n",
    "        ax3.set_ylabel('Z (D)')\n",
    "        ax3.set_title('XZ Projection')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return query_d, query_h, query_w\n",
    "    \n",
    "    else:\n",
    "        # Fallback for other shapes\n",
    "        print(f\"Unexpected attention shape: {attn.shape}\")\n",
    "        return None\n",
    "\n",
    "def visualize_attention_across_layers(attention_weights, save_path=\"attention_layers.png\"):\n",
    "    \"\"\"Compare attention patterns across different layers\"\"\"\n",
    "    if len(attention_weights) == 0:\n",
    "        print(\"No attention weights captured.\")\n",
    "        return\n",
    "    \n",
    "    num_layers = len(attention_weights)\n",
    "    fig, axes = plt.subplots(2, (num_layers + 1) // 2, figsize=(4 * num_layers, 8))\n",
    "    \n",
    "    if num_layers == 1:\n",
    "        axes = [axes]\n",
    "    elif num_layers > 1:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, attn in enumerate(attention_weights):\n",
    "        # Handle 6D attention weights\n",
    "        if len(attn.shape) == 6:  # [batch, heads, D, H, W, window_size]\n",
    "            attn = attn[0, 0]  # First batch, first head -> [D, H, W, window_size]\n",
    "            # Average over spatial dimensions to get [window_size] pattern\n",
    "            attn = attn.mean(dim=(0, 1, 2))  # Average over D, H, W\n",
    "        elif len(attn.shape) == 5:  # [heads, D, H, W, window_size]\n",
    "            attn = attn[0]  # First head -> [D, H, W, window_size]\n",
    "            attn = attn.mean(dim=(0, 1, 2))\n",
    "        elif len(attn.shape) == 4:  # [batch, heads, seq_len, seq_len]\n",
    "            attn = attn[0, 0]  # First batch, first head\n",
    "        elif len(attn.shape) == 3:  # [heads, seq_len, seq_len]\n",
    "            attn = attn[0]  # First head\n",
    "        \n",
    "        ax = axes[i] if num_layers > 1 else axes\n",
    "        \n",
    "        # If we have a 1D attention pattern (from averaging), show as bar plot\n",
    "        if len(attn.shape) == 1:\n",
    "            ax.bar(range(len(attn)), attn.numpy(), alpha=0.7, color='skyblue')\n",
    "            ax.set_title(f'Layer {i} - Avg Attention Pattern')\n",
    "            ax.set_xlabel('Window Position')\n",
    "            ax.set_ylabel('Average Attention Weight')\n",
    "        else:\n",
    "            # Create heatmap for 2D attention\n",
    "            im = ax.imshow(attn.numpy(), cmap='Blues', aspect='auto')\n",
    "            ax.set_title(f'Layer {i}')\n",
    "            ax.set_xlabel('Key Position')\n",
    "            ax.set_ylabel('Query Position')\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(num_layers, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def analyze_windowed_attention(attention_weights, layer_idx=0, head_idx=0, save_path=\"windowed_attention.png\"):\n",
    "    \"\"\"Analyze attention patterns in windowed (local) attention\"\"\"\n",
    "    if len(attention_weights) == 0:\n",
    "        print(\"No attention weights captured.\")\n",
    "        return\n",
    "    \n",
    "    attn = attention_weights[layer_idx]\n",
    "    \n",
    "    # Handle 6D attention weights [batch, heads, D, H, W, window_size]\n",
    "    if len(attn.shape) == 6:\n",
    "        attn = attn[0, head_idx]  # Select first batch and head -> [D, H, W, window_size]\n",
    "    elif len(attn.shape) == 5:\n",
    "        attn = attn[head_idx]  # Select head -> [D, H, W, window_size]\n",
    "    \n",
    "    print(f\"Analyzing windowed attention with shape: {attn.shape}\")\n",
    "    \n",
    "    D, H, W, window_size = attn.shape\n",
    "    \n",
    "    # Convert to numpy for easier manipulation\n",
    "    attn_np = attn.numpy()\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Average attention pattern across all spatial positions\n",
    "    avg_attention = np.mean(attn_np, axis=(0, 1, 2))  # Average over D, H, W\n",
    "    axes[0, 0].bar(range(window_size), avg_attention, alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].set_title('Average Attention Pattern')\n",
    "    axes[0, 0].set_xlabel('Window Position')\n",
    "    axes[0, 0].set_ylabel('Average Attention Weight')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Attention variance across spatial positions\n",
    "    var_attention = np.var(attn_np, axis=(0, 1, 2))\n",
    "    axes[0, 1].bar(range(window_size), var_attention, alpha=0.7, color='lightcoral')\n",
    "    axes[0, 1].set_title('Attention Variance')\n",
    "    axes[0, 1].set_xlabel('Window Position')\n",
    "    axes[0, 1].set_ylabel('Variance')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Attention entropy distribution\n",
    "    # Calculate entropy for each spatial position\n",
    "    entropy_map = np.zeros((D, H, W))\n",
    "    for d in range(D):\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "                probs = attn_np[d, h, w]\n",
    "                probs = probs + 1e-9  # Add small epsilon to avoid log(0)\n",
    "                entropy_map[d, h, w] = -np.sum(probs * np.log(probs))\n",
    "    \n",
    "    # Show entropy for middle slice\n",
    "    middle_slice = entropy_map[D//2, :, :]\n",
    "    im1 = axes[0, 2].imshow(middle_slice, cmap='viridis', aspect='auto')\n",
    "    axes[0, 2].set_title(f'Attention Entropy (Slice D={D//2})')\n",
    "    axes[0, 2].set_xlabel('W')\n",
    "    axes[0, 2].set_ylabel('H')\n",
    "    plt.colorbar(im1, ax=axes[0, 2])\n",
    "    \n",
    "    # 4. Attention focus (max attention weight) across space\n",
    "    max_attention = np.max(attn_np, axis=-1)  # Max over window_size\n",
    "    middle_slice_max = max_attention[D//2, :, :]\n",
    "    im2 = axes[1, 0].imshow(middle_slice_max, cmap='Blues', aspect='auto')\n",
    "    axes[1, 0].set_title(f'Max Attention (Slice D={D//2})')\n",
    "    axes[1, 0].set_xlabel('W')\n",
    "    axes[1, 0].set_ylabel('H')\n",
    "    plt.colorbar(im2, ax=axes[1, 0])\n",
    "    \n",
    "    # 5. Attention distribution histogram\n",
    "    axes[1, 1].hist(attn_np.flatten(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[1, 1].set_title('Attention Weight Distribution')\n",
    "    axes[1, 1].set_xlabel('Attention Weight')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Window position preference\n",
    "    # Reshape window to 3D coordinates assuming cubic window\n",
    "    window_cube_size = int(np.cbrt(window_size))\n",
    "    if window_cube_size ** 3 == window_size:\n",
    "        window_3d = avg_attention.reshape(window_cube_size, window_cube_size, window_cube_size)\n",
    "        # Show middle slice of the window\n",
    "        middle_window_slice = window_3d[window_cube_size//2, :, :]\n",
    "        im3 = axes[1, 2].imshow(middle_window_slice, cmap='Reds', aspect='auto')\n",
    "        axes[1, 2].set_title(f'Window Attention Pattern (Middle Slice)')\n",
    "        axes[1, 2].set_xlabel('Window W')\n",
    "        axes[1, 2].set_ylabel('Window H')\n",
    "        plt.colorbar(im3, ax=axes[1, 2])\n",
    "    else:\n",
    "        # If not cubic, show as 1D\n",
    "        axes[1, 2].plot(avg_attention, marker='o', linewidth=2, markersize=4)\n",
    "        axes[1, 2].set_title('Window Attention Pattern')\n",
    "        axes[1, 2].set_xlabel('Window Position')\n",
    "        axes[1, 2].set_ylabel('Attention Weight')\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"\\nAttention Statistics:\")\n",
    "    print(f\"Average attention range: {avg_attention.min():.4f} - {avg_attention.max():.4f}\")\n",
    "    print(f\"Most attended window position: {np.argmax(avg_attention)}\")\n",
    "    print(f\"Least attended window position: {np.argmin(avg_attention)}\")\n",
    "    print(f\"Average entropy: {np.mean(entropy_map):.4f}\")\n",
    "    print(f\"Entropy range: {entropy_map.min():.4f} - {entropy_map.max():.4f}\")\n",
    "\n",
    "\n",
    "output, attention_weights, partial, complete, ptl = test_model_with_attention(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    test_set=test_set,\n",
    "    sample_idx=0\n",
    ")\n",
    "\n",
    "# Visualize attention weights\n",
    "if attention_weights:\n",
    "    analyze_windowed_attention(attention_weights, layer_idx=0, head_idx=0)\n",
    "    visualize_attention_weights(attention_weights, layer_idx=0, head_idx=0)\n",
    "    visualize_3d_attention(attention_weights, layer_idx=0, head_idx=0)\n",
    "    visualize_attention_across_layers(attention_weights)\n",
    "else:\n",
    "    print(\"No attention weights captured. You may need to modify your model to store attention weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a172dae8",
   "metadata": {},
   "source": [
    "# multi-step inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "01302aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_multi_step(\n",
    "    model_path, \n",
    "    test_set, \n",
    "    sample_idx=0, \n",
    "    start_threshold=0.7, \n",
    "    threshold_step=0.05, \n",
    "    num_steps=3, \n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run multi-step inference on a test sample, binarizing and feeding output as next partial.\n",
    "    Args:\n",
    "        model_path: Path to model checkpoint\n",
    "        test_set: List of (complete, partial) tuples\n",
    "        sample_idx: Index of test sample\n",
    "        start_threshold: Initial threshold for binarization\n",
    "        threshold_step: Amount to decrease threshold each step\n",
    "        num_steps: Number of thresholds to try\n",
    "        device: torch.device\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VoxelCompletionTransformer(\n",
    "        d_model=96,        \n",
    "        num_heads=6,       \n",
    "        num_layers=6,      \n",
    "        window_size=3,    \n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    # Remove 'module.' prefix if present (from DataParallel)\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace('module.', '') if k.startswith('module.') else k\n",
    "        new_state_dict[new_key] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "    complete, partial = test_set[sample_idx]\n",
    "    ptl = partial\n",
    "    partial = partial.unsqueeze(0).unsqueeze(0).to(device)  # [1, 1, D, H, W]\n",
    "    thresholds = [start_threshold - i * threshold_step for i in range(num_steps)]\n",
    "    with torch.no_grad():\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            output = model(partial)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.squeeze().cpu()\n",
    "            # Binarize output and use as next partial\n",
    "            for x in range(output.shape[0]):\n",
    "                for y in range(output.shape[1]):\n",
    "                    for z in range(output.shape[2]):\n",
    "                        if output[x, y, z] > threshold:\n",
    "                            output[x, y, z] = 1.0\n",
    "                        else:\n",
    "                            output[x, y, z] = 0.0\n",
    "            print(f\"\\nThreshold {threshold:.2f} (step {i+1}/{num_steps}):\")\n",
    "            print(f\"  Predicted filled voxels: {int(output.sum().item())}\")\n",
    "            # Prepare for next step\n",
    "            partial = output.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    out_path = \"output_voxel.npy\"\n",
    "    complete_path = \"complete_voxel.npy\"\n",
    "    partial_path = \"partial_voxel.npy\"\n",
    "    np.save(out_path, output.numpy())\n",
    "    np.save(complete_path, complete)\n",
    "    np.save(partial_path, ptl)\n",
    "    print(\"Sample Index: \", sample_idx)\n",
    "    print(f\"Output saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8a845fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold 0.70 (step 1/3):\n",
      "  Predicted filled voxels: 434\n",
      "\n",
      "Threshold 0.65 (step 2/3):\n",
      "  Predicted filled voxels: 1205\n",
      "\n",
      "Threshold 0.60 (step 3/3):\n",
      "  Predicted filled voxels: 2246\n",
      "Sample Index:  0\n",
      "Output saved to output_voxel.npy\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_model_multi_step(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    test_set=test_samples,\n",
    "    sample_idx=\n",
    "    0,\n",
    "    # random.randint(0, len(test_samples) - 1),\n",
    "    start_threshold=0.7, \n",
    "    threshold_step=0.05, \n",
    "    num_steps=3, \n",
    "    device=device\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
